[
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "Papers\n\n\n\n\nPapers\n\nClick “[PDF]” to download each paper.\n\n\n2018\n\n\nSumanth Krishna, Isa AlDoseri, Srujana 2018, Acharya Institute of Technology: Breast Cancer Detection using Image Processing and Deep Learning Architecture [PDF]"
  },
  {
    "objectID": "awards.html",
    "href": "awards.html",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "1st Place (Twice) at GDG Manama ML Olympiad 2023 (Mar 2023)\n\nIssued by GDG Manama & Kerne There were 5 Kaggle competitions that each had an industry dataset to solve. Won 2-out-of-5 of the competitions @ 1st Place.\nCompetition 1: National Cyber Security Center\nCompetition 2: Bahrain RCSI\n\n\n\n\nMost Iconic Project Collaboration Award, AIESEC in Bahrain (Sep 2020)\n\nFor a project called Ebtikar which reached national exposure in the Kingdom of Bahrain and was able to deliver innovative solutions towards the fight against climate change, under the United Nations Sustainable Development Goal # 13 on Climate Action"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "Global Socio-Economic Factors Part 1: Data Processing\n\n\n\n\n\n\n\nml\n\n\n\n\nAn analysis of socio-economic factors affecting the world’s countries using PySpark\n\n\n\n\n\n\nOct 31, 2023\n\n\nIsa AlDoseri\n\n\n\n\n\n\n  \n\n\n\n\nBahrain’s Solar & Wind Power Data 2022\n\n\n\n\n\n\n\nml\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\nIsa AlDoseri\n\n\n\n\n\n\n  \n\n\n\n\nFoodie’s Guide to Finding a Home in Bahrain\n\n\n\n\n\n\n\nml\n\n\nsklearn\n\n\n\n\nBy clustering neighborhoods based on popular venues surrounding them\n\n\n\n\n\n\nSep 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\nMe and Earl and the Dying Girl (and myself)\n\n\n\n\n\n\n\nmovies\n\n\n\n\nA movie that I watched during a rough time in my life.\n\n\n\n\n\n\nJun 27, 2019\n\n\nIsa AlDoseri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/me-earl-and-the-dying-girl/index.html",
    "href": "posts/me-earl-and-the-dying-girl/index.html",
    "title": "Me and Earl and the Dying Girl (and myself)",
    "section": "",
    "text": "I loved it. Every moment of it. To my mind, only a few words were lodged in my head as I watched the film, which were poignant enough to set the tone for the ride I was about to experience.\nPausing for a bit; this is the first review I’d read in the top right corner of the Google search on the film.\n\nThe beauty of “Me and Earl and the Dying Girl” — and it truly is a beautiful film, both visually and emotionally — is that it doesn’t pander to anyone.\n\n\n- S. Jhoanna Robledo, Common Sense Media\n\nThe story is honest and pure; two qualities I admire the most in the world. I strive towards them and even as I write this article, I struggle to meet them. What I’m writing is less of a review, and more of a deep dive into my own subconscious. So let’s dive in.\nMe, and Earl, and the Dying girl, is a story about Greg Gaines — a high school kid who makes movie parodies with his work partner Earl — as he’s forced to console a girl named Rachel, who has just been diagnosed with Leukemia. Rachel rebukes him at first, but she soon goes along with it after seeing Greg’s funny side, in a rather humorous conversation they have, on how she should treat people who ask her stupid questions. This is where Greg, unfortunately, falls in love with her.\nWhat begins after that is a wonderful tale of their daily meetups and hangouts, and eventually how they finally get closer. Greg’s emotional fences, start to crumble down and he gradually shows his truer self. A self that he was never comfortable to reveal.\nGreg’s stubborn refusal to fit in to any of his cliques at school, spoke volumes of my own childhood. Instead of befriending people, I spoke less to them, and during college, I was pretty much like him. My intentions were the same in both cases, and the similarity to Greg’s situation, uncanny. He/I did it in order to spare himself/myself the heartbreak of rejection, and his/my self-hating tendencies that were built up, aggravated that feeling. We had experienced the same things growing up, albeit at a slightly slower pace. I really can’t say that I’m better off now, but I’ve learnt to acknowledge those experiences and my feelings much more.\nThe movie was filled with beautiful scenes from their homemade films, and what was especially noteworthy was the montage of Rachel attempting to watch each (and every) one of their movies. A film about making bad films, and one of the characters loves it!\nThe plot was simple, and their lives even simpler, yet it made me delighted and happier. And by the end of it, heartbroken.\nPerhaps, it was those touching scenes when they huddled together, all comfy and relaxed, watching one flick after another, that the film really portrayed how innocent and simple it all was. There was a possibility of romance, but nothing cropped up; again stating how this film didn’t pander to anyone. I can’t really say any more about the film. It is now, one of my fondest memories.\nAfter it was over, I felt a strong sense that I needed to write about it. Almost as if, it rekindled that short and brief moment when I loved writing. So far, I’ve just worked on a stick comic on how colors could be interpreted differently depending on the subject. I might post about this later. And a prose on a crush I once had. Soon after, I struggled to put my pen to paper. Brief as it was, it meant so much to me. This movie made me want to start that again for some reason.\nPerhaps, it was Greg’s life as a filmmaker that echoed my own yearnings in life. To make something from nothing; to manufacture a whole world out of sheer joy of seeing it come to life. Where anything was possible…\nIf you’d actually watched the film, you’d probably think it didn’t deserve this much praise, and for lack of a better word, that I lack taste. However, knowing how it made me feel in a tumultuous time in my life, I still think my perspective holds. Hope it makes you feel the same way."
  },
  {
    "objectID": "posts/global_socioecon_indicators/index.html",
    "href": "posts/global_socioecon_indicators/index.html",
    "title": "Global Socio-Economic Factors Part 1: Data Processing",
    "section": "",
    "text": "This is the dataset where we study historical indicators of wealth, prosperity and detriment (via CO2 production) in the world.\nThis data was taken from Kaggle. We will attempt to wrangle the data and bring it to a form that is suitable for predictive modelling later on (next chapter).\n\nimport pandas as pd\nfrom pathlib import Path\n\nThese are the datasets in question\n\ndata_folder = 'data'\n!ls -1 {data_folder}/*.csv\n\ndata/co2_production.csv\ndata/gross_national_income_per_capital.csv\ndata/human_development_index.csv\ndata/life_expectancy_by_birth.csv"
  },
  {
    "objectID": "posts/global_socioecon_indicators/index.html#introduction",
    "href": "posts/global_socioecon_indicators/index.html#introduction",
    "title": "Global Socio-Economic Factors Part 1: Data Processing",
    "section": "",
    "text": "This is the dataset where we study historical indicators of wealth, prosperity and detriment (via CO2 production) in the world.\nThis data was taken from Kaggle. We will attempt to wrangle the data and bring it to a form that is suitable for predictive modelling later on (next chapter).\n\nimport pandas as pd\nfrom pathlib import Path\n\nThese are the datasets in question\n\ndata_folder = 'data'\n!ls -1 {data_folder}/*.csv\n\ndata/co2_production.csv\ndata/gross_national_income_per_capital.csv\ndata/human_development_index.csv\ndata/life_expectancy_by_birth.csv"
  },
  {
    "objectID": "posts/global_socioecon_indicators/index.html#datasets",
    "href": "posts/global_socioecon_indicators/index.html#datasets",
    "title": "Global Socio-Economic Factors Part 1: Data Processing",
    "section": "Datasets",
    "text": "Datasets\nLet’s read one of them and print the columns…\n\ndata_folder = Path(data_folder)\nlife_exp = pd.read_csv(data_folder/'life_expectancy_by_birth.csv')\nlife_exp.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 206 entries, 0 to 205\nData columns (total 37 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   ISO3           195 non-null    object \n 1   Country        206 non-null    object \n 2   hdicode        191 non-null    object \n 3   region         151 non-null    object \n 4   hdi_rank_2021  191 non-null    float64\n 5   le_1990        206 non-null    float64\n 6   le_1991        206 non-null    float64\n 7   le_1992        206 non-null    float64\n 8   le_1993        206 non-null    float64\n 9   le_1994        206 non-null    float64\n 10  le_1995        206 non-null    float64\n 11  le_1996        206 non-null    float64\n 12  le_1997        206 non-null    float64\n 13  le_1998        206 non-null    float64\n 14  le_1999        206 non-null    float64\n 15  le_2000        206 non-null    float64\n 16  le_2001        206 non-null    float64\n 17  le_2002        206 non-null    float64\n 18  le_2003        206 non-null    float64\n 19  le_2004        206 non-null    float64\n 20  le_2005        206 non-null    float64\n 21  le_2006        206 non-null    float64\n 22  le_2007        206 non-null    float64\n 23  le_2008        206 non-null    float64\n 24  le_2009        206 non-null    float64\n 25  le_2010        206 non-null    float64\n 26  le_2011        206 non-null    float64\n 27  le_2012        206 non-null    float64\n 28  le_2013        206 non-null    float64\n 29  le_2014        206 non-null    float64\n 30  le_2015        206 non-null    float64\n 31  le_2016        206 non-null    float64\n 32  le_2017        206 non-null    float64\n 33  le_2018        206 non-null    float64\n 34  le_2019        206 non-null    float64\n 35  le_2020        206 non-null    float64\n 36  le_2021        206 non-null    float64\ndtypes: float64(33), object(4)\nmemory usage: 59.7+ KB\n\n\nNow let’s read the rest of the data\n\nco2 = pd.read_csv(data_folder/'co2_production.csv')\ngross_income_percapita = pd.read_csv(data_folder/'gross_national_income_per_capital.csv')\nhdi_index = pd.read_csv(data_folder/'human_development_index.csv')\n\nLet’s ensure all 206 countries are mentioned in all three datasets\n\nhdi_index.shape[0] == co2.shape[0] == gross_income_percapita.shape[0] == life_exp.shape[0]\n\nTrue\n\n\nWe can see the same number of columns too\n\nprint(hdi_index.shape[1])\nprint(co2.shape[1])\nprint(gross_income_percapita.shape[1])\nprint(life_exp.shape[1])\n\n37\n37\n37\n37"
  },
  {
    "objectID": "posts/global_socioecon_indicators/index.html#determining-id-column",
    "href": "posts/global_socioecon_indicators/index.html#determining-id-column",
    "title": "Global Socio-Economic Factors Part 1: Data Processing",
    "section": "Determining ID column",
    "text": "Determining ID column\nWe can look at the ISO3 & Country columns as global identifiers (we’ll single one out later on), let’s see if there’s any null values in the ISO3 column\n\nlife_exp[life_exp.ISO3.isnull()]\n\n\n\n\n\n\n\n\nISO3\nCountry\nhdicode\nregion\nhdi_rank_2021\nle_1990\nle_1991\nle_1992\nle_1993\nle_1994\n...\nle_2012\nle_2013\nle_2014\nle_2015\nle_2016\nle_2017\nle_2018\nle_2019\nle_2020\nle_2021\n\n\n\n\n195\nNaN\nVery high human development\nNaN\nNaN\nNaN\n73.776652\n73.931896\n74.058848\n73.852640\n74.007158\n...\n78.578754\n78.822196\n79.057616\n79.062435\n79.264933\n79.428843\n79.582768\n79.822662\n78.789745\n78.521301\n\n\n196\nNaN\nHigh human development\nNaN\nNaN\nNaN\n67.315701\n67.567552\n67.966260\n68.358081\n68.636211\n...\n74.212238\n74.463776\n74.732246\n74.944071\n75.152677\n75.261530\n75.600606\n75.785826\n75.120548\n74.709094\n\n\n197\nNaN\nMedium human development\nNaN\nNaN\nNaN\n58.757754\n58.991558\n59.592548\n59.880467\n60.250251\n...\n67.460860\n67.893291\n68.411503\n68.895967\n69.314997\n69.694263\n69.998178\n70.219458\n69.517580\n67.438318\n\n\n198\nNaN\nLow human development\nNaN\nNaN\nNaN\n50.351409\n50.608373\n50.467296\n50.606850\n51.070213\n...\n59.569566\n60.010001\n60.326771\n60.657080\n61.064843\n61.424061\n61.721181\n62.083426\n61.675690\n61.310991\n\n\n199\nNaN\nArab States\nNaN\nNaN\nNaN\n62.973324\n63.225764\n63.520079\n64.512882\n65.562309\n...\n70.261228\n70.274253\n70.570127\n70.836099\n71.019940\n71.508744\n71.711891\n71.922194\n71.002105\n70.895040\n\n\n200\nNaN\nEast Asia and the Pacific\nNaN\nNaN\nNaN\n67.161758\n67.383754\n67.905512\n68.375688\n68.640580\n...\n74.427861\n74.677412\n74.936109\n75.156317\n75.355177\n75.419246\n75.835426\n76.036110\n75.968330\n75.579650\n\n\n201\nNaN\nEurope and Central Asia\nNaN\nNaN\nNaN\n67.781962\n67.609336\n66.896095\n66.718280\n67.136734\n...\n72.606274\n72.992396\n73.357649\n73.681015\n73.983262\n74.343005\n74.505959\n74.700741\n72.780992\n72.856526\n\n\n202\nNaN\nLatin America and the Caribbean\nNaN\nNaN\nNaN\n67.770568\n68.127209\n68.458998\n68.786974\n69.166664\n...\n74.067656\n74.316945\n74.521732\n74.579895\n74.544240\n74.705572\n74.823478\n75.014905\n73.008316\n72.099890\n\n\n203\nNaN\nSouth Asia\nNaN\nNaN\nNaN\n58.830494\n59.102221\n59.727847\n60.058622\n60.441754\n...\n67.915940\n68.395439\n68.905849\n69.407382\n69.874722\n70.245467\n70.517018\n70.722315\n69.972908\n67.855530\n\n\n204\nNaN\nSub-Saharan Africa\nNaN\nNaN\nNaN\n49.868704\n49.989060\n49.900445\n49.817075\n49.722427\n...\n57.987394\n58.539213\n59.010080\n59.428775\n59.954595\n60.348837\n60.735440\n61.120031\n60.729051\n60.112467\n\n\n205\nNaN\nWorld\nNaN\nNaN\nNaN\n65.144798\n65.299082\n65.584015\n65.753616\n65.986642\n...\n71.289119\n71.581265\n71.886202\n72.111793\n72.370895\n72.568952\n72.816114\n73.012099\n72.257297\n71.365465\n\n\n\n\n11 rows × 37 columns\n\n\n\nThe NaN values aren’t related to any countries, rather, they are to related to group of countries for brevity. We will drop these rows from the table.\n\nlife_exp_countries = life_exp.dropna(subset='ISO3', axis=0)\nlife_exp_countries.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 195 entries, 0 to 194\nData columns (total 37 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   ISO3           195 non-null    object \n 1   Country        195 non-null    object \n 2   hdicode        191 non-null    object \n 3   region         151 non-null    object \n 4   hdi_rank_2021  191 non-null    float64\n 5   le_1990        195 non-null    float64\n 6   le_1991        195 non-null    float64\n 7   le_1992        195 non-null    float64\n 8   le_1993        195 non-null    float64\n 9   le_1994        195 non-null    float64\n 10  le_1995        195 non-null    float64\n 11  le_1996        195 non-null    float64\n 12  le_1997        195 non-null    float64\n 13  le_1998        195 non-null    float64\n 14  le_1999        195 non-null    float64\n 15  le_2000        195 non-null    float64\n 16  le_2001        195 non-null    float64\n 17  le_2002        195 non-null    float64\n 18  le_2003        195 non-null    float64\n 19  le_2004        195 non-null    float64\n 20  le_2005        195 non-null    float64\n 21  le_2006        195 non-null    float64\n 22  le_2007        195 non-null    float64\n 23  le_2008        195 non-null    float64\n 24  le_2009        195 non-null    float64\n 25  le_2010        195 non-null    float64\n 26  le_2011        195 non-null    float64\n 27  le_2012        195 non-null    float64\n 28  le_2013        195 non-null    float64\n 29  le_2014        195 non-null    float64\n 30  le_2015        195 non-null    float64\n 31  le_2016        195 non-null    float64\n 32  le_2017        195 non-null    float64\n 33  le_2018        195 non-null    float64\n 34  le_2019        195 non-null    float64\n 35  le_2020        195 non-null    float64\n 36  le_2021        195 non-null    float64\ndtypes: float64(33), object(4)\nmemory usage: 57.9+ KB\n\n\nlet’s repeat this step for the other datasets, and see if they match in # of rows\n\nco2_countries = co2.dropna(subset='ISO3', axis=0)\ngross_income_percapita_countries = gross_income_percapita.dropna(subset='ISO3', axis=0)\nhdi_index_countries = hdi_index.dropna(subset='ISO3', axis=0)\n\nco2_countries.shape[0] == hdi_index_countries.shape[0] == gross_income_percapita_countries.shape[0]\n\nTrue"
  },
  {
    "objectID": "posts/global_socioecon_indicators/index.html#time-series-data",
    "href": "posts/global_socioecon_indicators/index.html#time-series-data",
    "title": "Global Socio-Economic Factors Part 1: Data Processing",
    "section": "Time Series Data",
    "text": "Time Series Data\nThese datasets, when combined, have all the makings of a time-series dataset! We wager that we can combine them into one big dataset and save it to disk. This will simplify our modeling in the next chapter.\nWe see some miscellaneous data that can be put aside; particularly the hdicode & hdi_rank_2021 columns\n\n# Putting misc cols aside so it's not repeated\ncountry_miscellaneous_cols = ['region', 'hdicode', 'hdi_rank_2021', 'ISO3']\ncountry_misc_data = life_exp_countries[['Country']+country_miscellaneous_cols]\n\ntime_series_data = life_exp_countries.drop(country_miscellaneous_cols, axis=1)\\\n.merge(hdi_index_countries.drop(country_miscellaneous_cols, axis=1), on='Country', how='outer')\\\n.merge(co2_countries.drop(country_miscellaneous_cols, axis=1), on='Country', how='outer')\\\n.merge(gross_income_percapita_countries.drop(country_miscellaneous_cols, axis=1), on='Country', how='outer')\\\n\ntime_series_data\n\n\n\n\n\n\n\n\n\nCountry\nle_1990\nle_1991\nle_1992\nle_1993\nle_1994\nle_1995\nle_1996\nle_1997\nle_1998\n...\ngnipc_2012\ngnipc_2013\ngnipc_2014\ngnipc_2015\ngnipc_2016\ngnipc_2017\ngnipc_2018\ngnipc_2019\ngnipc_2020\ngnipc_2021\n\n\n\n\n0\nAfghanistan\n45.9672\n46.6631\n47.5955\n51.4664\n51.4945\n52.5442\n53.2433\n53.6342\n52.9431\n...\n2125.862821\n2193.553936\n2178.507021\n2101.589319\n2077.566899\n2085.487571\n2054.939895\n2097.889450\n1997.852149\n1824.190915\n\n\n1\nAngola\n41.8933\n43.8127\n42.2088\n42.1009\n43.4217\n45.8491\n46.0329\n46.3065\n45.0570\n...\n7280.845666\n7478.104777\n7704.231949\n7652.656486\n7189.426672\n6861.575738\n6381.521946\n6082.746624\n5593.142060\n5465.617791\n\n\n2\nAlbania\n73.1439\n73.3776\n73.7148\n73.9391\n74.1313\n74.3616\n74.5923\n73.9039\n74.9899\n...\n11146.263030\n11552.982470\n11691.648290\n12016.297600\n12484.624200\n12802.148310\n13302.705960\n13485.311240\n12996.762910\n14131.110390\n\n\n3\nAndorra\n78.4063\n77.9805\n80.3241\n78.6633\n82.6380\n78.9616\n80.3340\n80.9439\n79.4259\n...\n47126.814610\n46385.095200\n48483.720320\n49936.874540\n52267.738320\n52650.225760\n53483.306630\n54465.047400\n47878.666640\n51166.626610\n\n\n4\nUnited Arab Emirates\n71.9004\n72.2414\n72.3062\n72.5213\n72.5982\n72.6945\n72.7674\n72.9367\n73.0658\n...\n57445.954750\n60005.695360\n62573.505310\n65577.512240\n66881.329740\n67667.508460\n67195.095230\n68590.900940\n63016.401220\n62573.591810\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n190\nSamoa\n67.6584\n67.8814\n68.3855\n68.7009\n68.9795\n69.2984\n69.5877\n69.8746\n70.1711\n...\n5683.651395\n5675.889504\n5757.486050\n6222.783552\n6401.830980\n6287.673021\n6280.573936\n6356.988690\n5812.868328\n5307.953374\n\n\n191\nYemen\n58.6994\n59.0490\n59.4283\n59.8595\n59.7135\n60.4532\n60.5678\n61.1193\n61.3748\n...\n3152.900024\n3212.651579\n2775.842952\n1785.788608\n1494.230811\n1302.425254\n1341.656234\n1349.567046\n1370.601082\n1314.270189\n\n\n192\nSouth Africa\n63.3753\n63.2649\n63.3414\n63.0447\n62.6118\n62.2616\n61.4593\n60.8053\n60.0008\n...\n13602.253520\n13732.252520\n13700.834560\n13694.728110\n13545.358590\n13475.988210\n13491.221790\n13366.474640\n12449.671040\n12948.373250\n\n\n193\nZambia\n47.9263\n47.0971\n46.5119\n46.2094\n45.8543\n45.5534\n45.2326\n44.9446\n44.7011\n...\n3333.576512\n3389.478940\n3263.039162\n3403.471444\n3237.505650\n3330.552717\n3418.096158\n3365.410652\n3178.619722\n3217.767739\n\n\n194\nZimbabwe\n59.4264\n58.0911\n56.4354\n54.4264\n52.5878\n50.5310\n48.9551\n47.9933\n46.8192\n...\n3618.629526\n3632.111591\n3644.856047\n3638.532892\n3606.750671\n3728.918785\n3864.012419\n3674.564482\n3654.289051\n3809.887158\n\n\n\n\n195 rows × 129 columns"
  },
  {
    "objectID": "posts/global_socioecon_indicators/index.html#melt-dataset",
    "href": "posts/global_socioecon_indicators/index.html#melt-dataset",
    "title": "Global Socio-Economic Factors Part 1: Data Processing",
    "section": "Melt Dataset",
    "text": "Melt Dataset\nWe like the earlier form of the dataset, but we want something better; considering that there are too many columns that are hard to read. Let’s melt it!\nBasically, it will involve extracting the year out of the life_exp, hdi, etc. variables and having it as a separate column, with the value of that year displayed next to it. Finally we’re going to use that as a key to merge all the variables together.\nFinally each row will take the following form…\n\n# Melt the dataset\nlife_exp_year_columns = tuple(f'le_{year}' for year in range(1990, 2022))\nhdi_year_columns = tuple(f'hdi_{year}' for year in range(1990, 2022))\ngnipc_year_columns = tuple(f'gnipc_{year}' for year in range(1990, 2022))\nco2_year_columns = tuple(f'co2_prod_{year}' for year in range(1990, 2022))\n\ndef melt_df_by_var(df, value_name, value_vars):\n    melted = df.melt('Country', value_vars=value_vars, var_name='year', value_name=value_name)\n    melted.year = melted.year.str.slice(-4).astype(int)\n    return melted\n\nmelted_time_series = melt_df_by_var(time_series_data, 'life_exp', life_exp_year_columns)\\\n.merge(melt_df_by_var(time_series_data, 'hdi_index', hdi_year_columns), on=('Country', 'year'))\\\n.merge(melt_df_by_var(time_series_data, 'co2', co2_year_columns), on=('Country', 'year'))\\\n.merge(melt_df_by_var(time_series_data, 'gnipc', gnipc_year_columns), on=('Country', 'year'))\n\nmelted_time_series\n\n\n\n\n\n\n\n\nCountry\nyear\nlife_exp\nhdi_index\nco2\ngnipc\n\n\n\n\n0\nAfghanistan\n1990\n45.9672\n0.273\n0.209727\n2684.550019\n\n\n1\nAngola\n1990\n41.8933\nNaN\n0.429586\n4845.706901\n\n\n2\nAlbania\n1990\n73.1439\n0.647\n1.656902\n4742.215529\n\n\n3\nAndorra\n1990\n78.4063\nNaN\n7.461153\n43773.146500\n\n\n4\nUnited Arab Emirates\n1990\n71.9004\n0.728\n28.277672\n102433.136000\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n6235\nSamoa\n2021\n72.7675\n0.707\n1.238975\n5307.953374\n\n\n6236\nYemen\n2021\n63.7534\n0.455\n0.327510\n1314.270189\n\n\n6237\nSouth Africa\n2021\n62.3410\n0.713\n7.620420\n12948.373250\n\n\n6238\nZambia\n2021\n61.2234\n0.565\n0.357535\n3217.767739\n\n\n6239\nZimbabwe\n2021\n59.2531\n0.593\n0.708562\n3809.887158\n\n\n\n\n6240 rows × 6 columns\n\n\n\nFinally writing it to disk\n\nmelted_time_series.to_csv(data_folder/'processed/time_series.csv', index=False)"
  },
  {
    "objectID": "posts/global_socioecon_indicators/index.html#conclusion",
    "href": "posts/global_socioecon_indicators/index.html#conclusion",
    "title": "Global Socio-Economic Factors Part 1: Data Processing",
    "section": "Conclusion",
    "text": "Conclusion\nThis finalizes our steps to process the data. We’ll do a few more preprocessing steps necessary before modelling in the next chapter. Stay Tuned."
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html",
    "href": "posts/bahrain-renewable-2022/index.html",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "",
    "text": "In this notebook, we shall take a look at solar and wind power output for the year 2022. The paper titled Evaluating solar and wind electricity production in the Kingdom of Bahrain to combat climate change, remarks on readings taken for the whole year, and has even sourced data for the weather conditions for the whole year (though this may seem unsubstantial at first considering they are monthly averages)\nIn this notebook, we will try to find correlations between the weather variables and the power-output in the dataset, and ultimately, find predictors we can use to predict solar/wind power. We will look at both power outputs separately."
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#import-packages-setup",
    "href": "posts/bahrain-renewable-2022/index.html#import-packages-setup",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Import Packages & Setup",
    "text": "Import Packages & Setup\n\n\nCode\n# IMPORT PACKAGES\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\n\n\n\n\nCode\n# For the Kaggle Environment, please uncomment below\n# kaggle_root = '/kaggle/input/bahrain-solar-research-2022' \n# root_path = !ls\ndata_path = Path('data/')"
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#load-target-variables-solar-wind-power-output",
    "href": "posts/bahrain-renewable-2022/index.html#load-target-variables-solar-wind-power-output",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Load Target Variables (Solar + Wind Power Output)",
    "text": "Load Target Variables (Solar + Wind Power Output)\n\n\nCode\nsolar_file = data_path/'solar_daily_2022.csv'\nsolar = pd.read_csv(solar_file)\nsolar.date = pd.to_datetime(solar.date)\nsolar.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 365 entries, 0 to 364\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   date    365 non-null    datetime64[ns]\n 1   value   365 non-null    float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 5.8 KB\n\n\n\n\nCode\nwind = pd.read_csv(data_path/'wind_daily_2022.csv')\nwind.date = pd.to_datetime(wind.date)\nwind.info()\nwind\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 333 entries, 0 to 332\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype         \n---  ------  --------------  -----         \n 0   date    333 non-null    datetime64[ns]\n 1   value   333 non-null    float64       \ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 5.3 KB\n\n\n\n\n\n\n\n\n\ndate\nvalue\n\n\n\n\n0\n2022-01-31\n79.75\n\n\n1\n2022-02-01\n211.10\n\n\n2\n2022-02-02\n195.53\n\n\n3\n2022-02-03\n51.82\n\n\n4\n2022-02-04\n83.44\n\n\n...\n...\n...\n\n\n328\n2022-12-27\n120.22\n\n\n329\n2022-12-28\n926.98\n\n\n330\n2022-12-29\n462.69\n\n\n331\n2022-12-30\n273.46\n\n\n332\n2022-12-31\n170.90\n\n\n\n\n333 rows × 2 columns"
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#load-rest-of-the-data",
    "href": "posts/bahrain-renewable-2022/index.html#load-rest-of-the-data",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Load Rest of the Data",
    "text": "Load Rest of the Data\n\n\nCode\nhumidity_monthly = pd.read_csv(data_path/'humidity_percent_monthly_2022.csv')\ntemp_monthly = pd.read_csv(data_path/'temp_in_celcius_monthly_2022.csv')\nwind_speed_monthly = pd.read_csv(data_path/'wind_speed_metre_per_second_monthly_2022.csv')\nhumidity_monthly.info()\ntemp_monthly.info()\nwind_speed_monthly.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12 entries, 0 to 11\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   month   12 non-null     int64  \n 1   value   12 non-null     float64\ndtypes: float64(1), int64(1)\nmemory usage: 320.0 bytes\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12 entries, 0 to 11\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   month   12 non-null     int64  \n 1   value   12 non-null     float64\ndtypes: float64(1), int64(1)\nmemory usage: 320.0 bytes\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12 entries, 0 to 11\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   month   12 non-null     int64  \n 1   value   12 non-null     float64\ndtypes: float64(1), int64(1)\nmemory usage: 320.0 bytes"
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#plots",
    "href": "posts/bahrain-renewable-2022/index.html#plots",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Plots",
    "text": "Plots\nIt always help to view plots of the dataset and how it varies in time.\n\nDaily Power Output (Solar)\n\n\nCode\nplt.figure(figsize=(30, 7))\nsns.lineplot(data=solar, x='date', y='value')\n\n\n&lt;AxesSubplot: xlabel='date', ylabel='value'&gt;\n\n\n\n\n\n\n\nDaily Power Output (Wind)\n\n\nCode\nplt.figure(figsize=(30, 7))\nsns.lineplot(data=wind, x='date', y='value')\n\n\n&lt;AxesSubplot: xlabel='date', ylabel='value'&gt;\n\n\n\n\n\n\n\nWeather Conditions (Monthly Basis)\n\nTemperature (in Celcius)\n\n\nCode\n# plt.figure(figsize=(30, 7))\nsns.lineplot(data=temp_monthly, x='month', y='value')\n\n\n&lt;AxesSubplot: xlabel='month', ylabel='value'&gt;\n\n\n\n\n\n\n\nWind Speed (m/s)\n\n\nCode\nsns.lineplot(data=wind_speed_monthly, x='month', y='value')\n\n\n&lt;AxesSubplot: xlabel='month', ylabel='value'&gt;\n\n\n\n\n\n\n\nHumidity (%)\n\n\nCode\n# plt.figure(figsize=(30, 7))\nsns.lineplot(data=humidity_monthly, x='month', y='value')\n\n\n&lt;AxesSubplot: xlabel='month', ylabel='value'&gt;"
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#combine-data",
    "href": "posts/bahrain-renewable-2022/index.html#combine-data",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Combine Data",
    "text": "Combine Data\nIn this section, we will merge both the target variables and the features, to have one single dataframe to work with.\n\n\nCode\nwind_renamed = wind.rename(columns={'value': 'wind_power'})\nsolar_renamed = solar.rename(columns={'value': 'solar_power'})\ntarget_data = wind_renamed.merge(solar_renamed, how='outer', on='date')\ntarget_data = target_data.sort_values('date').reset_index(drop=True)\ntarget_data\n\n\n\n\n\n\n\n\n\ndate\nwind_power\nsolar_power\n\n\n\n\n0\n2022-01-01\nNaN\n527.55\n\n\n1\n2022-01-02\nNaN\n762.18\n\n\n2\n2022-01-03\nNaN\n1343.82\n\n\n3\n2022-01-04\nNaN\n1469.36\n\n\n4\n2022-01-05\nNaN\n1588.27\n\n\n...\n...\n...\n...\n\n\n360\n2022-12-27\n120.22\n1132.27\n\n\n361\n2022-12-28\n926.98\n1395.73\n\n\n362\n2022-12-29\n462.69\n1307.64\n\n\n363\n2022-12-30\n273.46\n746.64\n\n\n364\n2022-12-31\n170.90\n865.91\n\n\n\n\n365 rows × 3 columns\n\n\n\nThe wind power variable has missing values, let’s fill in the gap and see difference as different plots Wind power data with & without interpolation\n\nWind Power (Before Interpolation)\n\n\nCode\nplt.figure(figsize=(30, 7))\nsns.lineplot(target_data, x='date', y='wind_power')\n\n\n&lt;AxesSubplot: xlabel='date', ylabel='wind_power'&gt;\n\n\n\n\n\n\n\nWind Power (After Interpolation)\n\n\nCode\ntarget_data.wind_power = target_data.wind_power.interpolate(limit_direction='both')\nplt.figure(figsize=(30, 7))\nsns.lineplot(target_data, x='date', y='wind_power')\n\n\n&lt;AxesSubplot: xlabel='date', ylabel='wind_power'&gt;"
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#correlation-w-internal-data",
    "href": "posts/bahrain-renewable-2022/index.html#correlation-w-internal-data",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Correlation w/ Internal Data",
    "text": "Correlation w/ Internal Data\nWhat we will do now, is to find correlations between the weather variables and the power output (wind & solar). Considering that the weather conditions are only recorded once every month as opposed to the power output which is recorded on a daily basis; we will approximate the daily weather measurements with the month’s value i.e. if July had an average of 40 deg. celcius, then we will assume that every day of July had the same temperature.\nBefore that however, let’s combine these weather variables…\n\n\nCode\ntarget_data['month'] = target_data.date.dt.month\ntarget_data\n\n\n\n\n\n\n\n\n\ndate\nwind_power\nsolar_power\nmonth\n\n\n\n\n0\n2022-01-01\n79.75\n527.55\n1\n\n\n1\n2022-01-02\n79.75\n762.18\n1\n\n\n2\n2022-01-03\n79.75\n1343.82\n1\n\n\n3\n2022-01-04\n79.75\n1469.36\n1\n\n\n4\n2022-01-05\n79.75\n1588.27\n1\n\n\n...\n...\n...\n...\n...\n\n\n360\n2022-12-27\n120.22\n1132.27\n12\n\n\n361\n2022-12-28\n926.98\n1395.73\n12\n\n\n362\n2022-12-29\n462.69\n1307.64\n12\n\n\n363\n2022-12-30\n273.46\n746.64\n12\n\n\n364\n2022-12-31\n170.90\n865.91\n12\n\n\n\n\n365 rows × 4 columns\n\n\n\n\n\nCode\nwind_speed_monthly_renamed = wind_speed_monthly.rename(columns={'value': 'wind_speed'})\nhumidity_monthly_renamed = humidity_monthly.rename(columns={'value': 'humidity'})\ntemp_monthly_renamed = temp_monthly.rename(columns={'value': 'temp'})\n\n_ = target_data.merge(wind_speed_monthly_renamed, on='month', how='inner')\n_ = _.merge(humidity_monthly_renamed, on='month', how='inner')\n_ = _.merge(temp_monthly_renamed, on='month', how='inner')\n\n\nmerged_all = _.copy()\nmerged_all\n\n\n\n\n\n\n\n\n\ndate\nwind_power\nsolar_power\nmonth\nwind_speed\nhumidity\ntemp\n\n\n\n\n0\n2022-01-01\n79.75\n527.55\n1\n5.29\n68.57\n17.99\n\n\n1\n2022-01-02\n79.75\n762.18\n1\n5.29\n68.57\n17.99\n\n\n2\n2022-01-03\n79.75\n1343.82\n1\n5.29\n68.57\n17.99\n\n\n3\n2022-01-04\n79.75\n1469.36\n1\n5.29\n68.57\n17.99\n\n\n4\n2022-01-05\n79.75\n1588.27\n1\n5.29\n68.57\n17.99\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n360\n2022-12-27\n120.22\n1132.27\n12\n4.34\n66.92\n22.45\n\n\n361\n2022-12-28\n926.98\n1395.73\n12\n4.34\n66.92\n22.45\n\n\n362\n2022-12-29\n462.69\n1307.64\n12\n4.34\n66.92\n22.45\n\n\n363\n2022-12-30\n273.46\n746.64\n12\n4.34\n66.92\n22.45\n\n\n364\n2022-12-31\n170.90\n865.91\n12\n4.34\n66.92\n22.45\n\n\n\n\n365 rows × 7 columns\n\n\n\nCorrelation Plot\n\n\nCode\nvariables = ['wind_power', 'solar_power', 'wind_speed', 'humidity', 'temp']\ncorr = merged_all[variables].corr()\nsns.heatmap(corr, annot=True)\n\n\n&lt;AxesSubplot: &gt;\n\n\n\n\n\n\nResults\n\nFor wind power, we can see humidity & wind speed have the highest correlations\nFor solar power, we don’t see any strong correlations\nNotable features that correlate highly with one another are\n\n(temp, humidity)\n(wind_speed, humidity)"
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#sweetviz-package",
    "href": "posts/bahrain-renewable-2022/index.html#sweetviz-package",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Sweetviz Package",
    "text": "Sweetviz Package\nThe Sweetviz library is popular package to automate the analysis and display a competent report of the dataset. We will use this package with our original dataset, and with our extended set of features (manama.csv).\n\n\nCode\nimport sweetviz as sv\n\n\n\nWind Power Report\n\n\nCode\nfrom IPython.display import display, Markdown\n\n\n\n\nCode\nanalyze_report = sv.analyze([merged_all,'Wind Power Report'], 'wind_power')\nfilepath = 'reports/wind_power_internal_data.html'\nanalyze_report.show_html(filepath=filepath, layout='vertical', open_browser=False)\ndisplay(Markdown(f'**To view the report, [CLICK HERE]({filepath})**'))\n\n\nFeature: wind_power (TARGET)                 |█▎        | [ 12%]   00:00 -&gt; (00:00 left)Done! Use 'show' commands to display/save.   |██████████| [100%]   00:00 -&gt; (00:00 left)\n\n\nReport reports/wind_power_internal_data.html was generated.\n\n\nTo view the report, CLICK HERE\n\n\n\nConclusion\nWe can tell from this report, that there are a few variables, mainly wind_speed and humidity that corrleate highly with wind_power. We can use these as predictors\n\n\n\nSolar Power Report\n\n\nCode\nanalyze_report = sv.analyze([merged_all,'Solar Power Report'], 'solar_power')\nfilepath = 'reports/solar_power_internal_data.html'\nanalyze_report.show_html(filepath=filepath, layout='vertical', open_browser=False)\ndisplay(Markdown(f'**To view the report, [CLICK HERE]({filepath})**'))\n\n\nDone! Use 'show' commands to display/save.   |██████████| [100%]   00:00 -&gt; (00:00 left)\n\n\nReport reports/solar_power_internal_data.html was generated.\n\n\nTo view the report, CLICK HERE\n\n\n\nConclusion\nNo suitable predictors can be found for solar_power. We don’t attempt to build a model around it."
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#load-extended-weather-data",
    "href": "posts/bahrain-renewable-2022/index.html#load-extended-weather-data",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Load Extended Weather Data",
    "text": "Load Extended Weather Data\n\n\nCode\nexternal_weather_df = pd.read_csv(data_path/'manama.csv')\nexternal_weather_df.rename(columns={'windspeed': 'wind_speed', 'datetime': 'date'}, inplace=True)\nexternal_weather_df.date = pd.to_datetime(external_weather_df.date)\nexternal_weather_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 365 entries, 0 to 364\nData columns (total 33 columns):\n #   Column            Non-Null Count  Dtype         \n---  ------            --------------  -----         \n 0   name              365 non-null    object        \n 1   date              365 non-null    datetime64[ns]\n 2   tempmax           365 non-null    float64       \n 3   tempmin           365 non-null    float64       \n 4   temp              365 non-null    float64       \n 5   feelslikemax      365 non-null    float64       \n 6   feelslikemin      365 non-null    float64       \n 7   feelslike         365 non-null    float64       \n 8   dew               365 non-null    float64       \n 9   humidity          365 non-null    float64       \n 10  precip            365 non-null    float64       \n 11  precipprob        365 non-null    int64         \n 12  precipcover       365 non-null    float64       \n 13  preciptype        26 non-null     object        \n 14  snow              351 non-null    float64       \n 15  snowdepth         1 non-null      float64       \n 16  windgust          356 non-null    float64       \n 17  wind_speed        365 non-null    float64       \n 18  winddir           365 non-null    float64       \n 19  sealevelpressure  365 non-null    float64       \n 20  cloudcover        365 non-null    float64       \n 21  visibility        365 non-null    float64       \n 22  solarradiation    365 non-null    float64       \n 23  solarenergy       365 non-null    float64       \n 24  uvindex           365 non-null    int64         \n 25  severerisk        356 non-null    float64       \n 26  sunrise           365 non-null    object        \n 27  sunset            365 non-null    object        \n 28  moonphase         365 non-null    float64       \n 29  conditions        365 non-null    object        \n 30  description       365 non-null    object        \n 31  icon              365 non-null    object        \n 32  stations          365 non-null    object        \ndtypes: datetime64[ns](1), float64(22), int64(2), object(8)\nmemory usage: 94.2+ KB\n\n\n\n\nCode\nexternal_weather_df.columns\n\n\nIndex(['name', 'date', 'tempmax', 'tempmin', 'temp', 'feelslikemax',\n       'feelslikemin', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob',\n       'precipcover', 'preciptype', 'snow', 'snowdepth', 'windgust',\n       'wind_speed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility',\n       'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'sunrise',\n       'sunset', 'moonphase', 'conditions', 'description', 'icon', 'stations'],\n      dtype='object')\n\n\n\n\nCode\n# important_cols = ['date', 'temp', 'dew', 'humidity', 'precip', 'wind_speed', 'winddir', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'sunrise', 'sunset']\nimportant_cols = ['date', 'temp', 'humidity', 'wind_speed']\nexternal_weather_df_selected = external_weather_df[important_cols].copy()\nexternal_weather_df_selected.loc[:, 'wind_speed'] = external_weather_df_selected.wind_speed.apply(lambda x: x*1000/(60*60)) # convert from km/h to m/s\nexternal_weather_df_selected\n\n\n\n\n\n\n\n\n\ndate\ntemp\nhumidity\nwind_speed\n\n\n\n\n0\n2022-01-01\n19.4\n82.8\n7.638889\n\n\n1\n2022-01-02\n20.6\n85.5\n9.638889\n\n\n2\n2022-01-03\n18.2\n76.5\n6.722222\n\n\n3\n2022-01-04\n16.9\n61.6\n9.722222\n\n\n4\n2022-01-05\n15.3\n49.4\n9.250000\n\n\n...\n...\n...\n...\n...\n\n\n360\n2022-12-27\n19.6\n70.7\n4.777778\n\n\n361\n2022-12-28\n17.6\n63.1\n12.833333\n\n\n362\n2022-12-29\n18.0\n67.4\n10.222222\n\n\n363\n2022-12-30\n18.1\n65.2\n7.138889\n\n\n364\n2022-12-31\n18.3\n66.5\n6.611111\n\n\n\n\n365 rows × 4 columns\n\n\n\n\n\nCode\n# combine with target data\nmerged_with_external = target_data.merge(external_weather_df_selected, on='date')\nassert merged_with_external.isnull().sum().sum() == 0\nmerged_with_external\n\n\n\n\n\n\n\n\n\ndate\nwind_power\nsolar_power\nmonth\ntemp\nhumidity\nwind_speed\n\n\n\n\n0\n2022-01-01\n79.75\n527.55\n1\n19.4\n82.8\n7.638889\n\n\n1\n2022-01-02\n79.75\n762.18\n1\n20.6\n85.5\n9.638889\n\n\n2\n2022-01-03\n79.75\n1343.82\n1\n18.2\n76.5\n6.722222\n\n\n3\n2022-01-04\n79.75\n1469.36\n1\n16.9\n61.6\n9.722222\n\n\n4\n2022-01-05\n79.75\n1588.27\n1\n15.3\n49.4\n9.250000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n360\n2022-12-27\n120.22\n1132.27\n12\n19.6\n70.7\n4.777778\n\n\n361\n2022-12-28\n926.98\n1395.73\n12\n17.6\n63.1\n12.833333\n\n\n362\n2022-12-29\n462.69\n1307.64\n12\n18.0\n67.4\n10.222222\n\n\n363\n2022-12-30\n273.46\n746.64\n12\n18.1\n65.2\n7.138889\n\n\n364\n2022-12-31\n170.90\n865.91\n12\n18.3\n66.5\n6.611111\n\n\n\n\n365 rows × 7 columns"
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#sweetviz",
    "href": "posts/bahrain-renewable-2022/index.html#sweetviz",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Sweetviz",
    "text": "Sweetviz\n\nWind Power Report\n\n\nCode\nanalyze_report = sv.analyze([merged_with_external,'Wind Power Report'], 'wind_power')\nfilepath = 'reports/wind_power_external_data.html'\nanalyze_report.show_html(filepath=filepath, layout='vertical', open_browser=False)\ndisplay(Markdown(f'**To view the report, [CLICK HERE]({filepath})**'))\n\n\nDone! Use 'show' commands to display/save.   |██████████| [100%]   00:00 -&gt; (00:00 left)\n\n\nReport reports/wind_power_external_data.html was generated.\n\n\nTo view the report, CLICK HERE\n\n\n\n\nSolar Power Report\n\n\nCode\n\nanalyze_report = sv.analyze([merged_with_external,'Solar Power Report'], 'solar_power')\nfilepath = 'reports/solar_power_external_data.html'\nanalyze_report.show_html(filepath=filepath, layout='vertical', open_browser=False)\ndisplay(Markdown(f'**To view the report, [CLICK HERE]({filepath})**'))\n\n\nDone! Use 'show' commands to display/save.   |██████████| [100%]   00:00 -&gt; (00:00 left)\n\n\nReport reports/solar_power_external_data.html was generated.\n\n\nTo view the report, CLICK HERE"
  },
  {
    "objectID": "posts/bahrain-renewable-2022/index.html#using-all-important-features-from-the-extended-dataset",
    "href": "posts/bahrain-renewable-2022/index.html#using-all-important-features-from-the-extended-dataset",
    "title": "Bahrain’s Solar & Wind Power Data 2022",
    "section": "Using All Important Features from the Extended Dataset",
    "text": "Using All Important Features from the Extended Dataset\n\n\nCode\nimportant_cols = ['date', 'temp', 'dew', 'humidity', 'precip', 'wind_speed', 'winddir', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'sunrise', 'sunset']\nexternal_weather_df_selected = external_weather_df[important_cols].copy()\n# convert from km/h to ms/\nexternal_weather_df_selected.loc[:, 'wind_speed'] = external_weather_df_selected.wind_speed.apply(lambda x: x*1000/(60*60)) # convert from km/h to m/s\n\ndef get_diff_in_hours_and_mins(x):\n    diff = x['sunset']-x['sunrise']\n\n    return diff.total_seconds()/60\n\n# get sunlight duration\nexternal_weather_df_selected.sunrise = pd.to_datetime(external_weather_df_selected.sunrise)\nexternal_weather_df_selected.sunset = pd.to_datetime(external_weather_df_selected.sunset)\nexternal_weather_df_selected['sunlight_duration_in_secs'] = external_weather_df_selected.apply(get_diff_in_hours_and_mins, axis=1)\nexternal_weather_df_selected.drop(['sunrise', 'sunset'], axis=1, inplace=True)\n\n\nLet’s take a look at the plot of the Sunlight Duration, to notice any relationships\n\n\nCode\nsns.barplot(external_weather_df_selected, x='date', y='sunlight_duration_in_secs')\n\n\n&lt;AxesSubplot: xlabel='date', ylabel='sunlight_duration_in_secs'&gt;\n\n\n\n\n\n\n\nCode\n# combine with target data\nmerged_with_external = target_data.merge(external_weather_df_selected, on='date')\nassert merged_with_external.isnull().sum().sum() == 0\nmerged_with_external\n\n\n\n\n\n\n\n\n\ndate\nwind_power\nsolar_power\nmonth\ntemp\ndew\nhumidity\nprecip\nwind_speed\nwinddir\nvisibility\nsolarradiation\nsolarenergy\nuvindex\nsunlight_duration_in_secs\n\n\n\n\n0\n2022-01-01\n79.75\n527.55\n1\n19.4\n16.4\n82.8\n15.9\n7.638889\n55.0\n10.2\n127.7\n11.2\n6\n631.716667\n\n\n1\n2022-01-02\n79.75\n762.18\n1\n20.6\n18.0\n85.5\n3.1\n9.638889\n106.2\n9.9\n103.9\n9.0\n4\n632.133333\n\n\n2\n2022-01-03\n79.75\n1343.82\n1\n18.2\n13.9\n76.5\n0.0\n6.722222\n298.7\n7.4\n175.2\n15.2\n7\n632.566667\n\n\n3\n2022-01-04\n79.75\n1469.36\n1\n16.9\n9.4\n61.6\n0.0\n9.722222\n304.2\n10.3\n183.5\n15.9\n7\n633.050000\n\n\n4\n2022-01-05\n79.75\n1588.27\n1\n15.3\n4.7\n49.4\n0.0\n9.250000\n301.1\n11.5\n190.6\n16.5\n7\n633.550000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n360\n2022-12-27\n120.22\n1132.27\n12\n19.6\n14.1\n70.7\n0.3\n4.777778\n198.7\n8.3\n157.1\n13.5\n6\n630.150000\n\n\n361\n2022-12-28\n926.98\n1395.73\n12\n17.6\n10.4\n63.1\n0.1\n12.833333\n306.0\n9.9\n127.9\n10.9\n6\n630.366667\n\n\n362\n2022-12-29\n462.69\n1307.64\n12\n18.0\n11.7\n67.4\n0.0\n10.222222\n310.2\n8.4\n163.2\n14.0\n6\n630.633333\n\n\n363\n2022-12-30\n273.46\n746.64\n12\n18.1\n11.4\n65.2\n0.0\n7.138889\n305.8\n10.1\n123.2\n10.7\n5\n630.916667\n\n\n364\n2022-12-31\n170.90\n865.91\n12\n18.3\n11.9\n66.5\n0.1\n6.611111\n304.3\n10.1\n141.8\n12.0\n6\n631.250000\n\n\n\n\n365 rows × 15 columns\n\n\n\n\n\nCode\nanalyze_report = sv.analyze([merged_with_external,'Wind Power Report'], 'wind_power')\nfilepath = 'reports/Extra_Features_wind_power_external_data.html'\nanalyze_report.show_html(filepath=filepath, layout='vertical', open_browser=False)\ndisplay(Markdown(f'**To view the report, [CLICK HERE](\"{filepath}\")**'))\n\n\nDone! Use 'show' commands to display/save.   |██████████| [100%]   00:01 -&gt; (00:00 left)\n\n\nReport reports/Extra_Features_wind_power_external_data.html was generated.\n\n\nTo view the report, CLICK HERE\n\n\n\n\nCode\n\nanalyze_report = sv.analyze([merged_with_external,'Solar Power Report'], 'solar_power')\nfilepath = 'reports/Extra_Features_solar_power_external_data.html'\nanalyze_report.show_html(filepath=filepath, layout='vertical', open_browser=False)\ndisplay(Markdown(f'**To view the report, [CLICK HERE](\"{filepath}\")**'))\n\n\nDone! Use 'show' commands to display/save.   |██████████| [100%]   00:01 -&gt; (00:00 left)\n\n\nReport reports/Extra_Features_solar_power_external_data.html was generated.\n\n\nTo view the report, CLICK HERE"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html",
    "href": "posts/foodies-guide-to-finding-a-home/index.html",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "",
    "text": "I am a foodie looking for a place stay in Bahrain. I want to study certain areas in Bahrain and the kind of restaurants that surround them.\nI think that a lot of people, not just the youth could benefit from this, since the issue isn’t just finding a decent place to stay in Bahrain, but finding one that best serves their culinary interests perhaps.\nI mean there are obviously much better factors to look at besides food. However for this problem I want to stick to what I can gain from Foursquare with a free license. Thus, by neatly categorizing areas based on their attributes such as frequency of coffee shops, closeness to malls etc; I can can make a better guesstimate of where they might stay.\nFoursquare allows us to grab information on venues surrounding a given location, and therefore we will look into the most frequent kind of venues surrounding a given area, and cluster areas them on that.\nSo let’s get started!"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#scrap-bahrain-citiestown-data-from-wikipedia",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#scrap-bahrain-citiestown-data-from-wikipedia",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Scrap Bahrain Cities/Town Data from Wikipedia",
    "text": "Scrap Bahrain Cities/Town Data from Wikipedia\nI need to scrap data from Wikipedia to lookup towns and cities in Bahrain. We’re going to use the popular webscraper Beautiful Soup to do that.\n\nurl = 'https://en.wikipedia.org/wiki/Category:Populated_places_in_Bahrain'\nhtml_doc = requests.get(url).text # Get HTML Doc\nsoup = BeautifulSoup(html_doc, 'html.parser') # Parse using bs4\nblocks = soup.find_all(\"div\", {\"class\": \"mw-category-group\"})[1:]\n\nbh_data=[]\nfor block in blocks:\n    places = block.find('ul').find_all('li')\n    for place in places:\n        bh_data.append(place.a.text.split(',')[0])\n\nbh_data = pd.DataFrame(bh_data, columns=['Area'])\nremove_places = ['Rifa and Southern Region', 'Northern City'] # Exclude these places\nbh_data = bh_data[bh_data['Area'].apply(lambda item : item not in remove_places)].reset_index(drop=True)\nbh_data.head(5)\n\n\n\n\n\n\n\n\nArea\n\n\n\n\n0\nA'ali\n\n\n1\nAbu Baham\n\n\n2\nAbu Saiba\n\n\n3\nAl Garrya\n\n\n4\nAl Hajar\n\n\n\n\n\n\n\n\n\nSo there are about 82 areas in Bahrain to study."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#retrieving-coordinates-via-a-geocoder",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#retrieving-coordinates-via-a-geocoder",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Retrieving Coordinates via a Geocoder",
    "text": "Retrieving Coordinates via a Geocoder\nAfter that, we need to geocode them; convert them from a simple address to latitude & longitude values.\nPopular geocoders like OpenStreetMap & Map Quest will be used.\n\nimport os\napikey = \"API-KEY-XXXXXXXXXXX\"\nimport geocoder\n\nlats = []\nlngs = []\nfor city in bh_data['Area']:\n    geocoder_type = 'osm'\n    try:\n        g = geocoder.osm(f\"{city}, Bahrain\", key=apikey)\n        geodata = g.json\n        lats.append(geodata['lat'])\n    except:\n        geocoder_type = 'MAPQUEST'\n        g = geocoder.mapquest(f\"{city}, Bahrain\", key=apikey)\n        geodata = g.json\n        lats.append(geodata['lat'])\n    lngs.append(geodata['lng'])\n    print(city, \"|\", geocoder_type)\nbh_data['Latitude'] = lats\nbh_data['Longitude'] = lngs\n\nThese are the first few of them that were geocoded!\n\n\n\n\n\n\n\n\n\nArea\nLatitude\nLongitude\n\n\n\n\n0\nA'ali\n26.154454\n50.527364\n\n\n1\nAbu Baham\n26.205737\n50.541668\n\n\n2\nAbu Saiba\n30.325299\n48.266157\n\n\n3\nAl Garrya\n26.232690\n50.578110\n\n\n4\nAl Hajar\n26.225405\n50.590138"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#visualization-on-a-map",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#visualization-on-a-map",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Visualization on a Map",
    "text": "Visualization on a Map\nWe will now use Folium to visualize the map of Bahrain along with each area as points on the map\n\n# create map of Bahrain using latitude and longitude values\nlatitude, longitude = 26.0766404, 50.334118\n\nmap_bahrain = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, city in zip(bh_data['Latitude'], bh_data['Longitude'],\n                                           bh_data['Area']):\n    \n    label = folium.Popup(city, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=True).add_to(map_bahrain)  \nmap_bahrain\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#foursquare-exploring-areas-for-food-places",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#foursquare-exploring-areas-for-food-places",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Foursquare: Exploring Areas for Food Places",
    "text": "Foursquare: Exploring Areas for Food Places\nFuthermore, we’ll leverage the Foursquare API to gather the most common types of restaurants associated with an area within 500m of its center. We’ll then look at various food places and restaurants and extract their types for further analysis.\nNote: To filter only restaurants & food places, we will use the specific “Food” category ID : 4d4b7105d754a06374d81259\n\nfood_categoryId = \"4d4b7105d754a06374d81259\"\n\nAlright, let’s look at all food places surrouding the first area within a 500m radius\n\n\n… which happens to be A’ali\n\n\n\nradius = 500\nlat, lng = bh_data[['Latitude', 'Longitude']].iloc[0].values\n\nurl = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&categoryId={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, VERSION, lat, lng, food_categoryId, radius, LIMIT)\nresults = requests.get(url).json()\n\nLooking at the first food place in the results json, we get this output:\n\nresults['response']['venues'][0]\n\n{'id': '4e99da8f8231878c15393aa2',\n 'name': 'Costa Coffee',\n 'location': {'lat': 26.157464331750106,\n  'lng': 50.52587327276449,\n  'labeledLatLngs': [{'label': 'display',\n    'lat': 26.157464331750106,\n    'lng': 50.52587327276449}],\n  'distance': 366,\n  'cc': 'BH',\n  'city': 'Madīnat ‘Īsá',\n  'state': 'al Muḩāfaz̧ah Al Janūbīyah',\n  'country': 'البحرين',\n  'formattedAddress': ['Madīnat ‘Īsá', 'البحرين']},\n 'categories': [{'id': '4bf58dd8d48988d1e0931735',\n   'name': 'Coffee Shop',\n   'pluralName': 'Coffee Shops',\n   'shortName': 'Coffee Shop',\n   'icon': {'prefix': 'https://ss3.4sqi.net/img/categories_v2/food/coffeeshop_',\n    'suffix': '.png'},\n   'primary': True}],\n 'referralId': 'v-1631999729',\n 'hasPerk': False}\n\n\n\n\nThie first venue is Costa Coffee, and has a category: Coffee Shop\n\n\nSo now let’s build a helpful function to extract the category of each food place. We’ll use the same area as an example.\n\n# function that extracts the category of the restaurant\ndef get_category_type(row):\n    try:\n        categories_list = row['categories']\n    except:\n        categories_list = row['venue.categories']\n        \n    if len(categories_list) == 0:\n        return None\n    else:\n        return categories_list[0]['name']\n    \nvenues = results['response']['venues']\n    \nnearby_food = pd.json_normalize(venues) # flatten JSON\n\n# filter columns\nfiltered_columns = ['name', 'categories', 'location.lat', 'location.lng']\nnearby_food = nearby_food.loc[:, filtered_columns]\n\n# filter the category for each row\nnearby_food['categories'] = nearby_food.apply(get_category_type, axis=1)\n\n# clean columns\nnearby_food.columns = [col.split(\".\")[-1] for col in nearby_food.columns]\n\nnearby_food.head()\n\n\n\n\n\n\n\n\nname\ncategories\nlat\nlng\n\n\n\n\n0\nCosta Coffee\nCoffee Shop\n26.157464\n50.525873\n\n\n1\nChilis Aali\nDiner\n26.152996\n50.526268\n\n\n2\nLoop Cafe\nCafé\n26.156017\n50.531527\n\n\n3\nHospital Resturant (كافيتيريا المستشفى)\nRestaurant\n26.153012\n50.526232\n\n\n4\nكفتيريا المستشفى\nRestaurant\n26.153455\n50.528375\n\n\n\n\n\n\n\n\n\nThese are some of them, in total it returns 19 food places around A’ali."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#exploring-all-areas",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#exploring-all-areas",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Exploring All Areas",
    "text": "Exploring All Areas\n\n\nWe’ve got still got 82 places to explore, so let’s create a function to do this task much faster.\n\n\n\ndef getNearbyFoods(names, latitudes, longitudes, radius=500):\n    food_categoryId = \"4d4b7105d754a06374d81259\"\n    foods_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n\n            \n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&categoryId={}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            food_categoryId,\n            radius, \n            LIMIT)\n            \n        # make the GET request\n        try:\n            results = requests.get(url).json()[\"response\"]['venues']\n        except:\n            print(results)\n            raise KeyError\n        \n        venue_list = []\n        # return only relevant information for each nearby food place\n        for v in results:\n            vname, vlat, vlng = v['name'], v['location']['lat'], v['location']['lng']\n            try:\n                vcategory = v['categories'][0]['name']\n                venue_list.append((name, \n                                    lat, \n                                    lng,\n                                    vname, \n                                    vlat,\n                                    vlng,\n                                    vcategory))\n            except:\n                continue\n        foods_list.append(venue_list)\n    nearby_foods = pd.DataFrame([item for venue_list in foods_list for item in venue_list])\n    nearby_foods.columns = ['Area', \n                  'Area Latitude', \n                  'Area Longitude',\n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_foods)\n\nWe run the above function on each area and create a new dataframe called bh_foods.\n\nbh_food = getNearbyFoods(bh_data['Area'], bh_data['Latitude'], \n                                   bh_data['Longitude'], 500)\n\n\nbh_food.head()\n\n\n\n\n\n\n\n\nArea\nArea Latitude\nArea Longitude\nVenue\nVenue Latitude\nVenue Longitude\nVenue Category\n\n\n\n\n0\nA'ali\n26.154454\n50.527364\nCosta Coffee\n26.157464\n50.525873\nCoffee Shop\n\n\n1\nA'ali\n26.154454\n50.527364\nChilis Aali\n26.152996\n50.526268\nDiner\n\n\n2\nA'ali\n26.154454\n50.527364\nLoop Cafe\n26.156017\n50.531527\nCafé\n\n\n3\nA'ali\n26.154454\n50.527364\nكفتيريا المستشفى\n26.153455\n50.528375\nRestaurant\n\n\n4\nA'ali\n26.154454\n50.527364\nHospital Resturant (كافيتيريا المستشفى)\n26.153012\n50.526232\nRestaurant\n\n\n\n\n\n\n\n\n\nThis gives us a whopping 1962 food places\n\n\nWe can study the count for each area…\n\nbh_food.groupby('Area').count().head()\n\n\n\n\n\n\n\n\nArea Latitude\nArea Longitude\nVenue\nVenue Latitude\nVenue Longitude\nVenue Category\n\n\nArea\n\n\n\n\n\n\n\n\n\n\nA'ali\n19\n19\n19\n19\n19\n19\n\n\nAbu Baham\n9\n9\n9\n9\n9\n9\n\n\nAl Daih\n50\n50\n50\n50\n50\n50\n\n\nAl Dair\n9\n9\n9\n9\n9\n9\n\n\nAl Garrya\n50\n50\n50\n50\n50\n50\n\n\n\n\n\n\n\nWe’ve trimmed out the remaining areas for brevity’s sake.\n\n\nWhat’s interesting to notice from this data, is that there are 88 unique categories for food.\n\n\n\n\nSome of them include: Coffee Shop, Diner, Café, Restaurant, Breakfast Spot and so on."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#most-common-food-places",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#most-common-food-places",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Most Common Food Places",
    "text": "Most Common Food Places\nOur solution relies on segmenting areas based on the most common type of food places within that area. This gives us an idea about the kind of area it is from a culinary point-of-view, and allowing us to make judgments on whether the food is ideal to our taste or not. We also want to factor in the total number of food places within an area since some places in Bahrain may not be ideal to live in if they don’t even have enough places to eat.\nUsing the dataframe bh_food, we form a one-hot encoding of the Venue Category field that produces new columns for each category. Each record in this table corresponds to a certain venue and a 1 is placed in the category field for that area. The only other field that is retained is the area name. We will call this bh_onehot.\n\n# one hot encoding\nbh_onehot = pd.get_dummies(bh_food[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\nbh_onehot = pd.concat([bh_food[['Area']], bh_onehot], axis=1) \n\nbh_onehot.head()\n\n\n\n\n\n\n\n\nArea\nAfghan Restaurant\nAfrican Restaurant\nAmerican Restaurant\nArepa Restaurant\nAsian Restaurant\nBBQ Joint\nBagel Shop\nBakery\nBistro\nBreakfast Spot\nBubble Tea Shop\nBuffet\nBurger Joint\nBurrito Place\nCafeteria\nCafé\nChaat Place\nChinese Restaurant\nCoffee Shop\nCollege Lab\nComfort Food Restaurant\nCreperie\nCuban Restaurant\nCupcake Shop\nDeli / Bodega\nDessert Shop\nDiner\nDoner Restaurant\nDonut Shop\nDumpling Restaurant\nEastern European Restaurant\nEgyptian Restaurant\nFalafel Restaurant\nFarmers Market\nFast Food Restaurant\nFilipino Restaurant\nFish & Chips Shop\nFood\nFood Court\nFood Truck\nFrench Restaurant\nFried Chicken Joint\nFrozen Yogurt Shop\nGas Station\nGastropub\nGreek Restaurant\nHalal Restaurant\nHookah Bar\nHot Dog Joint\nIce Cream Shop\nIndian Restaurant\nIraqi Restaurant\nItalian Restaurant\nJapanese Restaurant\nJuice Bar\nKebab Restaurant\nKorean Restaurant\nLebanese Restaurant\nMediterranean Restaurant\nMexican Restaurant\nMiddle Eastern Restaurant\nMoroccan Restaurant\nMovie Theater\nNew American Restaurant\nNoodle House\nPastry Shop\nPersian Restaurant\nPie Shop\nPizza Place\nPortuguese Restaurant\nRestaurant\nSalad Place\nSandwich Place\nSeafood Restaurant\nShawarma Place\nSnack Place\nSouth Indian Restaurant\nSteakhouse\nSupermarket\nSushi Restaurant\nTea Room\nThai Restaurant\nTheme Restaurant\nTibetan Restaurant\nTurkish Restaurant\nVegetarian / Vegan Restaurant\nVietnamese Restaurant\nWings Joint\n\n\n\n\n0\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nNow, let’s group rows by area and by taking the mean of the frequency of occurrence of each category, along with the number of food places surrouding it (NumberOfFoodPlaces).\nLooking at the number of food places is significant considering that some areas have fewer restaurants, and could be a valid factor to segment, if a “foodie” is looking for a place to stay.\n\nbh_grouped = bh_onehot.groupby(['Area']).mean().reset_index()\nbh_grouped['NumberOfFoodPlaces'] = bh_onehot[['Area']].value_counts(sort=False).values\nbh_grouped.head()\n\n\n\n\n\n\n\n\nArea\nAfghan Restaurant\nAfrican Restaurant\nAmerican Restaurant\nArepa Restaurant\nAsian Restaurant\nBBQ Joint\nBagel Shop\nBakery\nBistro\nBreakfast Spot\nBubble Tea Shop\nBuffet\nBurger Joint\nBurrito Place\nCafeteria\nCafé\nChaat Place\nChinese Restaurant\nCoffee Shop\nCollege Lab\nComfort Food Restaurant\nCreperie\nCuban Restaurant\nCupcake Shop\nDeli / Bodega\nDessert Shop\nDiner\nDoner Restaurant\nDonut Shop\nDumpling Restaurant\nEastern European Restaurant\nEgyptian Restaurant\nFalafel Restaurant\nFarmers Market\nFast Food Restaurant\nFilipino Restaurant\nFish & Chips Shop\nFood\nFood Court\nFood Truck\nFrench Restaurant\nFried Chicken Joint\nFrozen Yogurt Shop\nGas Station\nGastropub\nGreek Restaurant\nHalal Restaurant\nHookah Bar\nHot Dog Joint\nIce Cream Shop\nIndian Restaurant\nIraqi Restaurant\nItalian Restaurant\nJapanese Restaurant\nJuice Bar\nKebab Restaurant\nKorean Restaurant\nLebanese Restaurant\nMediterranean Restaurant\nMexican Restaurant\nMiddle Eastern Restaurant\nMoroccan Restaurant\nMovie Theater\nNew American Restaurant\nNoodle House\nPastry Shop\nPersian Restaurant\nPie Shop\nPizza Place\nPortuguese Restaurant\nRestaurant\nSalad Place\nSandwich Place\nSeafood Restaurant\nShawarma Place\nSnack Place\nSouth Indian Restaurant\nSteakhouse\nSupermarket\nSushi Restaurant\nTea Room\nThai Restaurant\nTheme Restaurant\nTibetan Restaurant\nTurkish Restaurant\nVegetarian / Vegan Restaurant\nVietnamese Restaurant\nWings Joint\nNumberOfFoodPlaces\n\n\n\n\n0\nA'ali\n0.0\n0.0\n0.00\n0.00\n0.00\n0.000000\n0.0\n0.052632\n0.0\n0.052632\n0.0\n0.0\n0.00\n0.0\n0.000000\n0.210526\n0.0\n0.0\n0.105263\n0.0\n0.0\n0.0\n0.0\n0.105263\n0.0\n0.00\n0.052632\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.052632\n0.0\n0.000000\n0.00\n0.000000\n0.052632\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.000000\n0.0\n0.00\n0.000000\n0.0\n0.052632\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.052632\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.00\n0.0\n0.157895\n0.0\n0.052632\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n19\n\n\n1\nAbu Baham\n0.0\n0.0\n0.00\n0.00\n0.00\n0.111111\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.00\n0.0\n0.111111\n0.000000\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.00\n0.000000\n0.0\n0.111111\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.00\n0.111111\n0.000000\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.111111\n0.0\n0.00\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.111111\n0.0\n0.222222\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.00\n0.0\n0.111111\n0.0\n0.000000\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n9\n\n\n2\nAl Daih\n0.0\n0.0\n0.02\n0.00\n0.02\n0.020000\n0.0\n0.140000\n0.0\n0.100000\n0.0\n0.0\n0.04\n0.0\n0.020000\n0.060000\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.06\n0.040000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.020000\n0.0\n0.000000\n0.00\n0.000000\n0.000000\n0.00\n0.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.0\n0.020000\n0.0\n0.00\n0.020000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.160000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.00\n0.0\n0.060000\n0.0\n0.020000\n0.0\n0.0\n0.00\n0.0\n0.04\n0.02\n0.0\n0.00\n0.00\n0.0\n0.0\n0.06\n0.0\n0.0\n0.0\n50\n\n\n3\nAl Dair\n0.0\n0.0\n0.00\n0.00\n0.00\n0.111111\n0.0\n0.555556\n0.0\n0.000000\n0.0\n0.0\n0.00\n0.0\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.00\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.111111\n0.00\n0.000000\n0.000000\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.000000\n0.0\n0.00\n0.111111\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.00\n0.0\n0.111111\n0.0\n0.000000\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n9\n\n\n4\nAl Garrya\n0.0\n0.0\n0.02\n0.02\n0.00\n0.020000\n0.0\n0.020000\n0.0\n0.120000\n0.0\n0.0\n0.02\n0.0\n0.020000\n0.060000\n0.0\n0.0\n0.060000\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.02\n0.020000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.040000\n0.06\n0.000000\n0.000000\n0.02\n0.0\n0.0\n0.04\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.000000\n0.1\n0.02\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.020000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.02\n0.0\n0.180000\n0.0\n0.000000\n0.0\n0.0\n0.02\n0.0\n0.02\n0.00\n0.0\n0.02\n0.02\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n50\n\n\n\n\n\n\n\nLet’s call this this bh_grouped. Now that we have this processed information, we can analyze this data more clearly by reordering it so that only the 10 most common type of food places for an area are retained.\n\n# Function to sort venues by most common ones\ndef return_most_common_venues(row, num_top_venues):\n    row_categories = row.iloc[1:-1]\n    row_categories_sorted = row_categories.sort_values(ascending=False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]\n\nnum_top_venues = 10\n\nindicators = ['st', 'nd', 'rd']\n\n# create columns according to number of top venues\ncolumns = ['Area', 'NumberOfFoodPlaces']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Common Food Place'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Common Food Place'.format(ind+1))\n\n# create a new dataframe\nfoods_sorted = pd.DataFrame(columns=columns)\nfoods_sorted[['Area','NumberOfFoodPlaces']] = bh_grouped[['Area','NumberOfFoodPlaces']]\n\nfor ind in np.arange(bh_grouped.shape[0]):\n    foods_sorted.iloc[ind, 2:] = return_most_common_venues(bh_grouped.iloc[ind, :], num_top_venues)\n\n# Get the count    \nfoods_sorted.head()\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n0\nA'ali\n19\nCafé\nRestaurant\nCoffee Shop\nCupcake Shop\nBreakfast Spot\nFood\nSandwich Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nBakery\n\n\n1\nAbu Baham\n9\nMiddle Eastern Restaurant\nCafeteria\nIce Cream Shop\nDonut Shop\nBBQ Joint\nRestaurant\nFish & Chips Shop\nMediterranean Restaurant\nAfghan Restaurant\nNew American Restaurant\n\n\n2\nAl Daih\n50\nMiddle Eastern Restaurant\nBakery\nBreakfast Spot\nDessert Shop\nCafé\nRestaurant\nTurkish Restaurant\nBurger Joint\nDiner\nSteakhouse\n\n\n3\nAl Dair\n9\nBakery\nRestaurant\nBBQ Joint\nItalian Restaurant\nFast Food Restaurant\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n4\nAl Garrya\n50\nRestaurant\nBreakfast Spot\nIndian Restaurant\nCoffee Shop\nFilipino Restaurant\nCafé\nFried Chicken Joint\nFast Food Restaurant\nDiner\nMiddle Eastern Restaurant\n\n\n\n\n\n\n\nLet’s call this table foods_sorted."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#cluster-areas",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#cluster-areas",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Cluster Areas",
    "text": "Cluster Areas\nNow we are ready for further analysis and clustering. We will use the bh_grouped dataframe since it contains the necessary numerical values for machine learning.\nOur feature set is comprised of all the food categories (10 features).\nWe are excluding the NumberOfFoodPlaces feature as input to the ML model, since our problem requires segmenting areas by the type of food available. This quantity is only relevant to us to finally decide whether to live in an area or not.\nA more concrete reason to exclude it, is the fact that there are all sorts of factors involved that we’re neglecting due to lack of data, such as living costs, access to public transport etc.\nThis is a foodie’s guide to finding a place, and this venture shouldn’t be bogged-down by the fact that there are sometimes fewer number of restaurants than one would expect.\nOur target value will be cluster labels.\nFor our machine learning analysis, we will use the simplest clustering algorithm to separate the areas which is K-Means Clustering; an unsupervised machine learning approach to serve our purpose. We’ll use the popular machine learning library Sci-Kit Learn to do that in python.\nWe’ll run k-means to group the areas into 5 clusters. We pick this number for the sake of examination. We’ll fit the model on the entire data to learn these clusters.\n\n# set number of clusters\nkclusters = 5\n\nbh_grouped_clustering = bh_grouped.drop(['Area','NumberOfFoodPlaces'], 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(bh_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10] \n\narray([1, 0, 0, 2, 0, 0, 0, 0, 1, 1], dtype=int32)\n\n\nLet’s create a new dataframe bh_merged that includes the cluster as well as the top 10 food places for each area.\n\n# add clustering labels\ntry:\n    foods_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\nexcept:\n    # Allows me to retry if the Cluster Labels column exists\n    foods_sorted['Cluster Labels'] = kmeans.labels_\n\nbh_merged = bh_data\n\n# merge bh_grouped with bh_data to add latitude/longitude for each neighborhood\nbh_merged = bh_merged.join(foods_sorted.set_index('Area'), on='Area')\nbh_merged.dropna(how='any', axis=0, inplace=True)\nbh_merged['Cluster Labels'] = bh_merged['Cluster Labels'].astype(np.int32)\nbh_merged.head() # check the last columns!\n\n\n\n\n\n\n\n\nArea\nLatitude\nLongitude\nCluster Labels\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n0\nA'ali\n26.154454\n50.527364\n1\n19.0\nCafé\nRestaurant\nCoffee Shop\nCupcake Shop\nBreakfast Spot\nFood\nSandwich Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nBakery\n\n\n1\nAbu Baham\n26.205737\n50.541668\n0\n9.0\nMiddle Eastern Restaurant\nCafeteria\nIce Cream Shop\nDonut Shop\nBBQ Joint\nRestaurant\nFish & Chips Shop\nMediterranean Restaurant\nAfghan Restaurant\nNew American Restaurant\n\n\n3\nAl Garrya\n26.232690\n50.578110\n0\n50.0\nRestaurant\nBreakfast Spot\nIndian Restaurant\nCoffee Shop\nFilipino Restaurant\nCafé\nFried Chicken Joint\nFast Food Restaurant\nDiner\nMiddle Eastern Restaurant\n\n\n4\nAl Hajar\n26.225405\n50.590138\n0\n49.0\nCafé\nFilipino Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nCoffee Shop\nAsian Restaurant\nIndian Restaurant\nPizza Place\nBBQ Joint\nRestaurant\n\n\n5\nAl Kharijiya\n26.160230\n50.609140\n0\n16.0\nCafeteria\nAsian Restaurant\nFast Food Restaurant\nBakery\nWings Joint\nPizza Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nCafé\nFood Court\n\n\n\n\n\n\n\nFinally, let’s visualize the resulting clusters\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#examine-clusters-final-conclusion",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#examine-clusters-final-conclusion",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Examine Clusters & Final Conclusion",
    "text": "Examine Clusters & Final Conclusion\nNow, we can examine & determine the discriminating characteristics of each cluster.\n\nCluster 1\n\ncluster1 = bh_merged.loc[bh_merged['Cluster Labels'] == 0, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster1\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n1\nAbu Baham\n9.0\nMiddle Eastern Restaurant\nCafeteria\nIce Cream Shop\nDonut Shop\nBBQ Joint\nRestaurant\nFish & Chips Shop\nMediterranean Restaurant\nAfghan Restaurant\nNew American Restaurant\n\n\n3\nAl Garrya\n50.0\nRestaurant\nBreakfast Spot\nIndian Restaurant\nCoffee Shop\nFilipino Restaurant\nCafé\nFried Chicken Joint\nFast Food Restaurant\nDiner\nMiddle Eastern Restaurant\n\n\n4\nAl Hajar\n49.0\nCafé\nFilipino Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nCoffee Shop\nAsian Restaurant\nIndian Restaurant\nPizza Place\nBBQ Joint\nRestaurant\n\n\n5\nAl Kharijiya\n16.0\nCafeteria\nAsian Restaurant\nFast Food Restaurant\nBakery\nWings Joint\nPizza Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nCafé\nFood Court\n\n\n12\nArad\n50.0\nMiddle Eastern Restaurant\nRestaurant\nDessert Shop\nBurger Joint\nCafé\nFast Food Restaurant\nDiner\nIce Cream Shop\nBakery\nSandwich Place\n\n\n15\nBudaiya\n36.0\nMiddle Eastern Restaurant\nBakery\nCafeteria\nBurger Joint\nSeafood Restaurant\nTea Room\nCafé\nSandwich Place\nIce Cream Shop\nRestaurant\n\n\n16\nJid Ali\n48.0\nRestaurant\nMiddle Eastern Restaurant\nCafé\nItalian Restaurant\nCoffee Shop\nBreakfast Spot\nDessert Shop\nPizza Place\nDiner\nSeafood Restaurant\n\n\n18\nBani Jamra\n19.0\nBreakfast Spot\nRestaurant\nCafé\nCafeteria\nMiddle Eastern Restaurant\nVegetarian / Vegan Restaurant\nBakery\nSnack Place\nIndian Restaurant\nFast Food Restaurant\n\n\n19\nBarbar\n6.0\nPizza Place\nBBQ Joint\nBakery\nSandwich Place\nMiddle Eastern Restaurant\nJuice Bar\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n21\nBu Quwah\n12.0\nTurkish Restaurant\nCafeteria\nFood\nPizza Place\nAsian Restaurant\nBakery\nRestaurant\nFalafel Restaurant\nSeafood Restaurant\nCoffee Shop\n\n\n22\nBuri\n22.0\nIndian Restaurant\nBakery\nBreakfast Spot\nRestaurant\nFood\nFood Court\nMiddle Eastern Restaurant\nCafé\nBurger Joint\nCafeteria\n\n\n23\nBusaiteen\n43.0\nCoffee Shop\nCafé\nBurger Joint\nIce Cream Shop\nTea Room\nJuice Bar\nMiddle Eastern Restaurant\nDonut Shop\nRestaurant\nDessert Shop\n\n\n24\nAl Daih\n50.0\nMiddle Eastern Restaurant\nBakery\nBreakfast Spot\nDessert Shop\nCafé\nRestaurant\nTurkish Restaurant\nBurger Joint\nDiner\nSteakhouse\n\n\n28\nDiraz\n16.0\nCafeteria\nMiddle Eastern Restaurant\nFast Food Restaurant\nSteakhouse\nSnack Place\nVegetarian / Vegan Restaurant\nAsian Restaurant\nIndian Restaurant\nRestaurant\nBreakfast Spot\n\n\n32\nEast Hidd City\n47.0\nCoffee Shop\nFried Chicken Joint\nPizza Place\nCafeteria\nCafé\nIndian Restaurant\nBBQ Joint\nIce Cream Shop\nBakery\nBurrito Place\n\n\n34\nGalali\n50.0\nFast Food Restaurant\nRestaurant\nIce Cream Shop\nSandwich Place\nMiddle Eastern Restaurant\nTurkish Restaurant\nTea Room\nBurger Joint\nItalian Restaurant\nCafé\n\n\n35\nAl Hidd\n47.0\nCoffee Shop\nFried Chicken Joint\nPizza Place\nCafeteria\nCafé\nIndian Restaurant\nBBQ Joint\nIce Cream Shop\nBakery\nBurrito Place\n\n\n36\nHalat Bu Maher\n48.0\nMiddle Eastern Restaurant\nIce Cream Shop\nSeafood Restaurant\nCafé\nSandwich Place\nCafeteria\nFast Food Restaurant\nIndian Restaurant\nRestaurant\nTurkish Restaurant\n\n\n37\nHalat Nuaim\n12.0\nRestaurant\nFood Truck\nLebanese Restaurant\nIndian Restaurant\nDeli / Bodega\nBakery\nSeafood Restaurant\nBurger Joint\nCoffee Shop\nCafé\n\n\n38\nHamad Town\n7.0\nSandwich Place\nWings Joint\nBakery\nJuice Bar\nVegetarian / Vegan Restaurant\nVietnamese Restaurant\nHookah Bar\nLebanese Restaurant\nNew American Restaurant\nMovie Theater\n\n\n41\nHillat Abdul Saleh\n47.0\nMiddle Eastern Restaurant\nFast Food Restaurant\nCoffee Shop\nIce Cream Shop\nPizza Place\nJuice Bar\nDonut Shop\nCafé\nFood Truck\nBakery\n\n\n47\nJid Al-Haj\n48.0\nRestaurant\nMiddle Eastern Restaurant\nCafé\nItalian Restaurant\nCoffee Shop\nBreakfast Spot\nDessert Shop\nPizza Place\nDiner\nSeafood Restaurant\n\n\n48\nJidhafs\n49.0\nBakery\nBreakfast Spot\nMiddle Eastern Restaurant\nTurkish Restaurant\nDessert Shop\nAmerican Restaurant\nFood\nBurger Joint\nSteakhouse\nCafeteria\n\n\n51\nKarrana\n28.0\nMiddle Eastern Restaurant\nRestaurant\nIce Cream Shop\nAsian Restaurant\nFalafel Restaurant\nBakery\nCoffee Shop\nCafeteria\nItalian Restaurant\nFried Chicken Joint\n\n\n52\nKarzakan\n4.0\nVegetarian / Vegan Restaurant\nTurkish Restaurant\nRestaurant\nShawarma Place\nAfghan Restaurant\nKebab Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n53\nKhamis\n20.0\nMiddle Eastern Restaurant\nSandwich Place\nTurkish Restaurant\nCafeteria\nMediterranean Restaurant\nCafé\nRestaurant\nTea Room\nItalian Restaurant\nDiner\n\n\n57\nManama\n45.0\nIndian Restaurant\nFilipino Restaurant\nAsian Restaurant\nPizza Place\nMiddle Eastern Restaurant\nCoffee Shop\nCafé\nCafeteria\nBBQ Joint\nRestaurant\n\n\n59\nMuharraq\n48.0\nMiddle Eastern Restaurant\nSeafood Restaurant\nCafé\nCafeteria\nRestaurant\nIce Cream Shop\nTurkish Restaurant\nCoffee Shop\nPizza Place\nBurger Joint\n\n\n70\nSamaheej\n3.0\nRestaurant\nBakery\nAfghan Restaurant\nKorean Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n71\nSanad\n38.0\nMiddle Eastern Restaurant\nRestaurant\nIce Cream Shop\nSandwich Place\nPizza Place\nBreakfast Spot\nCafeteria\nBurger Joint\nCafé\nBakery\n\n\n72\nSar\n27.0\nPizza Place\nBurger Joint\nGas Station\nChinese Restaurant\nFast Food Restaurant\nBreakfast Spot\nThai Restaurant\nCoffee Shop\nFried Chicken Joint\nDeli / Bodega\n\n\n75\nShakhura\n45.0\nMiddle Eastern Restaurant\nBreakfast Spot\nCafé\nRestaurant\nFast Food Restaurant\nDessert Shop\nBakery\nPizza Place\nDonut Shop\nIce Cream Shop\n\n\n78\nTashan\n25.0\nMiddle Eastern Restaurant\nRestaurant\nTurkish Restaurant\nSeafood Restaurant\nCafeteria\nIndian Restaurant\nItalian Restaurant\nMediterranean Restaurant\nDiner\nDessert Shop\n\n\n79\nTubli\n34.0\nRestaurant\nBakery\nBurger Joint\nJuice Bar\nMiddle Eastern Restaurant\nCoffee Shop\nCafé\nTurkish Restaurant\nMediterranean Restaurant\nPie Shop\n\n\n\n\n\n\n\n\n\nThis cluster has 34 areas\n\n\n\n\nCluster 2\n\ncluster2 = bh_merged.loc[bh_merged['Cluster Labels'] == 1, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster2\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n0\nA'ali\n19.0\nCafé\nRestaurant\nCoffee Shop\nCupcake Shop\nBreakfast Spot\nFood\nSandwich Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nBakery\n\n\n6\nAl Markh\n17.0\nCafé\nFast Food Restaurant\nBurger Joint\nIce Cream Shop\nJuice Bar\nCoffee Shop\nMiddle Eastern Restaurant\nDessert Shop\nBakery\nBBQ Joint\n\n\n7\nAl Musalla\n17.0\nFast Food Restaurant\nCafé\nCoffee Shop\nMiddle Eastern Restaurant\nSeafood Restaurant\nSteakhouse\nFood Court\nRestaurant\nPizza Place\nJapanese Restaurant\n\n\n8\nAl Qadam\n4.0\nPizza Place\nCafé\nCafeteria\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n9\nAl Qala\n50.0\nCafé\nBurger Joint\nRestaurant\nDessert Shop\nCoffee Shop\nSandwich Place\nBakery\nJuice Bar\nIce Cream Shop\nPizza Place\n\n\n10\nAl Qurayyah\n11.0\nRestaurant\nCafé\nMediterranean Restaurant\nMiddle Eastern Restaurant\nComfort Food Restaurant\nCoffee Shop\nDiner\nJapanese Restaurant\nJuice Bar\nPastry Shop\n\n\n11\nAmwaj Islands\n37.0\nCafé\nMiddle Eastern Restaurant\nAmerican Restaurant\nIndian Restaurant\nRestaurant\nAsian Restaurant\nPizza Place\nDeli / Bodega\nDiner\nPortuguese Restaurant\n\n\n13\nAskar\n4.0\nCafeteria\nBurger Joint\nCafé\nPie Shop\nPastry Shop\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\n\n\n17\nBahrain Bay\n50.0\nCoffee Shop\nCafé\nIndian Restaurant\nPizza Place\nSteakhouse\nBurger Joint\nMiddle Eastern Restaurant\nRestaurant\nFried Chicken Joint\nAmerican Restaurant\n\n\n20\nBilad Al Qadeem\n46.0\nCafé\nMiddle Eastern Restaurant\nIce Cream Shop\nBreakfast Spot\nSandwich Place\nFast Food Restaurant\nPizza Place\nBurger Joint\nRestaurant\nBakery\n\n\n27\nDiplomatic Area\n50.0\nCoffee Shop\nCafé\nAmerican Restaurant\nFried Chicken Joint\nBurger Joint\nRestaurant\nFood Court\nMiddle Eastern Restaurant\nIndian Restaurant\nFrench Restaurant\n\n\n30\nDumistan\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n33\nEker\n6.0\nDiner\nSnack Place\nCreperie\nMiddle Eastern Restaurant\nCafé\nCafeteria\nPastry Shop\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n39\nHamala\n29.0\nCoffee Shop\nCafé\nBurger Joint\nPizza Place\nRestaurant\nSandwich Place\nItalian Restaurant\nMexican Restaurant\nHot Dog Joint\nMediterranean Restaurant\n\n\n42\nIsa Town\n43.0\nCafé\nIndian Restaurant\nRestaurant\nPizza Place\nBakery\nCafeteria\nCoffee Shop\nFast Food Restaurant\nTheme Restaurant\nItalian Restaurant\n\n\n43\nJanabiyah\n29.0\nCoffee Shop\nCafé\nBurger Joint\nPizza Place\nRestaurant\nSandwich Place\nItalian Restaurant\nMexican Restaurant\nHot Dog Joint\nMediterranean Restaurant\n\n\n49\nJurdab\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n50\nKarbabad\n12.0\nCafé\nSandwich Place\nCoffee Shop\nAsian Restaurant\nBakery\nMiddle Eastern Restaurant\nBurger Joint\nCafeteria\nAfghan Restaurant\nMediterranean Restaurant\n\n\n55\nMahazza\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n58\nMarquban\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n60\nMuqaba\n35.0\nCafé\nCoffee Shop\nRestaurant\nBakery\nBreakfast Spot\nCafeteria\nFried Chicken Joint\nFood Court\nMiddle Eastern Restaurant\nPersian Restaurant\n\n\n61\nMuqsha\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n64\nNuwaidrat\n7.0\nCafé\nAsian Restaurant\nRestaurant\nBakery\nMiddle Eastern Restaurant\nFalafel Restaurant\nAfghan Restaurant\nMediterranean Restaurant\nNoodle House\nNew American Restaurant\n\n\n65\nRiffa\n49.0\nCafé\nCoffee Shop\nBurger Joint\nRestaurant\nBakery\nSandwich Place\nDessert Shop\nPizza Place\nItalian Restaurant\nLebanese Restaurant\n\n\n66\nReef Island\n13.0\nCafé\nAmerican Restaurant\nMiddle Eastern Restaurant\nRestaurant\nFrench Restaurant\nJapanese Restaurant\nBreakfast Spot\nDessert Shop\nLebanese Restaurant\nMediterranean Restaurant\n\n\n68\nSakhir\n8.0\nBurger Joint\nDiner\nFood Truck\nMiddle Eastern Restaurant\nCafé\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\n\n\n69\nSalmabad\n12.0\nCafé\nCupcake Shop\nBakery\nCoffee Shop\nMiddle Eastern Restaurant\nBreakfast Spot\nJuice Bar\nFood\nAsian Restaurant\nJapanese Restaurant\n\n\n73\nSehla\n6.0\nBurger Joint\nCafé\nAmerican Restaurant\nIce Cream Shop\nBakery\nFood Court\nPastry Shop\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n81\nZallaq\n29.0\nCafé\nCoffee Shop\nRestaurant\nJuice Bar\nMiddle Eastern Restaurant\nFast Food Restaurant\nCreperie\nCafeteria\nBurrito Place\nBurger Joint\n\n\n\n\n\n\n\n\n\nThis cluster has 29 areas\n\n\n\n\nCluster 3\n\ncluster3 = bh_merged.loc[bh_merged['Cluster Labels'] == 2, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster3\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n25\nAl Dair\n9.0\nBakery\nRestaurant\nBBQ Joint\nItalian Restaurant\nFast Food Restaurant\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n44\nJannusan\n11.0\nBakery\nGastropub\nCafeteria\nIndian Restaurant\nSandwich Place\nFish & Chips Shop\nSeafood Restaurant\nSnack Place\nNew American Restaurant\nMovie Theater\n\n\n46\nJaww\n1.0\nBakery\nAfghan Restaurant\nKorean Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\nMediterranean Restaurant\n\n\n54\nMa'ameer\n3.0\nCreperie\nDiner\nBakery\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\n\n\n76\nSitra\n2.0\nTurkish Restaurant\nBakery\nAfghan Restaurant\nKorean Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n\n\n\n\n\n\n\nThis cluster has 5 areas\n\n\n\n\nCluster 4\n\ncluster4 = bh_merged.loc[bh_merged['Cluster Labels'] == 3, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster4\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n14\nAwali\n1.0\nCafé\nAfghan Restaurant\nPersian Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\nMediterranean Restaurant\n\n\n\n\n\n\n\n\n\nThis cluster has 1 area\n\n\n\n\nCluster 5\n\ncluster5 = bh_merged.loc[bh_merged['Cluster Labels'] == 4, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster5\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n26\nDar Kulaib\n4.0\nRestaurant\nCoffee Shop\nSandwich Place\nBreakfast Spot\nAfghan Restaurant\nLebanese Restaurant\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\n\n\n56\nMalkiya\n2.0\nIce Cream Shop\nCoffee Shop\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n77\nSufala\n3.0\nCoffee Shop\nRestaurant\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n\n\n\n\n\n\n\nThis cluster has 3 areas\n\n\n\n\nConclusion\nPhew! We’re done with finding our clusters, and finding out which areas fall into it. To understand the constraints and my discussion to conclude this solution, please refer to my report available on my github repo, where you will find the datasets I’ve used :D\nI hope you’ve enjoyed reading & learning something new from this post. Doing this was part of my data-science course, and I hope you can do the same with your hobby projects.\nUntil next time, cheers!"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Aug 2021 - Present, Bahrain\n\nRoles\n\nSpearheading the IT infrastructure of the company and coordinating with stakeholders to further its growth in the GCC.\nOrchestrating and developing systems on our ERP internally (Odoo v14), which includes outsourcing most of that development.\nOperating as a Full-Stack Developer and Odoo Consultant/Developer.\nOptimizing IT processes to achieve maximum efficiency and documenting said processes.\nOffering capacity building and training on new processes.\n\nAchievements\n\nIntroduced 15 crucial workflow improvements with Odoo modules.\nImplemented a robust reporting infrastructure using Apache Airflow, AWS, and Apache Superset for data visualization. Designed and managed ETL pipelines with Postgres, delivering actionable business insights to senior management.\nIncreased sales by reducing order cancellations with Instashop by 34% by building a real-time stock API integration, eliminating the need for merchandisers’ labor.\nCreated various sales reports, inventory reports, and accounting reports.\nOrganized, instructed, and developed several data migration projects to ensure smooth transitions using pandas, SQL, and python-odoo modules. Examples include our entire inventory and stock valuation and our eCommerce site https://petarabia.com.\nManaged multiple technical capacity building sessions for HR, Accounting, Sales, and Inventory departments in the form of in-person meetings and/or videos.\nSucceeded in building a Tap-to-Pay system for recurring payments as part of Pet Arabia’s Bath Club Membership service using Flutter and Python-Flask. First of its kind.\nDeveloped a Flutter app for attendance tracking.\n\n\n\n\n\nOct 2019 - Present, Malaysia (Remote) & Bahrain\n\nRoles\n\nDeveloping ML algorithms to derive insights on accelerating yield for farmers and eliminating diseases that plague them.\nGrowing the agri-tech business in the GCC by securing sales meetings in and out of Bahrain to build a strong base for the company.\n\nAchievements\n\nIncorporated an experimental version of the yield prediction algorithm as part of the plantOS application using Sci-Kit Learn.\nDeveloped a Fruit Object Detection algorithm using PyTorch (Fast Mask-RCNN model used) to showcase to the Ministry of Agriculture in Malaysia.\n\n\n\n\n\nMar 2021 - Feb 2022, Bahrain (Hybrid & Part-Time)\n\nRoles\n\nAct as a part-time coach and influencer to individuals interested in switching to a Data Science career, by having them go through a 3-month program and shipping a product by the end of it.\nEach batch runs for a minimum of 3 months.\n\nAchievements\n\nSuccessfully ran 3 batches back-to-back with a total of 8 students.\nSecured 7 successful capstone data-science projects with the students.\n\n\n\n\n\nMay 2021 - Jul 2021, Bahrain\n\nRoles\n\nAs a data science/software engineer, spearheading the ML capabilities for the startup in the field of Cyber Security by leveraging my experience in software development.\n\nAchievements\n\nOur team successfully researched and built a prototype intrusion-detection system within headquarters using Sci-Kit Learn and Pandas. Source-code is confidential.\n\n\n\n\n\nApr 2020 - Jul 2022, MEA (Remote)\n\nDeveloped and maintained dashboards and data structures with the Regional Team of AIESEC International to track performance growth for members in 28 countries, concretely and transparently.\nCoached Vice Presidents and Team Leaders on the use of the dashboard tool.\n\n\n\n\nApr 2020 - Jan 2021, Bahrain\n\nDeveloped dashboards and data structures with the National Team of AIESEC in Bahrain to track performance and potential growth of the whole entity more transparently.\nCoached Vice Presidents and Team Leaders on the use of our data management tools.\n\n\n\n\nFeb 2020 - Jan 2021, Bahrain\n\nRoles\n\nVP of a local chapter for AIESEC in Bahrain, leading a department to create projects that deliver social impact in the country under the United Nations Sustainable Development Goals.\n\nAchievements\n\nExecuted 10 social impact projects from start to finish, driving measurable impact towards the United Nations Sustainable Development Goals.\nCoached a total of 6 team leaders on project management and leadership skills. These team leaders, in turn, coached their teams to realize these social projects.\nFacilitated various local/national chapter conferences with my executive board team.\nCollaborated with multiple stakeholders as partners.\n\n\n\n\n\nFeb 2019 - Feb 2020, Bahrain\n\nManaged teams within the Global Exchange products to connect a group of diverse and talented individuals from around the globe with startups and NGOs in the Kingdom of Bahrain to take on internships, offering them life-changing experiences in the process.\n\n\n\n\nMay 2019 - Jun 2019, Bahrain (Part-time)\n\nTaught kids from years 2-5 and introduced them to world-building using Minecraft at St. Christopher’s Primary School.\n\n\n\n\nNov 2018 - Feb 2019, Bahrain\n\nRoles\n\nWorked as a Software Developer.\nTutored workshops and managed tutorials.\n\nAchievements\n\nManaged and developed tutorials on Basic Electronics and Computers aimed at kids and young adults.\nFurther developed motion tracking software using pose estimation for clients to compete in Mashro3i 2019.\nConducted a Basic Computer workshop for a primary school in Karzakan, Bahrain.\n\n\n\n\n\n\n\n\nJan 2018 - May 2018, Bangalore, India\n\nInterned at a Data Science Startup on Customer Care Support.\nSucceeded in creating a sentence generation tool along with a dialog script for a chat-bot using Natural Language Processing to resolve chat queries.\nRecommendation Letter 🔥\n\n\n\n\n\n\n\nAug 2014 - Jul 2018, Bangalore, India\n\nGraduated with First Class Honors (Distinction).\nFinal Year Project on Breast Cancer Detection. Researched Convolutional Neural Networks and implemented one using the Keras API to distinguish between different breast cancers.\nExtracurricular\n\nMember of Nature Watch (2016-Present): An eco-club based off campus.\nVoracious Quizzer, won 1st and 2nd place in multiple competitions across Bangalore City.\n\n\n\n\n\nSep 2012 - Aug 2013, Bahrain\n\nStudied Physics, Math & Computer Science.\nGPA 3.1\nBuilt an invoicing system for a laundry shop using VB.NET as part of my final year.\n\n\n\n\n\nPersonal Blog (2019-present)\n\nAuthored a series of articles covering a variety of topics and tools related to Data Science (and pop-culture!).\n\nRealtime 2D Pose Estimation GUI with OpenPose (Feb 2019)\n\nAssociated with The Hive, Bahrain.\nSuccessfully built a UI for label recognition on top of a pose estimation algorithm to compete in Mashro3i 2019.\n\nRemote Control Car with M5Stack Wristwatch (Jan 2019)\n\nAssociated with The Hive, Bahrain.\nBuilding a simple remote control kit to educate kids and young adults on the basics of IoT during a workshop at Brinc IoT Building.\n\nBreast Cancer Screening using Keras (Jan 2018 - Jun 2018)\n\nAssociated with Acharya Institutes.\nFinal year team project on distinguishing forms of breast cancer using Keras; a deep learning framework.\nCompiled a report on the study and worked on it while interning at a Data Science startup. LINK TO REPORT\nContributors: Sumanth Krishna, Srujana, Isa AlDoseri\n\nAIESEC EXPA Assistant Tool (2019)\n\nAssociated with AIESEC in Bahrain.\nDeveloped an automation tool using Python Selenium to scrap data from the AIESEC Management Site to update our CRM workspace.\n\n\n\n\nAIESEC in Bahrain: Ebtikar (Jul 2020 - Aug 2020)\n\nAssociated with AIESEC in Bahrain.\nUnder The Patronage of H.E. the President of Sustainable Energy Authority, Ebtikar is a program where participants prepare, compete (in teams) and learn more about renewable energy and climate change. And they do so in teams where they will compete for the best idea.\n\n\n\n\n\n\nIBM Data Science Professional Certificate | Coursera | Issued Aug 2021\nTableau Essential Training (2020.1) | LinkedIn Learning ⋅ Course Certificate | Issued Jan 2021 Original Source\nCisco Certified Network Associate Routing and Switching (CCNA) | Cisco | Issued Oct 2020 · Expires Oct 2023\nChatGPT Prompt Engineering for Developers | DeepLearning.AI | Issued Jun 2024"
  },
  {
    "objectID": "CV.html#work-experience",
    "href": "CV.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Aug 2021 - Present, Bahrain\n\nRoles\n\nSpearheading the IT infrastructure of the company and coordinating with stakeholders to further its growth in the GCC.\nOrchestrating and developing systems on our ERP internally (Odoo v14), which includes outsourcing most of that development.\nOperating as a Full-Stack Developer and Odoo Consultant/Developer.\nOptimizing IT processes to achieve maximum efficiency and documenting said processes.\nOffering capacity building and training on new processes.\n\nAchievements\n\nIntroduced 15 crucial workflow improvements with Odoo modules.\nImplemented a robust reporting infrastructure using Apache Airflow, AWS, and Apache Superset for data visualization. Designed and managed ETL pipelines with Postgres, delivering actionable business insights to senior management.\nIncreased sales by reducing order cancellations with Instashop by 34% by building a real-time stock API integration, eliminating the need for merchandisers’ labor.\nCreated various sales reports, inventory reports, and accounting reports.\nOrganized, instructed, and developed several data migration projects to ensure smooth transitions using pandas, SQL, and python-odoo modules. Examples include our entire inventory and stock valuation and our eCommerce site https://petarabia.com.\nManaged multiple technical capacity building sessions for HR, Accounting, Sales, and Inventory departments in the form of in-person meetings and/or videos.\nSucceeded in building a Tap-to-Pay system for recurring payments as part of Pet Arabia’s Bath Club Membership service using Flutter and Python-Flask. First of its kind.\nDeveloped a Flutter app for attendance tracking.\n\n\n\n\n\nOct 2019 - Present, Malaysia (Remote) & Bahrain\n\nRoles\n\nDeveloping ML algorithms to derive insights on accelerating yield for farmers and eliminating diseases that plague them.\nGrowing the agri-tech business in the GCC by securing sales meetings in and out of Bahrain to build a strong base for the company.\n\nAchievements\n\nIncorporated an experimental version of the yield prediction algorithm as part of the plantOS application using Sci-Kit Learn.\nDeveloped a Fruit Object Detection algorithm using PyTorch (Fast Mask-RCNN model used) to showcase to the Ministry of Agriculture in Malaysia.\n\n\n\n\n\nMar 2021 - Feb 2022, Bahrain (Hybrid & Part-Time)\n\nRoles\n\nAct as a part-time coach and influencer to individuals interested in switching to a Data Science career, by having them go through a 3-month program and shipping a product by the end of it.\nEach batch runs for a minimum of 3 months.\n\nAchievements\n\nSuccessfully ran 3 batches back-to-back with a total of 8 students.\nSecured 7 successful capstone data-science projects with the students.\n\n\n\n\n\nMay 2021 - Jul 2021, Bahrain\n\nRoles\n\nAs a data science/software engineer, spearheading the ML capabilities for the startup in the field of Cyber Security by leveraging my experience in software development.\n\nAchievements\n\nOur team successfully researched and built a prototype intrusion-detection system within headquarters using Sci-Kit Learn and Pandas. Source-code is confidential.\n\n\n\n\n\nApr 2020 - Jul 2022, MEA (Remote)\n\nDeveloped and maintained dashboards and data structures with the Regional Team of AIESEC International to track performance growth for members in 28 countries, concretely and transparently.\nCoached Vice Presidents and Team Leaders on the use of the dashboard tool.\n\n\n\n\nApr 2020 - Jan 2021, Bahrain\n\nDeveloped dashboards and data structures with the National Team of AIESEC in Bahrain to track performance and potential growth of the whole entity more transparently.\nCoached Vice Presidents and Team Leaders on the use of our data management tools.\n\n\n\n\nFeb 2020 - Jan 2021, Bahrain\n\nRoles\n\nVP of a local chapter for AIESEC in Bahrain, leading a department to create projects that deliver social impact in the country under the United Nations Sustainable Development Goals.\n\nAchievements\n\nExecuted 10 social impact projects from start to finish, driving measurable impact towards the United Nations Sustainable Development Goals.\nCoached a total of 6 team leaders on project management and leadership skills. These team leaders, in turn, coached their teams to realize these social projects.\nFacilitated various local/national chapter conferences with my executive board team.\nCollaborated with multiple stakeholders as partners.\n\n\n\n\n\nFeb 2019 - Feb 2020, Bahrain\n\nManaged teams within the Global Exchange products to connect a group of diverse and talented individuals from around the globe with startups and NGOs in the Kingdom of Bahrain to take on internships, offering them life-changing experiences in the process.\n\n\n\n\nMay 2019 - Jun 2019, Bahrain (Part-time)\n\nTaught kids from years 2-5 and introduced them to world-building using Minecraft at St. Christopher’s Primary School.\n\n\n\n\nNov 2018 - Feb 2019, Bahrain\n\nRoles\n\nWorked as a Software Developer.\nTutored workshops and managed tutorials.\n\nAchievements\n\nManaged and developed tutorials on Basic Electronics and Computers aimed at kids and young adults.\nFurther developed motion tracking software using pose estimation for clients to compete in Mashro3i 2019.\nConducted a Basic Computer workshop for a primary school in Karzakan, Bahrain."
  },
  {
    "objectID": "CV.html#internships",
    "href": "CV.html#internships",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Jan 2018 - May 2018, Bangalore, India\n\nInterned at a Data Science Startup on Customer Care Support.\nSucceeded in creating a sentence generation tool along with a dialog script for a chat-bot using Natural Language Processing to resolve chat queries.\nRecommendation Letter 🔥"
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Aug 2014 - Jul 2018, Bangalore, India\n\nGraduated with First Class Honors (Distinction).\nFinal Year Project on Breast Cancer Detection. Researched Convolutional Neural Networks and implemented one using the Keras API to distinguish between different breast cancers.\nExtracurricular\n\nMember of Nature Watch (2016-Present): An eco-club based off campus.\nVoracious Quizzer, won 1st and 2nd place in multiple competitions across Bangalore City.\n\n\n\n\n\nSep 2012 - Aug 2013, Bahrain\n\nStudied Physics, Math & Computer Science.\nGPA 3.1\nBuilt an invoicing system for a laundry shop using VB.NET as part of my final year."
  },
  {
    "objectID": "CV.html#technical-projects",
    "href": "CV.html#technical-projects",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Personal Blog (2019-present)\n\nAuthored a series of articles covering a variety of topics and tools related to Data Science (and pop-culture!).\n\nRealtime 2D Pose Estimation GUI with OpenPose (Feb 2019)\n\nAssociated with The Hive, Bahrain.\nSuccessfully built a UI for label recognition on top of a pose estimation algorithm to compete in Mashro3i 2019.\n\nRemote Control Car with M5Stack Wristwatch (Jan 2019)\n\nAssociated with The Hive, Bahrain.\nBuilding a simple remote control kit to educate kids and young adults on the basics of IoT during a workshop at Brinc IoT Building.\n\nBreast Cancer Screening using Keras (Jan 2018 - Jun 2018)\n\nAssociated with Acharya Institutes.\nFinal year team project on distinguishing forms of breast cancer using Keras; a deep learning framework.\nCompiled a report on the study and worked on it while interning at a Data Science startup. LINK TO REPORT\nContributors: Sumanth Krishna, Srujana, Isa AlDoseri\n\nAIESEC EXPA Assistant Tool (2019)\n\nAssociated with AIESEC in Bahrain.\nDeveloped an automation tool using Python Selenium to scrap data from the AIESEC Management Site to update our CRM workspace."
  },
  {
    "objectID": "CV.html#social-projects",
    "href": "CV.html#social-projects",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "AIESEC in Bahrain: Ebtikar (Jul 2020 - Aug 2020)\n\nAssociated with AIESEC in Bahrain.\nUnder The Patronage of H.E. the President of Sustainable Energy Authority, Ebtikar is a program where participants prepare, compete (in teams) and learn more about renewable energy and climate change. And they do so in teams where they will compete for the best idea."
  },
  {
    "objectID": "CV.html#licenses-certifications",
    "href": "CV.html#licenses-certifications",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "IBM Data Science Professional Certificate | Coursera | Issued Aug 2021\nTableau Essential Training (2020.1) | LinkedIn Learning ⋅ Course Certificate | Issued Jan 2021 Original Source\nCisco Certified Network Associate Routing and Switching (CCNA) | Cisco | Issued Oct 2020 · Expires Oct 2023\nChatGPT Prompt Engineering for Developers | DeepLearning.AI | Issued Jun 2024"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "Isa AlDoseri is a skilled software engineer with 4+ years of experience in data science, data visualization, machine learning, and Python automation. He’s completed his Bachelor’s in Engineering from AIT, Bangalore, where he honed his programming and data analysis skills. Isa has experience implementing machine learning algorithms to solve complex business problems and is proficient in Python, Rust, SQL, AWS, and Heroku. He’s a great communicator, team player, and mentor who’s taught at GoMyCode, Bahrain as an AI instructor.\nIsa is interested in IoT technology, has won the ML Olympiad conducted by GDG Manama twice in the year 2023, and enjoys automating repetitive tasks. Isa is also a proud parent of a kitten named Apollo, who brings joy to his life. He loves listening to rock music from the 90s, travelling to new countries, doing CrossFit, reading non-fiction, and attending stand-up comedy shows. He’s committed to sharing his knowledge and experience in ML, Data Science, automation (and moview reviews) through his blog, attending conferences and workshops, while staying up-to-date with the latest technology developments.\nOn this site you will find his technical blog, his resume and a list of publications.\n\n\n\n\n\n\nBreast Cancer Detection using Image Processing and Deep Learning Architecture: Sumanth Krishna, Isa AlDoseri, Srujana 2018; Acharya Institute of Technology [PDF]"
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "Breast Cancer Detection using Image Processing and Deep Learning Architecture: Sumanth Krishna, Isa AlDoseri, Srujana 2018; Acharya Institute of Technology [PDF]"
  },
  {
    "objectID": "TODO.html",
    "href": "TODO.html",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "Add certifications and courses\nAdd CV\nAdd Notable Coding Projects\nFix the links in the github repo for the Foodie’s guide\nEnsure people know about the utterances thing to communicate with me.\njustify align the text in main page.\nexpand on system analyst role in pet arabia.\nmake the job titles in the experience pages bolder.\nadd the logos to each job title in the experience page.\ndegree in what?\nremove my school, al noor international school\nhighlight ebtikar\nAdd cert assset links to my CV.qmd"
  }
]