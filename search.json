[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "Foodie’s Guide to Finding a Home in Bahrain\n\n\n\n\n\n\n\nml\n\n\nsklearn\n\n\n\n\nBy clustering neighborhoods based on popular venues surrounding them\n\n\n\n\n\n\nSep 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\nMe and Earl and the Dying Girl (and myself)\n\n\n\n\n\n\n\nmovies\n\n\n\n\nA movie that I watched during a rough time in my life.\n\n\n\n\n\n\nJun 27, 2019\n\n\nIsa AlDoseri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quotes/index.html",
    "href": "quotes/index.html",
    "title": "Quotes",
    "section": "",
    "text": "I admit that I am perhaps missing part of a larger picture that encourages or even requires the use of cop shit in the classroom. Yet when my students come to me with reams of documentation for a simple cold, or fearful that I will dole out unexcused absences for them taking job interviews, I have to wonder what that larger picture could be. I’ve only taught for a few semesters and I don’t pretend that my experiences are generalizable to the level of empirical data, but my students seem to do perfectly well in the absence—as much as I can make—of cop shit.\nSo why do we have cop shit in our classrooms?\nOne provisional answer is that the people who sell cop shit are very good at selling cop shit, whether that cop shit takes the form of a learning management system or a new pedagogical technique. Like any product, cop shit claims to solve a problem. We might express that problem like this: the work of managing a classroom, at all its levels, is increasingly complex and fraught, full of poorly defined standards, distractions to our students’ attentions, and new opportunities for grift. Cop shit, so cop shit argues, solves these problems by bringing order to the classroom. Cop shit defines parameters. Cop shit ensures compliance. Cop shit gives students and teachers alike instant feedback in the form of legible metrics.\nIn short, cop shit operates according the the logic of datafication.\n[…]\nCop shit is seductive. It makes metrics transparent. It allows for the clear progress toward learning objectives. It also subsumes education within a market logic. “Here,” cop shit says, “you will learn how to do this thing. We will know you learned it by the acquisition of this gold star. But in order for me to award you this gold star, I must parse you, sense you, track you, collect you, and—” here’s the key, “I will presume that you will attempt to flout me at every turn. We are both scamming each other, you and I, and I intend to win.” When a classroom becomes adversarial, of course, as cop shit presumes, then there must be a clear winner and loser. The student’s education then becomes not a victory for their own self-improvement or -enrichment, but rather that the teacher conquered the student’s presumed inherent laziness, shiftiness, etc. to instill some kernel of a lesson.\n— Jeffrey Moro: Against Cop Shit\n\nEngineers working for Evil, Inc. are like engineers anywhere; they include a mix of people inclined to doing quick hardcoded scripts or perhaps writing Fully General Frameworks For Exploitation Of The Global Economy. The first group ends up doing far more damage because the second group, predictably, doesn’t ship much usable software.\n— Patrick McKenzie: Credit cards as a legacy system\n\n1957 - John Backus and IBM create FORTRAN. There’s nothing funny about IBM or FORTRAN. It is a syntax error to write FORTRAN while not wearing a blue tie.\n[…]\n1972 - Dennis Ritchie invents a powerful gun that shoots both forward and backward simultaneously. Not satisfied with the number of deaths and permanent maimings from that invention he invents C and Unix.\n[…]\n1983 - In honor of Ada Lovelace’s ability to create programs that never ran, Jean Ichbiah and the US Department of Defense create the Ada programming language. In spite of the lack of evidence that any significant Ada program is ever completed historians believe Ada to be a successful public works project that keeps several thousand roving defense contractors out of gangs.\n[…]\n1990 - A committee formed by Simon Peyton-Jones, Paul Hudak, Philip Wadler, Ashton Kutcher, and People for the Ethical Treatment of Animals creates Haskell, a pure, non-strict, functional language. Haskell gets some resistance due to the complexity of using monads to control side effects. Wadler tries to appease critics by explaining that “a monad is a monoid in the category of endofunctors, what’s the problem?”\n[…]\n1996 - James Gosling invents Java. Java is a relatively verbose, garbage collected, class based, statically typed, single dispatch, object oriented language with single implementation inheritance and multiple interface inheritance. Sun loudly heralds Java’s novelty.\n2001 - Anders Hejlsberg invents C#. C# is a relatively verbose, garbage collected, class based, statically typed, single dispatch, object oriented language with single implementation inheritance and multiple interface inheritance. Microsoft loudly heralds C#’s novelty.\n— James Iry\n\nClimate is what on average we may expect; weather is what we actually get.\n— Andrew John Herbertson, Outlines of Physiography (1901)\n\nWe are as gods. We may as well get good at it.\n— Stewart Brand\n\nWithout data, you’re just another person with an opinion\n— W. Edwards Deming\n\nWithout data, you’re just another person with an opinion. […] With data, you’re still just another person with an opinion. Expert analysts understand this in their very bones.\n— Cassie Kozyrkov\n\nThe three golden rules to ensure computer security are: do not own a computer; do not power it on; and do not use it.\n— Robert Morris\n\nMy second meta-principle of statistics is the methodological attribution problem, which is that the many useful contributions of a good statistical consultant, or collaborator, will often be attributed to the statistician’s methods or philosophy rather than to the artful efforts of the statistician himself or herself. Don Rubin has told me that scientists are fundamentally Bayesian (even if they do not realize it), in that they interpret uncertainty intervals Bayesianly. Brad Efron has talked vividly about how his scientific collaborators find permutation tests and p-values to be the most convincing form of evidence. Judea Pearl assures me that graphical models describe how people really think about causality. And so on. I am sure that all these accomplished researchers, and many more, are describing their experiences accurately. Rubin wielding a posterior distribution is a powerful thing, as is Efron with a permutation test or Pearl with a graphical model, and I believe that (a) all three can be helping people solve real scientific problems, and (b) it is natural for their collaborators to attribute some of these researchers’ creativity to their methods.\nThe result is that each of us tends to come away from a collaboration or consulting experience with the warm feeling that our methods really work, and that they represent how scientists really think. In stating this, I am not trying to espouse some sort of empty pluralism — the claim that, for example, we would be doing just as well if we were all using fuzzy sets, or correspondence analysis, or some other obscure statistical method. There is certainly a reason that methodological advances are made, and this reason is typically that existing methods have their failings. Nonetheless, I think we all have to be careful about attributing too much from our collaborators’ and clients’ satisfaction with our methods.\n— Andrew Gelman: Bayesian Statistics Then and Now\n\nEven one of the most elementary things taught on a statistics course, the standard deviation, is more complex than it need be, and is considered here as an example of how convenience for mathematical manipulation often over-rides pragmatism in research methods.\n[…]\nIn essence, the claim made for the standard deviation is that we can compute a number (SD) from our observations that has a relatively consistent relationship with a number computed in the same way form the population figures. This claim, in itself, is of no great value. Reliability alone does not make that number of any valid use. For example, if the computation led to a constant whatever figures were used then there would be a perfectly consistent relationship between the parameters for the sample and population. But to what end? Surely the key issue is not how stable the statistic but whether it encapsulates what we want it to.\n[…]\nOf course, much of the rest of traditional statistics is now based on the standard deviation, but it is important to realise that it need not be.\n— Stephen Gorard: Revisiting a 90-Year-Old Debate: The Advantages of the Mean Deviation\n\nOpinionated software is only cool if you have cool opinions\n— Tom MacWright: soapbox: longitude, latitude is the right way\n\nThere was some discussion in the comments thread here about axis labels and zero. Sometimes zero has no particular meaning (for example when graphing degrees Fahrenheit), but usually it has a clear interpretation, in which case it can be pleasant to include it on the graph. On the other hand, if you’re plotting something that varies from, say 184 to 197, it would typically be a bad idea to extend the axis all the way to zero, as this would destroy your visual resolution.\nThe advice we usually give is: If zero is in the neighborhood, invite it in.\n— Andrew Gelman: Graphing advice\n\nYoung man, in mathematics you don’t understand things. You just get used to them.\n— John Von Neumann\n\nTo deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say ‘fourteen’ to yourself very loudly. Everyone does it.\n— Geoffrey Hinton: A geometrical view of perceptrons\n\nDependencies are an open invitation for other people to break your code.\n— Nathan Eastwood: useR 2021 talk on poorman\n\nRAM is cheap and thinking hurts.\n— Uwe Ligges: R-Help\n\nSoftware people are not alone in facing complexity. Physics deals with terribly complex objects even at the “fundamental” particle level. The physicist labors on, however, in a firm faith that there are unifying principles to be found, whether in quarks or in unified field theories. Einstein repeatedly argued that there must be simplified explanations of nature, because God is not capricious or arbitrary.\nNo such faith comforts the software engineer. Much of the complexity he must master is arbitrary complexity, forced without rhyme or reason by the many human institutions and systems to which his interfaces must confirm. These differ from interface to interface, and from time to time, not because of necessity but only because they were designed by different people, rather than by God.\n— Frederick P. Brooks, Jr.: No Silver Bullet —- Essence and Accident in Software Engineering\n\nOn-prem is a lock-in. Cloud is a lock-in. Every single language you program in is a type of lock-in. Python is easy to get started with, but soon you run into packaging issues and are optimizing the garbage collector. Scala is great, but everyone winds up migrating away from it. And on and on.\nEvery piece of code written in a given language or framework is a step away from any other language, and five more minutes you’ll have to spend migrating it to something else. That’s fine. You just have to decide what you’re willing to be locked into.\n— Vicki Boykis: Commit to your lock-in\n\nI believe we need a ‘Digital Earth’. A multi-resolution, three-dimensional representation of the planet, into which we can embed vast quantities of georeferenced data.\n[…]\nImagine, for example, a young child going to a Digital Earth exhibit at a local museum. After donning a head-mounted display, she sees Earth as it appears from space. Using a data glove, she zooms in, using higher and higher levels of resolution, to see continents, then regions, countries, cities, and finally individual houses, trees, and other natural and man-made objects. Having found an area of the planet she is interested in exploring, she takes the equivalent of a ‘magic carpet ride’ through a 3-D visualization of the terrain. Of course, terrain is only one of the many kinds of data with which she can interact.\n[…]\nShe is able to request information on land cover, distribution of plant and animal species, real-time weather, roads, political boundaries, and population.\n— Al Gore: The Digital Earth: Understanding our Planet in the 21st Century, 1998\n\nTo consult the statistician after an experiment is ﬁnished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.\n— R.A. Fisher\n\nHiawatha, mighty hunter,\nHe could shoot ten arrows upward,\nShoot them with such strength and swiftness\nThat the last had left the bow-string\nEre the first to earth descended.\n- This was commonly regarded\nAs a feat of skill and cunning.\nSeveral sarcastic spirits\nPointed out to him, however,\nThat it might be much more useful\nIf he sometimes hit the target.\n“Why not shoot a little straighter\nAnd employ a smaller sample?”\nHiawatha, who at college\nMajored in applied statistics,\nConsequently felt entitled\nTo instruct his fellow man\nIn any subject whatsoever\n[…]\nHiawatha, in a temper,\nQuoted parts of R. A. Fisher,\nQuoted Yates and quoted Finney,\nQuoted reams of Oscar Kempthorne,\nQuoted Anderson and Bancroft\n(practically in extenso)\nTrying to impress upon them\nThat what actually mattered\nWas to estimate the error.\n- Several of them admitted:\n“Such a thing might have its uses;\nStill,” they said, “he would do better\nIf he shot a little straighter.”\n[…]\nIn a corner of the forest\nSits alone my Hiawatha\nPermanently cogitating\nOn the normal law of errors.\nWondering in idle moments\nIf perhaps increased precision\nMight perhaps be sometimes better\nEven at the cost of bias,\nIf one could thereby now and then\nRegister upon a target.\n— . E. Mientka, “Professor Leo Moser - Reflections of a Visit”\n\nFor in much wisdom is much grief: and he that increaseth knowledge increaseth sorrow.\n— Ecclesiastes 1:18\n\nAn extra year of experience has not changed my belief in a disconnect between traditional statistical regression methods and the pure prediction algorithms. Section 8 of the paper, featuring Table 5, makes the case directly in terms of six criteria. Five of the six emerged more or less unscathed from the discussion. Criteria 2, long-time scientific truth versus possibly short-term prediction accuracy, was doubted by FHT and received vigorous push-back from Yu/Barter:\n\n…but in our experience the “truth” that traditional regression methods supposedly represent is rarely justified or validated…\n\nThis is a hard-line Breimanian point of view. That “rarely” is over the top. The truism that begins “all models are wrong” ends with “but some are useful.” Traditional models tend to err on the side of over-simplicity (not enough interactions, etc.) but still manage to capture at least some aspect of the underlying mechanism. “Eternal truth” is a little too much to ask for, but in the Neonate example we did wind up believing that respiratory strength had something lasting to do with the babies’ survival.\n[…]\nThe fathers of statistical theory — Pearson, Fisher, Neyman, Hotelling, Wald — forgot to provide us with a comprehensive theory of optimal prediction. We will have to count on the current generation of young statisticians to fill the gap and put prediction on a principled foundation.\n— Bradley Efron: Rejoinder to Prediction, Estimation, and Attribution\n\nIt’s important to be realistic: most people don’t care about program performance most of the time. Modern computers are so fast that most programs run fast enough even with very slow language implementations. In that sense, I agree with Daniel’s premise: optimising compilers are often unimportant. But “often” is often unsatisfying, as it is here. Users find themselves transitioning from not caring at all about performance to suddenly really caring, often in the space of a single day.\nThis, to me, is where optimising compilers come into their own: they mean that even fewer people need care about program performance. And I don’t mean that they get us from, say, 98 to 99 people out of 100 not needing to care: it’s probably more like going from 80 to 99 people out of 100 not needing to care. This is, I suspect, more significant than it seems: it means that many people can go through an entire career without worrying about performance. Martin Berger reminded me of A N Whitehead’s wonderful line that “civilization advances by extending the number of important operations which we can perform without thinking about them” and this seems a classic example of that at work.\n— Laurence Tratt: What Challenges and Trade-Offs do Optimising Compilers Face?\n\nPay attention to unexpected data that has no natural constituency, and to lack of data that are in high demand.\n— Whitney R. Robinson: More on meta-epistemology: an epidemiologist’s perspective\n\nHalf of what you’ll learn in medical school will be shown to be either dead wrong or out of date within five years of your graduation; the trouble is that nobody can tell you which half–so the most important thing to learn is how to learn on your own.\n— David Sackett (playing off a common phrase)\n\nFile organization and naming are powerful weapons against chaos. (Jenny Bryan)\nYour closest collaborator is you six months ago, but you don’t reply to emails. (Mark Holder)\nI will let the data speak for itself when it cleans itself. (Allison Reichel)\nWorking with data is not about rules to follow but about decisions to make. (Naupaka Zimmerman)\nI’m not worried about being scooped, I’m worried about being ignored. (Magnus Nordborg)\nTeach stats as you would cake baking: make a few before you delve into the theory of leavening agents. (Jenny Bryan)\nThe opposite of “open” isn’t “closed”. The opposite of “open” is “broken”. (John Wilbanks)\n— Collected by Karl Broman\n\nThis was not meant to be an “emperor has no clothes” kind of story, rather “the emperor has nice clothes but they’re not suitable for every occasion.” Where they are suitable, the pure prediction algorithms can be stunningly successful. When one reads an enthusiastic AI-related story in the press, there’s usually one of these algorithms, operating in enormous scale, doing the heavy lifting. Regression methods have come a long and big way since the time of Gauss.\n— Bradley Efron: Prediction, Estimation, and Attribution.\n\nThere are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.\n— Leo Breiman: Statistical Modeling: The Two Cultures.\n\nPS Please excuse my not mailing this — but I don’t know your new address.\n— Richard Feynman: Letter to his dead wife, Arline, 17 Oct 1946\n\nThe more instructions something has, the worse its design. It’s cheaper to add instructions later than to design something well.\n— Scott Berkun: How Design Makes the World\n\nWe prove that last digits are approximately uniform for distributions with an absolutely continuous distribution function. From a practical perspective, that result, of course, is only moderately interesting. For that reason, we derive a result for ‘certain’ sums of lattice-variables as well. That justification is provided in terms of stationary distributions.\n— Stephan Dlugosz and Ulrich Muller-Funk: The value of the last digit: statistical fraud detection with digit analysis\n\nThe first of McIlroy’s dicta is often paraphrased as “do one thing and do it well”, which is shortened from “Make each program do one thing well. To do a new job, build afresh rather than complicate old programs by adding new ‘features.’”\nMcIlroy’s example of this dictum is:\n\nSurprising to outsiders is the fact that UNIX compilers produce no listings: printing can be done better and more flexibly by a separate program.\n\n[…]\nMcIlroy implies that the problem is that people didn’t think hard enough, the old school UNIX mavens would have sat down in the same room and thought longer and harder until they came up with a set of consistent tools that has “unusual simplicity”. But that was never going to scale, the philosophy made the mess we’re in inevitable. It’s not a matter of not thinking longer or harder; it’s a matter of having a philosophy that cannot scale unless you have a relatively small team with a shared cultural understanding, able to to sit down in the same room.\nIf anyone can write a tool and the main instruction comes from “the unix philosophy”, people will have different opinions about what “simplicity” or “doing one thing” means, what the right way to do things is, and inconsistency will bloom, resulting in the kind of complexity you get when dealing with a wildly inconsistent language, like PHP. People make fun of PHP and javascript for having all sorts of warts and weird inconsistencies, but as a language and a standard library, any commonly used shell plus the collection of widely used *nix tools taken together is much worse and contains much more accidental complexity due to inconsistency even within a single Linux distro and there’s no other way it could have turned out. If you compare across Linux distros, BSDs, Solaris, AIX, etc., the amount of accidental complexity that users have to hold in their heads when switching systems dwarfs PHP or javascript’s incoherence. The most widely mocked programming languages are paragons of great design by comparison.\nTo be clear, I’m not saying that I or anyone else could have done better with the knowledge available in the 70s in terms of making a system that was practically useful at the time that would be elegant today. It’s easy to look back and find issues with the benefit of hindsight. What I disagree with are comments from Unix mavens speaking today; comments like McIlroy’s, which imply that we just forgot or don’t understand the value of simplicity, or Ken Thompson saying that C is as safe a language as any and if we don’t want bugs we should just write bug-free code. These kinds of comments imply that there’s not much to learn from hindsight; in the 70s, we were building systems as effectively as anyone can today; five decades of collective experience, tens of millions of person-years, have taught us nothing; if we just go back to building systems like the original Unix mavens did, all will be well. I respectfully disagree.\n— Dan Luu: The growth of command line options, 1979-Present\n\nThis isn’t to say there’s no cost to adding options – more options means more maintenance burden, but that’s a cost that maintainers pay to benefit users, which isn’t obviously unreasonable considering the ratio of maintainers to users. This is analogous to Gary Bernhardt’s comment that it’s reasonable to practice a talk fifty times since, if there’s a three hundred person audience, the ratio of time spent watching to the talk to time spent practicing will still only be 1:6. In general, this ratio will be even more extreme with commonly used command line tools.\n— Dan Luu: The growth of command line options, 1979-Present\n\nobjects: Everything that exists in R is an object.\nfunctions: Everything that happens in R is a function call.\ninterfaces: Interfaces to other languages are a part of R.\n— John Chambers - S, R, and Data Science\n\nPHP is an embarrassment, a blight upon my craft. It’s so broken, but so lauded by every empowered amateur who’s yet to learn anything else, as to be maddening. It has paltry few redeeming qualities and I would prefer to forget it exists at all.\n[…]\nDo not tell me that “good developers can write good code in any language”, or bad developers blah blah. That doesn’t mean anything. A good carpenter can drive in a nail with either a rock or a hammer, but how many carpenters do you see bashing stuff with rocks? Part of what makes a good developer is the ability to choose the tools that work best.\n[…]\nPHP is built to keep chugging along at all costs. When faced with either doing something nonsensical or aborting with an error, it will do something nonsensical. Anything is better than nothing.\n— PHP: A fractal of bad design\n\nThe joy in mathematics is often the receding of the pain.\n[…]\nClaim: You can’t control the use of knowledge.\nThe choice is not between “peaceful” and “warlike” science. For most of science, the only choice is “relevant” and “irrelevant”.\nThe only way to make sure your scientific output will not be used by the military for war: Make sure it irrelevant, or wrong, or better: Both.\nAlmost anything that will give one side an advantage will eventually be used in warfare.\nNonetheless: The individual still has the responsibility to judge what they are working on. This is often not easy.\n[…]\nAre better weapons bad?\nPeople are inclined to believe that better weapons are (morally) bad.\nI visited Japan in early 2016 - both Hiroshima and Tanegashima. I will not talk about Hiroshima…. Tanegashima is an island in the south of Japan.\n1467: Japan’s Feudal system collapses. Sengoku period starts, permanent internal warfare. No side could get an upper hand.\n76 years later, the first Musket arrives in Japan (1543). Full adoption into the battle around 1570s; decisive in battle from 1575.\nFrom 1615 onward: Japan unified, century of peace. 108 years of war, ended in 40 years.\nProlonged warfare is terrible.\nSuperior weaponry does not always mean more civilian casualties.\nSome people argue that the terrible power of nuclear weapons are what enabled the last 75 years without World Wars.\nAre better weapons bad?\nI do not have an answer. I think about the history of Tanegashima, with little result.\n[…]\nBob Morris Sr. asked me when I met what I do.\n“I study math.”\n“For whom?”\n[…]\nIt is difficult to get a man to understand something when his salary depends upon his not understanding it. People tend to pick their ideologies by function.\n— Halvar Flake - OffensiveCon 2020 Keynote\n\nData analysis is hard, and part of the problem is that few people can explain how to do it. It’s not that there aren’t any people doing data analysis on a regular basis. It’s that the people who are really good at it have yet to enlighten us about the thought process that goes on in their heads.\nImagine you were to ask a songwriter how she writes her songs. There are many tools upon which she can draw. We have a general understanding of how a good song should be structured: how long it should be, how many verses, maybe there’s a verse followed by a chorus, etc. In other words, there’s an abstract framework for songs in general. Similarly, we have music theory that tells us that certain combinations of notes and chords work well together and other combinations don’t sound good. As good as these tools might be, ultimately, knowledge of song structure and music theory alone doesn’t make for a good song. Something else is needed.\nIn Donald Knuth’s legendary 1974 essay Computer Programming as an Art, Knuth talks about the difference between art and science. In that essay, he was trying to get across the idea that although computer programming involved complex machines and very technical knowledge, the act of writing a computer program had an artistic component. In this essay, he says that\n\nScience is knowledge which we understand so well that we can teach it to a computer.\n\nEverything else is art.\nAt some point, the songwriter must inject a creative spark into the process to bring all the songwriting tools together to make something that people want to listen to. This is a key part of the art of songwriting. That creative spark is difficult to describe, much less write down, but it’s clearly essential to writing good songs. If it weren’t, then we’d have computer programs regularly writing hit songs. For better or for worse, that hasn’t happened yet.\nMuch like songwriting (and computer programming, for that matter), it’s important to realize that data analysis is an art. It is not something yet that we can teach to a computer.\n— Roger D. Peng and Elizabeth Matsui: The Art of Data Science\n\nThere are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models andtreats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems.\n— Leo Breiman: Statistical Modeling: The Two Cultures\n\nAt first glance Leo Breiman’s stimulating paper looks like an argument against parsimony and scientific insight, and in favor of black boxes with lots of knobs to twiddle. At second glance it still looks that way, but the paper is stimulating, and Leo has some important points to hammer home\n[…]\nRule 1. New methods always look better than old ones. Neural nets are better than logistic regression, support vector machines are better than neural nets, etc. In fact it is very difficult to run an honest simulation comparison, and easy to inadvertently cheat by choosing favorable examples, or by not putting as much effort into optimizing the dull old standard as the exciting new challenger.\nRule 2. Complicated methods are harder to criticize than simple ones. By now it is easy to check the efficiency of a logistic regression, but it is no small matter to analyze the limitations of a support vector machine.\n— Brad Efron: Comment on Statistical Modeling: The Two Cultures\n\nOccasionally, one sees frequentism defined in careerist terms, e.g., “A statistician who always rejects null hypotheses at the 95% level will over time make only 5% errors of the first kind.” This is not a comforting criterion for the statistician’s clients.\n[…]\nSomething important changed in the world of statistics in the new millennium. Twentieth-century statistics, even after the heated expansion of itslate period, could still be contained within the classic Bayesian–frequentist–Fisherian inferential triangle (Figure 14.1). This is not so in the twenty-first century. Some of the topics discussed in Part III—false-discovery rates,post-selection inference, empirical Bayes modeling, the lasso—fit within the triangle but others seem to have escaped, heading south from the frequentist corner, perhaps in the direction of computer science.\n— Bradley Efron & Trevor Hastie: Computer Age Statistical Inference\n\nPeople sometimes think (or complain) that working with quantitative data like this inures you to the reality of the human lives that lie behind the numbers. Numbers and measures are crude; they pick up the wrong things; they strip out the meaning of what’s happening to real people; they make it easy to ignore what can’t be counted. There’s something to those complaints. But it’s mostly a lazy critique. In practice, I find that far from distancing you from questions of meaning, quantitative data forces you to confront them. The numbers draw you in. Working with data like this is an unending exercise in humility, a constant compulsion to think through what you can and cannot see, and a standing invitation to understand what the measures really capture—what they mean, and for whom. Those regular spikes in the driving data are the pulse of everyday life as people go out to have a good time at the weekend. That peak there is the Mardi Gras parade in New Orleans. That bump in Detroit was a Garth Brooks concert. Right across the country, that is the sudden shock of the shutdown the second weekend in March. It was a huge collective effort to buy time that, as it turns out, the federal government has more or less entirely wasted. And now through May here comes the gradual return to something like the baseline level of activity from January, proceeding much more quickly in some cities than in others.\nI sit at my kitchen-counter observatory and look at the numbers. Before my coffee is ready, I can quickly pull down a few million rows of data courtesy of a national computer network originally designed by the government to be disaggregated and robust, because they were convinced that was what it would take for communication to survive a nuclear war. I can process it using software originally written by academics in their spare time, because they were convinced that sophisticated tools should be available to everyone for free. Through this observatory I can look out without even looking up, surveying the scale and scope of the country’s ongoing, huge, avoidable failure. Everything about this is absurd.\n— Kieran Healy: The Kitchen Counter Observatory\n\nSurgisphere appears to be the Theranos, or possibly the Cornell Food and Brand Lab, of medical research, and Lancet is a serial enabler of research fraud (see this news article by Michael Hiltzik), and it’s easy to focus on that. But remember all the crappy papers these journals publish that don’t get retracted, cos they’re not fraudulent, they’re just crappy. Retracting papers just cos they’re crappy—no fraud, they’re just bad science—I think that’s never ever ever gonna happen. Retraction is taken as some kind of personal punishment meted out to an author and a journal. This frustrates me to no end. What’s important is the science, not the author. But it’s not happening. So, when we hear about glamorous/seedy stories of fraud, remember the bad research, the research that’s not evilicious but just incompetent, maybe never even had a chance of working. That stuff will stay in the published literature forever, and journals love publishing it.\nAs we say in statistics, the shitty is the enemy of the good.\n\nOpen code, open data, open review . . .\n\nSo, you knew I’d get to this…\nJust remember, honesty and transparency are not enuf. Open data and code don’t mean your work is any good. A preregistered study can be a waste of time. The point of open data and code is that it makes it easier to do post-publication review. If you’re open, it makes it easier for other people to find flaws in your work. And that’s a good thing.\nAn egg is just a chicken’s way of making another egg.\nAnd the point of science and policy analysis is not to build beautiful careers. The purpose is to learn about and improve the world.\n— Andrew Gelman: bla bla bla PEER REVIEW bla bla bla\n\nSometimes somebody says something to me, like a whisper of a hint of an echo of something half-forgotten, and it lands on me like an invocation. The mania sets in, and it isn’t enough to believe; I have to know.\n[…]\nSo: the technical reason we started counting arrays at zero is that in the mid-1960’s, you could shave a few cycles off of a program’s compilation time on an IBM 7094. The social reason is that we had to save every cycle we could, because if the job didn’t finish fast it might not finish at all and you never know when you’re getting bumped off the hardware because the President of IBM just called and fuck your thesis, it’s yacht-racing time.\nThere are a few points I want to make here.\nThe first thing is that as far as I can tell nobody has ever actually looked this up.\nWhatever programmers think about themselves and these towering logic-engines we’ve erected, we’re a lot more superstitious than we realize. We tell and retell this collection of unsourced, inaccurate stories about the nature of the world without ever doing the research ourselves, and there’s no other word for that but “mythology”. Worse, by obscuring the technical and social conditions that led humans to make these technical and social decisions, by talking about the nature of computing as we find it today as though it’s an inevitable consequence of an immutable set of physical laws, we’re effectively denying any responsibility for how we got here. And worse than that, by refusing to dig into our history and understand the social and technical motivations for those choices, by steadfastly refusing to investigate the difference between a motive and a justification, we’re disavowing any agency we might have over the shape of the future. We just keep mouthing platitudes and pretending the way things are is nobody’s fault, and the more history you learn and the more you look at the sad state of modern computing the the more pathetic and irresponsible that sounds.\n— mhoye: Citation Needed\n\nOf course, someone has to write for loops. It doesn’t have to be you.\n— Jenny Bryan: Row Oriented Workflows in R With the Tidyverse\n\nAn over-simplified and dangerously reductive diagram of a data system might look like this:\nCollection → Computation → Representation\nWhenever you look at data — as a spreadsheet or database view or a visualization, you are looking at an artifact of such a system. What this diagram doesn’t capture is the immense branching of choice that happens at each step along the way. As you make each decision — to omit a row of data, or to implement a particular database structure or to use a specific colour palette you are treading down a path through this wild, tall grass of possibility. It will be tempting to look back and see your trail as the only one that you could have taken, but in reality a slightly divergent you who’d made slightly divergent choices might have ended up somewhere altogether different. To think in data systems is to consider all three of these stages at once.\n— Jer Thorp: You Say Data, I Say System\n\nThe thing that is most alluring about the Gini coefficient also turns out to be its greatest shortcoming. By collapsing the whole rainbow of the income distribution into a single statistical point of white light, it necessarily conceals much of great interest. That is of course true of any single summary measure… The best measures are those that match our purpose, or pick up on the places where important changes are happening. We should pick and mix with that in mind.\n— Angus Deaton and Angela Case\n\nAfter describing the perverse interaction between wealth gaps, education, mortality trends and political economy, Case and Deaton note that “You could crunch the Gini coefficient to as many decimal places as you like, and you’d learn next to nothing about what’s really going on here.” Quite so, but surely it is unreasonable to demand of one measure of inequality along one dimension that it shed light on complex social interactions or detect causal relationships.\nThat would be like urging us to abandon the Centigrade scale as a measure of temperature because it so badly fails to inform us about how climate change plays havoc with rainfall patterns, the incidence of extreme weather events, or the extent of sea level rises. You could calculate average global temperature increases to as many decimal places in degrees Celsius as you liked, and you would be none the wiser about the diversity of consequences of climate change around the world.\n— Francisco Ferreira\n\nSoftware has been around since the 1940s. Which means that people have been faking their way through meetings about software, and the code that builds it, for generations. Now that software lives in our pockets, runs our cars and homes, and dominates our waking lives, ignorance is no longer acceptable. The world belongs to people who code. Those who don’t understand will be left behind. (Josh Tyrangiel, header)\n[…]\nA computer is a clock with benefits. They all work the same, doing second-grade math, one step at a time: Tick, take a number and put it in box one. Tick, take another number, put it in box two. Tick, operate (an operation might be addition or subtraction) on those two numbers and put the resulting number in box one. Tick, check if the result is zero, and if it is, go to some other box and follow a new set of instructions.\n[…]\nIt’s a good and healthy exercise to ponder what your computer is doing right now. Maybe you’re reading this on a laptop: What are the steps and layers between what you’re doing and the Lilliputian mechanisms within? When you double-click an icon to open a program such as a word processor, the computer must know where that program is on the disk. It has some sort of accounting process to do that. And then it loads that program into its memory—which means that it loads an enormous to-do list into its memory and starts to step through it. What does that list look like?\nMaybe you’re reading this in print. No shame in that. In fact, thank you. The paper is the artifact of digital processes. Remember how we put that “a” on screen? See if you can get from some sleepy writer typing that letter on a keyboard in Brooklyn, N.Y., to the paper under your thumb. What framed that fearful symmetry?\nThinking this way will teach you two things about computers:\nOne, there’s no magic, no matter how much it looks like there is. There’s just work to make things look like magic.\nAnd two, it’s crazy in there.\n[…]\nYou can tell how well code is organized from across the room. Or by squinting or zooming out. The shape of code from 20 feet away is incredibly informative. Clean code is idiomatic, as brief as possible, obvious even if it’s not heavily documented. Colloquial and friendly. As was written in Structure and Interpretation of Computer Programs (aka SICP), the seminal textbook of programming taught for years at MIT, “A computer language is not just a way of getting a computer to perform operations – it is a novel formal medium for expressing ideas about methodology. Thus, programs must be written for people to read, and only incidentally for machines to execute.” A great program is a letter from current you to future you or to the person who inherits your code. A generous humanistic document.\nOf course all of this is nice and flowery; it needs to work, too.\n— Paul Ford: What is Code?\n\nThe Postulates of Mathematics Were Not on the Stone Tablets that Moses Brought Down from Mt. Sinai. It is necessary to emphasize this. We begin with a vague concept in our minds, then we create various sets of postulates, and gradually we settle down to one particular set. In the rigorous postulational approach, the original concept is now replaced by what the postulates define. This makes further evolution of the concept rather difficult and as a result tends to slow down the evolution of mathematics. It is not that the postulation approach is wrong, only that its arbitrariness should be clearly recognized, and we should be prepared to change postulates when the need becomes apparent.\n— Richard Hamming\n\nTeachers should prepare the student for the student’s future, not for the teacher’s past.\n[…]\nEducation is what, when, and why to do things. Training is how to do it.\n[…]\nIn science, if you know what you are doing, you should not be doing it. In engineering, if you do not know what you are doing, you should not be doing it. Of course, you seldom, if ever, see either pure state.\n—Richard Hamming: The Art of Doing Science and Engineering\n\nI note with fear and horror that even in 1980, language designers and users have not learned this lesson [mandatory run-time checking of array bounds]. In any respectable branch of engineering, failure to observe such elementary precautions would have long been against the law.\n[…]\nI conclude that there are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult. It demands the same skill, devotion, insight, and even inspiration as the discovery of the simple physical laws which underlie the complex phenomena of nature. It also requires a willingness to accept objectives which are limited by physical, logical, and technological constraints, and to accept a compromise when conflicting objectives cannot be met. No committee will ever do this until it is too late.\n— C.A.R. Hoare: The Emperor’s Old Clothes\n\nI like this definition: A tool addresses human needs by amplifying human capabilities. That is, a tool converts what we can do into what we want to do. A great tool is designed to fit both sides.\n— Bret Victor: A Brief Rant on the Future of Interaction Design\n\nDiscoverability is often cited as npm’s biggest flaw. Many blog posts – scratch that, entire websites – have been created to try and mitigate the difficulty of finding what you need on npm. Everyone has an idea about how to make it easier to find needles in the haystack, but no-one bothers to ask what all this hay is doing here in the first place.\n— Rich Harris: Small modules: it’s not quite that simple\n\nBrave to write it, all by itself, and then brave to show it. It is like opening your ribcage, and letting someone see the little bird you have inside. What if they don’t love the bird? It’s not like you can change it. I mean.. That’s your bird.\n— Tycho: Fan Fiction\n\nLook, you have two choices. You can say, “I’m a pessimist, nothing’s gonna work, I’m giving up, I’ll help ensure that the worst will happen.” Or you can grasp onto the opportunities that do exist, the rays of hope that exist, and say, “Well, maybe we can make it a better world.” It’s not a much of a choice.\n— Noam Chomskey: Interview\n\nThere’s really only one important question worth asking, which is: what is a life well-lived?… That’s a question that can’t be answered, but one thing we can say, with a lot of certainty, is that a life well-lived is not going to be a life in which every moment is scrutinized.\n— Frank Lantz: Hearts and Minds\n\nI hate abstraction. Here are some examples.\n— Adam Cadre: Fatal abstraction\n\nI find that the single thing which inhibits young professionals, new students most severely, is their acceptance of standards that are too low. If I ask a student whether her design is as good as Chartres, she often smiles tolerantly at me as if to say, “Of course not, that isn’t what I am trying to do…. I could never do that.”\nThen, I express my disagreement, and tell her: “That standard must be our standard. If you are going to be a builder, no other standard is worthwhile. That is what I expect of myself in my own buildings, and it is what I expect of my students.” Gradually, I show the students that they have a right to ask this of themselves, and must ask this of themselves. Once that level of standard is in their minds, they will be able to figure out, for themselves, how to do better, how to make something that is as profound as that.\n— Christopher Alexander: Foreward to Patterns of Software\n\nThe Stone Age didn’t end for lack of stone, and the Oil Age will end long before the world runs out of oil.\n— Sheik Ahmed Zaki Yamani\n\n[I told the fourth-graders] I was thinking of a number between 1 and 10,000. … They still cling stubbornly to the idea that the only good answer is a yes answer. This, of course, is the result of miseducation in which “right answers” are the only ones that pay off. They have not learned how to learn from a mistake, or even that learning from mistakes is possible. If they say, “Is the number between 5,000 and 10,000?” and I say yes, they cheer; if I say no, they groan, even though they get exactly the same amount of information in either case. The more anxious ones will, over and over again, ask questions that have already been answered, just for the satisfaction of hearing a yes.\n— John Holt: How Children Fail\n\nIan is a game design teacher and a professional skeptic. People call him a “curmudgeon”, but they don’t really understand how much love, how much actual faith, that kind of skepticism takes. On a pretty regular basis one of us will IM the other something like “help” or “fuck” or “people are terrible”.\nOnly when you fully believe in how wonderful something is supposed to be does every little daily indignity start to feel like some claw of malaise. At least, that’s how I explain Ian to other people.\n— Leigh Alexander: The Unearthing\n\nQ: So it’s fine to say, everybody should learn a little bit about how to program and this way of thinking because it’s valuable and important. But then maybe that’s just not realistic. Donald Knuth told me that he thinks two percent of the population have brains wired the right way to think about programming.\nA: That same logic would lead you to say that one percent of the US’s population is wired to understand Mandarin. The reasoning there is equivalent.\n— [Hal Abselson: Interview]\n\nMany professions require some form of programming. Accountants program spreadsheets; musicians program synthesizers; authors program word processors; and web designers program style sheets.\n[…]\nThe typical course on programming teaches a “tinker until it works” approach. When it works, students exclaim “It works!” and move on. Sadly, this phrase is also the shortest lie in computing, and it has cost many people many hours of their lives.\n[…]\nBy “good programming,” we mean an approach to the creation of software that relies on systematic thought, planning, and understanding from the very beginning, at every stage, and for every step. To emphasize the point, we speak of systematic program design and systematically designed programs. Critically, the latter articulates the rationale of the desired functionality. Good programming also satisfies an aesthetic sense of accomplishment; the elegance of a good program is comparable to time-tested poems or the black-and-white photographs of a bygone era. In short, programming differs from good programming like crayon sketches in a diner from oil paintings in a museum. No, this book won’t turn anyone into a master painter. But, we would not have spent fifteen years writing this edition if we didn’t believe that\neveryone can design programs\nand\neveryone can experience the satisfaction that comes with creative design.\n[…]\nWhen you were a small child, your parents taught you to count and perform simple calculations with your fingers: “1 + 1 is 2”; “1 + 2 is 3”; and so on. Then they would ask “what’s 3 + 2?” and you would count off the fingers of one hand. They programmed, and you computed. And in some way, that’s really all there is to programming and computing. Now it is time to switch roles.\n— How to Design Programs\n\nYou can’t sell someone the solution before they’ve bought the problem.\n— Chip Morningstar: Smart people can rationalize anything\n\nwhen you don’t create things, you become defined by your tastes rather than ability. your tastes only narrow & exclude people. so create\n— why the lucky stiff\n\nWhen you believe you have a future, you think in terms of generations and years. When you do not, you live not just by the day – but by the minute.\n— Iris Chang – Suicide Note\n\nIt seems like most people ask: “How can I throw my life away in the least unhappy way?”\n— Stewart Brand\n\nQ: Are you ever afraid of someone stealing your thunder?\nA: I think anybody really good is going to want to do their own thing. Anybody who’s not really good, you don’t have to worry too much about.\n— Chris Hecker: Interview\n\nLiving organisms are shaped by evolution to survive, not necessarily to get a clear picture of the universe. For example, frogs’ brains are set up to recognize food as moving objects that are oblong in shape. So if we take a frog’s normal food – flies – paralyze them with a little chloroform and put them in front of the frog, it will not notice them or try to eat them.\nIt will starve in front of its food! But if we throw little rectangular pieces of cardboard at the frog it will eat them until it is stuffed! The frog only sees a little of the world we see, but it still thinks it perceives the whole world.\nNow, of course, we are not like frogs! Or are we?\n— Alan Kay: The Center of “Why?”\n\nPick a plane or a cave wall to project the shadow of the Real World onto, and tell a story about the outlines it makes. The trick is to shrug and smile and pick another plane and do it all again to get a completely different shadow, until you find the one most useful for the day. It’s a magic trick for most folks.\n— Down is just the most common way out\n\nTo an architect, imagination is mostly about the future. To invent the future, one must live in it, which means living (at least partly) in a world that does not yet exist. Just as a driver whizzing along a highway pays more attention to the front window than the rear, the architect steers by looking ahead. This can sometimes make them seem aloof or absent-minded, as if they are someplace else. In fact, they are. For them, the past is a lesson, the present is fleeting; but the future is real. It is infinite and malleable, brimming with possibility.\n— Danny Hillis: The Power of Conviction\n\nIf the first line of your [R] script is setwd(\"C:\\Users\\jenny\\path\\that\\only\\I\\have\"), I will come into your lab and SET YOUR COMPUTER ON FIRE.\n— Jenny Bryan: here, here\n\nIt becomes important in such a climate of opinion to emphasize that books do not store knowledge. They contain symbolic codes that can serve us as external mnemonics for knowledge. Knowledge can exist only in living human minds.\n— Kieran Egan: The Educated Mind: How Cognitive Tools Shape Our Understanding\n\nPeople are much smarter when they can use their full intellect and when they can relate what they are learning to situations or phenomena which are real to them. The natural reaction, when someone is having trouble understanding what you are explaining, is to break up the explanation into smaller pieces and explain the pieces one by one. This tends not to work, so you back up even further and fill in even more details.\nBut human minds do not work like computers: it is harder, not easier, to understand something broken down into all the precise little rules than to grasp it as a whole. It is very hard for a person to read a computer assembly language program and figure out what it is about…\nStudying mathematics one rule at a time is like studying a language by first memorizing the vocabulary and the detailed linguistic rules, then building phrases and sentences, and only afterwards learning to read, write, and converse. Native speakers of a language are not aware of the linguistic rules: they assimilate the language by focusing on a higher level, and absorbing the rules and patterns subconsciously. The rules and patterns are much harder for people to learn explicitly than is the language itself.\n— William Thurston: Mathematical Education\n\nA lot of the stuff going on [in AI] is not very ambitious. In machine learning, one of the big steps that happened in the mid-’80s was to say, “Look, here’s some real data – can I get my program to predict accurately on parts of the data that I haven’t yet provided to it?” What you see now in machine learning is that people see that as the only task.\n— Stuart Russell\n\nLike most readers, I had functionally consigned [our game] to the furnace. I had let it float away on one of those little lantern boats in a way that brought me closure, if no one else. Insufficient.\nFucking insufficient.\nYou have to get back on the horse. Somehow, and I don’t know how this kind of thing starts, we have started to lionize horseback-not-getting-on: these casual, a priori assertions of inevitable failure, which is nothing more than a gauze draped over your own pulsing terror. Every creative act is open war against The Way It Is. What you are saying when you make something is that the universe is not sufficient, and what it really needs is more you. And it does, actually; it does. Go look outside. You can’t tell me that we are done making the world.\n— Tycho: A Matter of Scale\n\nQ: At the [NYU] Game Center, we’re interested in the role of the university as an alternate place for thinking about games… What in your opinion are some of the big interesting problems that students should be working on?\nA: My advice for students is… I question the question. I don’t think there are problems that students should be working on. I think students should be making games that are interesting and push the boundaries, and those will generate the problems.\n— Chris Hecker\n\nA complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.\n— John Gall: Systemantics\n\nWe have this limited bubble of experience. We can only have so many experiences in our lifetime to build models from, and we’re abstracting from that data. We’ve found, through evolution, two ways to get more data, to build more elaborate models of the world. One is to have toy experiences, little counterfeit experiences. The other one is to learn from the experience of others. When somebody tells you a story, you can actually learn from that story, incorporate it into your model of the world to make your model more accurate based upon that data that you got from somebody else. So over time, we have come to call one of these things “play” and the other one “storytelling”. These are both fundamentally educational technologies that allow us to build more elaborate models of the world around us, by supplanting our limited experience with other experiences.\n— Will Wright: Gaming Reality\n\nJohn [McCarthy]’s world is a world of ideas, a world in which ideas don’t belong to anyone, and when an idea is wrong, just the idea - not the person - is wrong. A world in which ideas are like young birds, and we catch them and proudly show them to our friends. The bird’s beauty and the hunter’s are distinct….\nSome people won’t show you the birds they’ve caught until they are sure, certain, positive that they - the birds, or themselves - are gorgeous, or rare, or remarkable. When your mind can separate yourself from your bird, you will share it sooner, and the beauty of the bird will be sooner enjoyed. And what is a bird but for being enjoyed?\n— Richard Gabriel: The Design of Parallel Programming Languages"
  },
  {
    "objectID": "posts/me-earl-and-the-dying-girl/index.html",
    "href": "posts/me-earl-and-the-dying-girl/index.html",
    "title": "Me and Earl and the Dying Girl (and myself)",
    "section": "",
    "text": "I loved it. Every moment of it. To my mind, only a few words were lodged in my head as I watched the film, which were poignant enough to set the tone for the ride I was about to experience.\nPausing for a bit; this is the first review I’d read in the top right corner of the Google search on the film.\n\nThe beauty of “Me and Earl and the Dying Girl” — and it truly is a beautiful film, both visually and emotionally — is that it doesn’t pander to anyone.\n\n\n- S. Jhoanna Robledo, Common Sense Media\n\nThe story is honest and pure; two qualities I admire the most in the world. I strive towards them and even as I write this article, I struggle to meet them. What I’m writing is less of a review, and more of a deep dive into my own subconscious. So let’s dive in.\nMe, and Earl, and the Dying girl, is a story about Greg Gaines — a high school kid who makes movie parodies with his work partner Earl — as he’s forced to console a girl named Rachel, who has just been diagnosed with Leukemia. Rachel rebukes him at first, but she soon goes along with it after seeing Greg’s funny side, in a rather humorous conversation they have, on how she should treat people who ask her stupid questions. This is where Greg, unfortunately, falls in love with her.\nWhat begins after that is a wonderful tale of their daily meetups and hangouts, and eventually how they finally get closer. Greg’s emotional fences, start to crumble down and he gradually shows his truer self. A self that he was never comfortable to reveal.\nGreg’s stubborn refusal to fit in to any of his cliques at school, spoke volumes of my own childhood. Instead of befriending people, I spoke less to them, and during college, I was pretty much like him. My intentions were the same in both cases, and the similarity to Greg’s situation, uncanny. He/I did it in order to spare himself/myself the heartbreak of rejection, and his/my self-hating tendencies that were built up, aggravated that feeling. We had experienced the same things growing up, albeit at a slightly slower pace. I really can’t say that I’m better off now, but I’ve learnt to acknowledge those experiences and my feelings much more.\nThe movie was filled with beautiful scenes from their homemade films, and what was especially noteworthy was the montage of Rachel attempting to watch each (and every) one of their movies. A film about making bad films, and one of the characters loves it!\nThe plot was simple, and their lives even simpler, yet it made me delighted and happier. And by the end of it, heartbroken.\nPerhaps, it was those touching scenes when they huddled together, all comfy and relaxed, watching one flick after another, that the film really portrayed how innocent and simple it all was. There was a possibility of romance, but nothing cropped up; again stating how this film didn’t pander to anyone. I can’t really say any more about the film. It is now, one of my fondest memories.\nAfter it was over, I felt a strong sense that I needed to write about it. Almost as if, it rekindled that short and brief moment when I loved writing. So far, I’ve just worked on a stick comic on how colors could be interpreted differently depending on the subject. I might post about this later. And a prose on a crush I once had. Soon after, I struggled to put my pen to paper. Brief as it was, it meant so much to me. This movie made me want to start that again for some reason.\nPerhaps, it was Greg’s life as a filmmaker that echoed my own yearnings in life. To make something from nothing; to manufacture a whole world out of sheer joy of seeing it come to life. Where anything was possible…\nIf you’d actually watched the film, you’d probably think it didn’t deserve this much praise, and for lack of a better word, that I lack taste. However, knowing how it made me feel in a tumultuous time in my life, I still think my perspective holds. Hope it makes you feel the same way."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html",
    "href": "posts/foodies-guide-to-finding-a-home/index.html",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "",
    "text": "I am a foodie looking for a place stay in Bahrain. I want to study certain areas in Bahrain and the kind of restaurants that surround them.\nI think that a lot of people, not just the youth could benefit from this, since the issue isn’t just finding a decent place to stay in Bahrain, but finding one that best serves their culinary interests perhaps.\nI mean there are obviously much better factors to look at besides food. However for this problem I want to stick to what I can gain from Foursquare with a free license. Thus, by neatly categorizing areas based on their attributes such as frequency of coffee shops, closeness to malls etc; I can can make a better guesstimate of where they might stay.\nFoursquare allows us to grab information on venues surrounding a given location, and therefore we will look into the most frequent kind of venues surrounding a given area, and cluster areas them on that.\nSo let’s get started!"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#scrap-bahrain-citiestown-data-from-wikipedia",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#scrap-bahrain-citiestown-data-from-wikipedia",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Scrap Bahrain Cities/Town Data from Wikipedia",
    "text": "Scrap Bahrain Cities/Town Data from Wikipedia\nI need to scrap data from Wikipedia to lookup towns and cities in Bahrain. We’re going to use the popular webscraper Beautiful Soup to do that.\n\nurl = 'https://en.wikipedia.org/wiki/Category:Populated_places_in_Bahrain'\nhtml_doc = requests.get(url).text # Get HTML Doc\nsoup = BeautifulSoup(html_doc, 'html.parser') # Parse using bs4\nblocks = soup.find_all(\"div\", {\"class\": \"mw-category-group\"})[1:]\n\nbh_data=[]\nfor block in blocks:\n    places = block.find('ul').find_all('li')\n    for place in places:\n        bh_data.append(place.a.text.split(',')[0])\n\nbh_data = pd.DataFrame(bh_data, columns=['Area'])\nremove_places = ['Rifa and Southern Region', 'Northern City'] # Exclude these places\nbh_data = bh_data[bh_data['Area'].apply(lambda item : item not in remove_places)].reset_index(drop=True)\nbh_data.head(5)\n\n\n\n\n\n\n\n\nArea\n\n\n\n\n0\nA'ali\n\n\n1\nAbu Baham\n\n\n2\nAbu Saiba\n\n\n3\nAl Garrya\n\n\n4\nAl Hajar\n\n\n\n\n\n\n\n\n\nSo there are about 82 areas in Bahrain to study."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#retrieving-coordinates-via-a-geocoder",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#retrieving-coordinates-via-a-geocoder",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Retrieving Coordinates via a Geocoder",
    "text": "Retrieving Coordinates via a Geocoder\nAfter that, we need to geocode them; convert them from a simple address to latitude & longitude values.\nPopular geocoders like OpenStreetMap & Map Quest will be used.\n\nimport os\napikey = \"API-KEY-XXXXXXXXXXX\"\nimport geocoder\n\nlats = []\nlngs = []\nfor city in bh_data['Area']:\n    geocoder_type = 'osm'\n    try:\n        g = geocoder.osm(f\"{city}, Bahrain\", key=apikey)\n        geodata = g.json\n        lats.append(geodata['lat'])\n    except:\n        geocoder_type = 'MAPQUEST'\n        g = geocoder.mapquest(f\"{city}, Bahrain\", key=apikey)\n        geodata = g.json\n        lats.append(geodata['lat'])\n    lngs.append(geodata['lng'])\n    print(city, \"|\", geocoder_type)\nbh_data['Latitude'] = lats\nbh_data['Longitude'] = lngs\n\nThese are the first few of them that were geocoded!\n\n\n\n\n\n\n\n\n\nArea\nLatitude\nLongitude\n\n\n\n\n0\nA'ali\n26.154454\n50.527364\n\n\n1\nAbu Baham\n26.205737\n50.541668\n\n\n2\nAbu Saiba\n30.325299\n48.266157\n\n\n3\nAl Garrya\n26.232690\n50.578110\n\n\n4\nAl Hajar\n26.225405\n50.590138"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#visualization-on-a-map",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#visualization-on-a-map",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Visualization on a Map",
    "text": "Visualization on a Map\nWe will now use Folium to visualize the map of Bahrain along with each area as points on the map\n\n# create map of Bahrain using latitude and longitude values\nlatitude, longitude = 26.0766404, 50.334118\n\nmap_bahrain = folium.Map(location=[latitude, longitude], zoom_start=10)\n\n# add markers to map\nfor lat, lng, city in zip(bh_data['Latitude'], bh_data['Longitude'],\n                                           bh_data['Area']):\n    \n    label = folium.Popup(city, parse_html=True)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=True,\n        fill_color='#3186cc',\n        fill_opacity=0.7,\n        parse_html=True).add_to(map_bahrain)  \nmap_bahrain\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#foursquare-exploring-areas-for-food-places",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#foursquare-exploring-areas-for-food-places",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Foursquare: Exploring Areas for Food Places",
    "text": "Foursquare: Exploring Areas for Food Places\nFuthermore, we’ll leverage the Foursquare API to gather the most common types of restaurants associated with an area within 500m of its center. We’ll then look at various food places and restaurants and extract their types for further analysis.\nNote: To filter only restaurants & food places, we will use the specific “Food” category ID : 4d4b7105d754a06374d81259\n\nfood_categoryId = \"4d4b7105d754a06374d81259\"\n\nAlright, let’s look at all food places surrouding the first area within a 500m radius\n\n\n… which happens to be A’ali\n\n\n\nradius = 500\nlat, lng = bh_data[['Latitude', 'Longitude']].iloc[0].values\n\nurl = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&categoryId={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, VERSION, lat, lng, food_categoryId, radius, LIMIT)\nresults = requests.get(url).json()\n\nLooking at the first food place in the results json, we get this output:\n\nresults['response']['venues'][0]\n\n{'id': '4e99da8f8231878c15393aa2',\n 'name': 'Costa Coffee',\n 'location': {'lat': 26.157464331750106,\n  'lng': 50.52587327276449,\n  'labeledLatLngs': [{'label': 'display',\n    'lat': 26.157464331750106,\n    'lng': 50.52587327276449}],\n  'distance': 366,\n  'cc': 'BH',\n  'city': 'Madīnat ‘Īsá',\n  'state': 'al Muḩāfaz̧ah Al Janūbīyah',\n  'country': 'البحرين',\n  'formattedAddress': ['Madīnat ‘Īsá', 'البحرين']},\n 'categories': [{'id': '4bf58dd8d48988d1e0931735',\n   'name': 'Coffee Shop',\n   'pluralName': 'Coffee Shops',\n   'shortName': 'Coffee Shop',\n   'icon': {'prefix': 'https://ss3.4sqi.net/img/categories_v2/food/coffeeshop_',\n    'suffix': '.png'},\n   'primary': True}],\n 'referralId': 'v-1631999729',\n 'hasPerk': False}\n\n\n\n\nThie first venue is Costa Coffee, and has a category: Coffee Shop\n\n\nSo now let’s build a helpful function to extract the category of each food place. We’ll use the same area as an example.\n\n# function that extracts the category of the restaurant\ndef get_category_type(row):\n    try:\n        categories_list = row['categories']\n    except:\n        categories_list = row['venue.categories']\n        \n    if len(categories_list) == 0:\n        return None\n    else:\n        return categories_list[0]['name']\n    \nvenues = results['response']['venues']\n    \nnearby_food = pd.json_normalize(venues) # flatten JSON\n\n# filter columns\nfiltered_columns = ['name', 'categories', 'location.lat', 'location.lng']\nnearby_food = nearby_food.loc[:, filtered_columns]\n\n# filter the category for each row\nnearby_food['categories'] = nearby_food.apply(get_category_type, axis=1)\n\n# clean columns\nnearby_food.columns = [col.split(\".\")[-1] for col in nearby_food.columns]\n\nnearby_food.head()\n\n\n\n\n\n\n\n\nname\ncategories\nlat\nlng\n\n\n\n\n0\nCosta Coffee\nCoffee Shop\n26.157464\n50.525873\n\n\n1\nChilis Aali\nDiner\n26.152996\n50.526268\n\n\n2\nLoop Cafe\nCafé\n26.156017\n50.531527\n\n\n3\nHospital Resturant (كافيتيريا المستشفى)\nRestaurant\n26.153012\n50.526232\n\n\n4\nكفتيريا المستشفى\nRestaurant\n26.153455\n50.528375\n\n\n\n\n\n\n\n\n\nThese are some of them, in total it returns 19 food places around A’ali."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#exploring-all-areas",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#exploring-all-areas",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Exploring All Areas",
    "text": "Exploring All Areas\n\n\nWe’ve got still got 82 places to explore, so let’s create a function to do this task much faster.\n\n\n\ndef getNearbyFoods(names, latitudes, longitudes, radius=500):\n    food_categoryId = \"4d4b7105d754a06374d81259\"\n    foods_list=[]\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n\n            \n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&categoryId={}&radius={}&limit={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            food_categoryId,\n            radius, \n            LIMIT)\n            \n        # make the GET request\n        try:\n            results = requests.get(url).json()[\"response\"]['venues']\n        except:\n            print(results)\n            raise KeyError\n        \n        venue_list = []\n        # return only relevant information for each nearby food place\n        for v in results:\n            vname, vlat, vlng = v['name'], v['location']['lat'], v['location']['lng']\n            try:\n                vcategory = v['categories'][0]['name']\n                venue_list.append((name, \n                                    lat, \n                                    lng,\n                                    vname, \n                                    vlat,\n                                    vlng,\n                                    vcategory))\n            except:\n                continue\n        foods_list.append(venue_list)\n    nearby_foods = pd.DataFrame([item for venue_list in foods_list for item in venue_list])\n    nearby_foods.columns = ['Area', \n                  'Area Latitude', \n                  'Area Longitude',\n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_foods)\n\nWe run the above function on each area and create a new dataframe called bh_foods.\n\nbh_food = getNearbyFoods(bh_data['Area'], bh_data['Latitude'], \n                                   bh_data['Longitude'], 500)\n\n\nbh_food.head()\n\n\n\n\n\n\n\n\nArea\nArea Latitude\nArea Longitude\nVenue\nVenue Latitude\nVenue Longitude\nVenue Category\n\n\n\n\n0\nA'ali\n26.154454\n50.527364\nCosta Coffee\n26.157464\n50.525873\nCoffee Shop\n\n\n1\nA'ali\n26.154454\n50.527364\nChilis Aali\n26.152996\n50.526268\nDiner\n\n\n2\nA'ali\n26.154454\n50.527364\nLoop Cafe\n26.156017\n50.531527\nCafé\n\n\n3\nA'ali\n26.154454\n50.527364\nكفتيريا المستشفى\n26.153455\n50.528375\nRestaurant\n\n\n4\nA'ali\n26.154454\n50.527364\nHospital Resturant (كافيتيريا المستشفى)\n26.153012\n50.526232\nRestaurant\n\n\n\n\n\n\n\n\n\nThis gives us a whopping 1962 food places\n\n\nWe can study the count for each area…\n\nbh_food.groupby('Area').count().head()\n\n\n\n\n\n\n\n\nArea Latitude\nArea Longitude\nVenue\nVenue Latitude\nVenue Longitude\nVenue Category\n\n\nArea\n\n\n\n\n\n\n\n\n\n\nA'ali\n19\n19\n19\n19\n19\n19\n\n\nAbu Baham\n9\n9\n9\n9\n9\n9\n\n\nAl Daih\n50\n50\n50\n50\n50\n50\n\n\nAl Dair\n9\n9\n9\n9\n9\n9\n\n\nAl Garrya\n50\n50\n50\n50\n50\n50\n\n\n\n\n\n\n\nWe’ve trimmed out the remaining areas for brevity’s sake.\n\n\nWhat’s interesting to notice from this data, is that there are 88 unique categories for food.\n\n\n\n\nSome of them include: Coffee Shop, Diner, Café, Restaurant, Breakfast Spot and so on."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#most-common-food-places",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#most-common-food-places",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Most Common Food Places",
    "text": "Most Common Food Places\nOur solution relies on segmenting areas based on the most common type of food places within that area. This gives us an idea about the kind of area it is from a culinary point-of-view, and allowing us to make judgments on whether the food is ideal to our taste or not. We also want to factor in the total number of food places within an area since some places in Bahrain may not be ideal to live in if they don’t even have enough places to eat.\nUsing the dataframe bh_food, we form a one-hot encoding of the Venue Category field that produces new columns for each category. Each record in this table corresponds to a certain venue and a 1 is placed in the category field for that area. The only other field that is retained is the area name. We will call this bh_onehot.\n\n# one hot encoding\nbh_onehot = pd.get_dummies(bh_food[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\nbh_onehot = pd.concat([bh_food[['Area']], bh_onehot], axis=1) \n\nbh_onehot.head()\n\n\n\n\n\n\n\n\nArea\nAfghan Restaurant\nAfrican Restaurant\nAmerican Restaurant\nArepa Restaurant\nAsian Restaurant\nBBQ Joint\nBagel Shop\nBakery\nBistro\nBreakfast Spot\nBubble Tea Shop\nBuffet\nBurger Joint\nBurrito Place\nCafeteria\nCafé\nChaat Place\nChinese Restaurant\nCoffee Shop\nCollege Lab\nComfort Food Restaurant\nCreperie\nCuban Restaurant\nCupcake Shop\nDeli / Bodega\nDessert Shop\nDiner\nDoner Restaurant\nDonut Shop\nDumpling Restaurant\nEastern European Restaurant\nEgyptian Restaurant\nFalafel Restaurant\nFarmers Market\nFast Food Restaurant\nFilipino Restaurant\nFish & Chips Shop\nFood\nFood Court\nFood Truck\nFrench Restaurant\nFried Chicken Joint\nFrozen Yogurt Shop\nGas Station\nGastropub\nGreek Restaurant\nHalal Restaurant\nHookah Bar\nHot Dog Joint\nIce Cream Shop\nIndian Restaurant\nIraqi Restaurant\nItalian Restaurant\nJapanese Restaurant\nJuice Bar\nKebab Restaurant\nKorean Restaurant\nLebanese Restaurant\nMediterranean Restaurant\nMexican Restaurant\nMiddle Eastern Restaurant\nMoroccan Restaurant\nMovie Theater\nNew American Restaurant\nNoodle House\nPastry Shop\nPersian Restaurant\nPie Shop\nPizza Place\nPortuguese Restaurant\nRestaurant\nSalad Place\nSandwich Place\nSeafood Restaurant\nShawarma Place\nSnack Place\nSouth Indian Restaurant\nSteakhouse\nSupermarket\nSushi Restaurant\nTea Room\nThai Restaurant\nTheme Restaurant\nTibetan Restaurant\nTurkish Restaurant\nVegetarian / Vegan Restaurant\nVietnamese Restaurant\nWings Joint\n\n\n\n\n0\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nA'ali\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\nNow, let’s group rows by area and by taking the mean of the frequency of occurrence of each category, along with the number of food places surrouding it (NumberOfFoodPlaces).\nLooking at the number of food places is significant considering that some areas have fewer restaurants, and could be a valid factor to segment, if a “foodie” is looking for a place to stay.\n\nbh_grouped = bh_onehot.groupby(['Area']).mean().reset_index()\nbh_grouped['NumberOfFoodPlaces'] = bh_onehot[['Area']].value_counts(sort=False).values\nbh_grouped.head()\n\n\n\n\n\n\n\n\nArea\nAfghan Restaurant\nAfrican Restaurant\nAmerican Restaurant\nArepa Restaurant\nAsian Restaurant\nBBQ Joint\nBagel Shop\nBakery\nBistro\nBreakfast Spot\nBubble Tea Shop\nBuffet\nBurger Joint\nBurrito Place\nCafeteria\nCafé\nChaat Place\nChinese Restaurant\nCoffee Shop\nCollege Lab\nComfort Food Restaurant\nCreperie\nCuban Restaurant\nCupcake Shop\nDeli / Bodega\nDessert Shop\nDiner\nDoner Restaurant\nDonut Shop\nDumpling Restaurant\nEastern European Restaurant\nEgyptian Restaurant\nFalafel Restaurant\nFarmers Market\nFast Food Restaurant\nFilipino Restaurant\nFish & Chips Shop\nFood\nFood Court\nFood Truck\nFrench Restaurant\nFried Chicken Joint\nFrozen Yogurt Shop\nGas Station\nGastropub\nGreek Restaurant\nHalal Restaurant\nHookah Bar\nHot Dog Joint\nIce Cream Shop\nIndian Restaurant\nIraqi Restaurant\nItalian Restaurant\nJapanese Restaurant\nJuice Bar\nKebab Restaurant\nKorean Restaurant\nLebanese Restaurant\nMediterranean Restaurant\nMexican Restaurant\nMiddle Eastern Restaurant\nMoroccan Restaurant\nMovie Theater\nNew American Restaurant\nNoodle House\nPastry Shop\nPersian Restaurant\nPie Shop\nPizza Place\nPortuguese Restaurant\nRestaurant\nSalad Place\nSandwich Place\nSeafood Restaurant\nShawarma Place\nSnack Place\nSouth Indian Restaurant\nSteakhouse\nSupermarket\nSushi Restaurant\nTea Room\nThai Restaurant\nTheme Restaurant\nTibetan Restaurant\nTurkish Restaurant\nVegetarian / Vegan Restaurant\nVietnamese Restaurant\nWings Joint\nNumberOfFoodPlaces\n\n\n\n\n0\nA'ali\n0.0\n0.0\n0.00\n0.00\n0.00\n0.000000\n0.0\n0.052632\n0.0\n0.052632\n0.0\n0.0\n0.00\n0.0\n0.000000\n0.210526\n0.0\n0.0\n0.105263\n0.0\n0.0\n0.0\n0.0\n0.105263\n0.0\n0.00\n0.052632\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.052632\n0.0\n0.000000\n0.00\n0.000000\n0.052632\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.000000\n0.0\n0.00\n0.000000\n0.0\n0.052632\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.052632\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.00\n0.0\n0.157895\n0.0\n0.052632\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n19\n\n\n1\nAbu Baham\n0.0\n0.0\n0.00\n0.00\n0.00\n0.111111\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.00\n0.0\n0.111111\n0.000000\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.00\n0.000000\n0.0\n0.111111\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.00\n0.111111\n0.000000\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.111111\n0.0\n0.00\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.111111\n0.0\n0.222222\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.00\n0.0\n0.111111\n0.0\n0.000000\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n9\n\n\n2\nAl Daih\n0.0\n0.0\n0.02\n0.00\n0.02\n0.020000\n0.0\n0.140000\n0.0\n0.100000\n0.0\n0.0\n0.04\n0.0\n0.020000\n0.060000\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.06\n0.040000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.020000\n0.0\n0.000000\n0.00\n0.000000\n0.000000\n0.00\n0.0\n0.0\n0.02\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.0\n0.020000\n0.0\n0.00\n0.020000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.160000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.00\n0.0\n0.060000\n0.0\n0.020000\n0.0\n0.0\n0.00\n0.0\n0.04\n0.02\n0.0\n0.00\n0.00\n0.0\n0.0\n0.06\n0.0\n0.0\n0.0\n50\n\n\n3\nAl Dair\n0.0\n0.0\n0.00\n0.00\n0.00\n0.111111\n0.0\n0.555556\n0.0\n0.000000\n0.0\n0.0\n0.00\n0.0\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.00\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.111111\n0.00\n0.000000\n0.000000\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.000000\n0.0\n0.00\n0.111111\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.00\n0.0\n0.111111\n0.0\n0.000000\n0.0\n0.0\n0.00\n0.0\n0.00\n0.00\n0.0\n0.00\n0.00\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n9\n\n\n4\nAl Garrya\n0.0\n0.0\n0.02\n0.02\n0.00\n0.020000\n0.0\n0.020000\n0.0\n0.120000\n0.0\n0.0\n0.02\n0.0\n0.020000\n0.060000\n0.0\n0.0\n0.060000\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.02\n0.020000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.040000\n0.06\n0.000000\n0.000000\n0.02\n0.0\n0.0\n0.04\n0.0\n0.0\n0.0\n0.0\n0.00\n0.0\n0.0\n0.000000\n0.1\n0.02\n0.000000\n0.0\n0.000000\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.020000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.02\n0.0\n0.02\n0.0\n0.180000\n0.0\n0.000000\n0.0\n0.0\n0.02\n0.0\n0.02\n0.00\n0.0\n0.02\n0.02\n0.0\n0.0\n0.00\n0.0\n0.0\n0.0\n50\n\n\n\n\n\n\n\nLet’s call this this bh_grouped. Now that we have this processed information, we can analyze this data more clearly by reordering it so that only the 10 most common type of food places for an area are retained.\n\n# Function to sort venues by most common ones\ndef return_most_common_venues(row, num_top_venues):\n    row_categories = row.iloc[1:-1]\n    row_categories_sorted = row_categories.sort_values(ascending=False)\n    \n    return row_categories_sorted.index.values[0:num_top_venues]\n\nnum_top_venues = 10\n\nindicators = ['st', 'nd', 'rd']\n\n# create columns according to number of top venues\ncolumns = ['Area', 'NumberOfFoodPlaces']\nfor ind in np.arange(num_top_venues):\n    try:\n        columns.append('{}{} Most Common Food Place'.format(ind+1, indicators[ind]))\n    except:\n        columns.append('{}th Most Common Food Place'.format(ind+1))\n\n# create a new dataframe\nfoods_sorted = pd.DataFrame(columns=columns)\nfoods_sorted[['Area','NumberOfFoodPlaces']] = bh_grouped[['Area','NumberOfFoodPlaces']]\n\nfor ind in np.arange(bh_grouped.shape[0]):\n    foods_sorted.iloc[ind, 2:] = return_most_common_venues(bh_grouped.iloc[ind, :], num_top_venues)\n\n# Get the count    \nfoods_sorted.head()\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n0\nA'ali\n19\nCafé\nRestaurant\nCoffee Shop\nCupcake Shop\nBreakfast Spot\nFood\nSandwich Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nBakery\n\n\n1\nAbu Baham\n9\nMiddle Eastern Restaurant\nCafeteria\nIce Cream Shop\nDonut Shop\nBBQ Joint\nRestaurant\nFish & Chips Shop\nMediterranean Restaurant\nAfghan Restaurant\nNew American Restaurant\n\n\n2\nAl Daih\n50\nMiddle Eastern Restaurant\nBakery\nBreakfast Spot\nDessert Shop\nCafé\nRestaurant\nTurkish Restaurant\nBurger Joint\nDiner\nSteakhouse\n\n\n3\nAl Dair\n9\nBakery\nRestaurant\nBBQ Joint\nItalian Restaurant\nFast Food Restaurant\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n4\nAl Garrya\n50\nRestaurant\nBreakfast Spot\nIndian Restaurant\nCoffee Shop\nFilipino Restaurant\nCafé\nFried Chicken Joint\nFast Food Restaurant\nDiner\nMiddle Eastern Restaurant\n\n\n\n\n\n\n\nLet’s call this table foods_sorted."
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#cluster-areas",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#cluster-areas",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Cluster Areas",
    "text": "Cluster Areas\nNow we are ready for further analysis and clustering. We will use the bh_grouped dataframe since it contains the necessary numerical values for machine learning.\nOur feature set is comprised of all the food categories (10 features).\nWe are excluding the NumberOfFoodPlaces feature as input to the ML model, since our problem requires segmenting areas by the type of food available. This quantity is only relevant to us to finally decide whether to live in an area or not.\nA more concrete reason to exclude it, is the fact that there are all sorts of factors involved that we’re neglecting due to lack of data, such as living costs, access to public transport etc.\nThis is a foodie’s guide to finding a place, and this venture shouldn’t be bogged-down by the fact that there are sometimes fewer number of restaurants than one would expect.\nOur target value will be cluster labels.\nFor our machine learning analysis, we will use the simplest clustering algorithm to separate the areas which is K-Means Clustering; an unsupervised machine learning approach to serve our purpose. We’ll use the popular machine learning library Sci-Kit Learn to do that in python.\nWe’ll run k-means to group the areas into 5 clusters. We pick this number for the sake of examination. We’ll fit the model on the entire data to learn these clusters.\n\n# set number of clusters\nkclusters = 5\n\nbh_grouped_clustering = bh_grouped.drop(['Area','NumberOfFoodPlaces'], 1)\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(bh_grouped_clustering)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10] \n\narray([1, 0, 0, 2, 0, 0, 0, 0, 1, 1], dtype=int32)\n\n\nLet’s create a new dataframe bh_merged that includes the cluster as well as the top 10 food places for each area.\n\n# add clustering labels\ntry:\n    foods_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\nexcept:\n    # Allows me to retry if the Cluster Labels column exists\n    foods_sorted['Cluster Labels'] = kmeans.labels_\n\nbh_merged = bh_data\n\n# merge bh_grouped with bh_data to add latitude/longitude for each neighborhood\nbh_merged = bh_merged.join(foods_sorted.set_index('Area'), on='Area')\nbh_merged.dropna(how='any', axis=0, inplace=True)\nbh_merged['Cluster Labels'] = bh_merged['Cluster Labels'].astype(np.int32)\nbh_merged.head() # check the last columns!\n\n\n\n\n\n\n\n\nArea\nLatitude\nLongitude\nCluster Labels\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n0\nA'ali\n26.154454\n50.527364\n1\n19.0\nCafé\nRestaurant\nCoffee Shop\nCupcake Shop\nBreakfast Spot\nFood\nSandwich Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nBakery\n\n\n1\nAbu Baham\n26.205737\n50.541668\n0\n9.0\nMiddle Eastern Restaurant\nCafeteria\nIce Cream Shop\nDonut Shop\nBBQ Joint\nRestaurant\nFish & Chips Shop\nMediterranean Restaurant\nAfghan Restaurant\nNew American Restaurant\n\n\n3\nAl Garrya\n26.232690\n50.578110\n0\n50.0\nRestaurant\nBreakfast Spot\nIndian Restaurant\nCoffee Shop\nFilipino Restaurant\nCafé\nFried Chicken Joint\nFast Food Restaurant\nDiner\nMiddle Eastern Restaurant\n\n\n4\nAl Hajar\n26.225405\n50.590138\n0\n49.0\nCafé\nFilipino Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nCoffee Shop\nAsian Restaurant\nIndian Restaurant\nPizza Place\nBBQ Joint\nRestaurant\n\n\n5\nAl Kharijiya\n26.160230\n50.609140\n0\n16.0\nCafeteria\nAsian Restaurant\nFast Food Restaurant\nBakery\nWings Joint\nPizza Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nCafé\nFood Court\n\n\n\n\n\n\n\nFinally, let’s visualize the resulting clusters\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "posts/foodies-guide-to-finding-a-home/index.html#examine-clusters-final-conclusion",
    "href": "posts/foodies-guide-to-finding-a-home/index.html#examine-clusters-final-conclusion",
    "title": "Foodie’s Guide to Finding a Home in Bahrain",
    "section": "Examine Clusters & Final Conclusion",
    "text": "Examine Clusters & Final Conclusion\nNow, we can examine & determine the discriminating characteristics of each cluster.\n\nCluster 1\n\ncluster1 = bh_merged.loc[bh_merged['Cluster Labels'] == 0, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster1\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n1\nAbu Baham\n9.0\nMiddle Eastern Restaurant\nCafeteria\nIce Cream Shop\nDonut Shop\nBBQ Joint\nRestaurant\nFish & Chips Shop\nMediterranean Restaurant\nAfghan Restaurant\nNew American Restaurant\n\n\n3\nAl Garrya\n50.0\nRestaurant\nBreakfast Spot\nIndian Restaurant\nCoffee Shop\nFilipino Restaurant\nCafé\nFried Chicken Joint\nFast Food Restaurant\nDiner\nMiddle Eastern Restaurant\n\n\n4\nAl Hajar\n49.0\nCafé\nFilipino Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nCoffee Shop\nAsian Restaurant\nIndian Restaurant\nPizza Place\nBBQ Joint\nRestaurant\n\n\n5\nAl Kharijiya\n16.0\nCafeteria\nAsian Restaurant\nFast Food Restaurant\nBakery\nWings Joint\nPizza Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nCafé\nFood Court\n\n\n12\nArad\n50.0\nMiddle Eastern Restaurant\nRestaurant\nDessert Shop\nBurger Joint\nCafé\nFast Food Restaurant\nDiner\nIce Cream Shop\nBakery\nSandwich Place\n\n\n15\nBudaiya\n36.0\nMiddle Eastern Restaurant\nBakery\nCafeteria\nBurger Joint\nSeafood Restaurant\nTea Room\nCafé\nSandwich Place\nIce Cream Shop\nRestaurant\n\n\n16\nJid Ali\n48.0\nRestaurant\nMiddle Eastern Restaurant\nCafé\nItalian Restaurant\nCoffee Shop\nBreakfast Spot\nDessert Shop\nPizza Place\nDiner\nSeafood Restaurant\n\n\n18\nBani Jamra\n19.0\nBreakfast Spot\nRestaurant\nCafé\nCafeteria\nMiddle Eastern Restaurant\nVegetarian / Vegan Restaurant\nBakery\nSnack Place\nIndian Restaurant\nFast Food Restaurant\n\n\n19\nBarbar\n6.0\nPizza Place\nBBQ Joint\nBakery\nSandwich Place\nMiddle Eastern Restaurant\nJuice Bar\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n21\nBu Quwah\n12.0\nTurkish Restaurant\nCafeteria\nFood\nPizza Place\nAsian Restaurant\nBakery\nRestaurant\nFalafel Restaurant\nSeafood Restaurant\nCoffee Shop\n\n\n22\nBuri\n22.0\nIndian Restaurant\nBakery\nBreakfast Spot\nRestaurant\nFood\nFood Court\nMiddle Eastern Restaurant\nCafé\nBurger Joint\nCafeteria\n\n\n23\nBusaiteen\n43.0\nCoffee Shop\nCafé\nBurger Joint\nIce Cream Shop\nTea Room\nJuice Bar\nMiddle Eastern Restaurant\nDonut Shop\nRestaurant\nDessert Shop\n\n\n24\nAl Daih\n50.0\nMiddle Eastern Restaurant\nBakery\nBreakfast Spot\nDessert Shop\nCafé\nRestaurant\nTurkish Restaurant\nBurger Joint\nDiner\nSteakhouse\n\n\n28\nDiraz\n16.0\nCafeteria\nMiddle Eastern Restaurant\nFast Food Restaurant\nSteakhouse\nSnack Place\nVegetarian / Vegan Restaurant\nAsian Restaurant\nIndian Restaurant\nRestaurant\nBreakfast Spot\n\n\n32\nEast Hidd City\n47.0\nCoffee Shop\nFried Chicken Joint\nPizza Place\nCafeteria\nCafé\nIndian Restaurant\nBBQ Joint\nIce Cream Shop\nBakery\nBurrito Place\n\n\n34\nGalali\n50.0\nFast Food Restaurant\nRestaurant\nIce Cream Shop\nSandwich Place\nMiddle Eastern Restaurant\nTurkish Restaurant\nTea Room\nBurger Joint\nItalian Restaurant\nCafé\n\n\n35\nAl Hidd\n47.0\nCoffee Shop\nFried Chicken Joint\nPizza Place\nCafeteria\nCafé\nIndian Restaurant\nBBQ Joint\nIce Cream Shop\nBakery\nBurrito Place\n\n\n36\nHalat Bu Maher\n48.0\nMiddle Eastern Restaurant\nIce Cream Shop\nSeafood Restaurant\nCafé\nSandwich Place\nCafeteria\nFast Food Restaurant\nIndian Restaurant\nRestaurant\nTurkish Restaurant\n\n\n37\nHalat Nuaim\n12.0\nRestaurant\nFood Truck\nLebanese Restaurant\nIndian Restaurant\nDeli / Bodega\nBakery\nSeafood Restaurant\nBurger Joint\nCoffee Shop\nCafé\n\n\n38\nHamad Town\n7.0\nSandwich Place\nWings Joint\nBakery\nJuice Bar\nVegetarian / Vegan Restaurant\nVietnamese Restaurant\nHookah Bar\nLebanese Restaurant\nNew American Restaurant\nMovie Theater\n\n\n41\nHillat Abdul Saleh\n47.0\nMiddle Eastern Restaurant\nFast Food Restaurant\nCoffee Shop\nIce Cream Shop\nPizza Place\nJuice Bar\nDonut Shop\nCafé\nFood Truck\nBakery\n\n\n47\nJid Al-Haj\n48.0\nRestaurant\nMiddle Eastern Restaurant\nCafé\nItalian Restaurant\nCoffee Shop\nBreakfast Spot\nDessert Shop\nPizza Place\nDiner\nSeafood Restaurant\n\n\n48\nJidhafs\n49.0\nBakery\nBreakfast Spot\nMiddle Eastern Restaurant\nTurkish Restaurant\nDessert Shop\nAmerican Restaurant\nFood\nBurger Joint\nSteakhouse\nCafeteria\n\n\n51\nKarrana\n28.0\nMiddle Eastern Restaurant\nRestaurant\nIce Cream Shop\nAsian Restaurant\nFalafel Restaurant\nBakery\nCoffee Shop\nCafeteria\nItalian Restaurant\nFried Chicken Joint\n\n\n52\nKarzakan\n4.0\nVegetarian / Vegan Restaurant\nTurkish Restaurant\nRestaurant\nShawarma Place\nAfghan Restaurant\nKebab Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n53\nKhamis\n20.0\nMiddle Eastern Restaurant\nSandwich Place\nTurkish Restaurant\nCafeteria\nMediterranean Restaurant\nCafé\nRestaurant\nTea Room\nItalian Restaurant\nDiner\n\n\n57\nManama\n45.0\nIndian Restaurant\nFilipino Restaurant\nAsian Restaurant\nPizza Place\nMiddle Eastern Restaurant\nCoffee Shop\nCafé\nCafeteria\nBBQ Joint\nRestaurant\n\n\n59\nMuharraq\n48.0\nMiddle Eastern Restaurant\nSeafood Restaurant\nCafé\nCafeteria\nRestaurant\nIce Cream Shop\nTurkish Restaurant\nCoffee Shop\nPizza Place\nBurger Joint\n\n\n70\nSamaheej\n3.0\nRestaurant\nBakery\nAfghan Restaurant\nKorean Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n71\nSanad\n38.0\nMiddle Eastern Restaurant\nRestaurant\nIce Cream Shop\nSandwich Place\nPizza Place\nBreakfast Spot\nCafeteria\nBurger Joint\nCafé\nBakery\n\n\n72\nSar\n27.0\nPizza Place\nBurger Joint\nGas Station\nChinese Restaurant\nFast Food Restaurant\nBreakfast Spot\nThai Restaurant\nCoffee Shop\nFried Chicken Joint\nDeli / Bodega\n\n\n75\nShakhura\n45.0\nMiddle Eastern Restaurant\nBreakfast Spot\nCafé\nRestaurant\nFast Food Restaurant\nDessert Shop\nBakery\nPizza Place\nDonut Shop\nIce Cream Shop\n\n\n78\nTashan\n25.0\nMiddle Eastern Restaurant\nRestaurant\nTurkish Restaurant\nSeafood Restaurant\nCafeteria\nIndian Restaurant\nItalian Restaurant\nMediterranean Restaurant\nDiner\nDessert Shop\n\n\n79\nTubli\n34.0\nRestaurant\nBakery\nBurger Joint\nJuice Bar\nMiddle Eastern Restaurant\nCoffee Shop\nCafé\nTurkish Restaurant\nMediterranean Restaurant\nPie Shop\n\n\n\n\n\n\n\n\n\nThis cluster has 34 areas\n\n\n\n\nCluster 2\n\ncluster2 = bh_merged.loc[bh_merged['Cluster Labels'] == 1, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster2\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n0\nA'ali\n19.0\nCafé\nRestaurant\nCoffee Shop\nCupcake Shop\nBreakfast Spot\nFood\nSandwich Place\nFalafel Restaurant\nMiddle Eastern Restaurant\nBakery\n\n\n6\nAl Markh\n17.0\nCafé\nFast Food Restaurant\nBurger Joint\nIce Cream Shop\nJuice Bar\nCoffee Shop\nMiddle Eastern Restaurant\nDessert Shop\nBakery\nBBQ Joint\n\n\n7\nAl Musalla\n17.0\nFast Food Restaurant\nCafé\nCoffee Shop\nMiddle Eastern Restaurant\nSeafood Restaurant\nSteakhouse\nFood Court\nRestaurant\nPizza Place\nJapanese Restaurant\n\n\n8\nAl Qadam\n4.0\nPizza Place\nCafé\nCafeteria\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n9\nAl Qala\n50.0\nCafé\nBurger Joint\nRestaurant\nDessert Shop\nCoffee Shop\nSandwich Place\nBakery\nJuice Bar\nIce Cream Shop\nPizza Place\n\n\n10\nAl Qurayyah\n11.0\nRestaurant\nCafé\nMediterranean Restaurant\nMiddle Eastern Restaurant\nComfort Food Restaurant\nCoffee Shop\nDiner\nJapanese Restaurant\nJuice Bar\nPastry Shop\n\n\n11\nAmwaj Islands\n37.0\nCafé\nMiddle Eastern Restaurant\nAmerican Restaurant\nIndian Restaurant\nRestaurant\nAsian Restaurant\nPizza Place\nDeli / Bodega\nDiner\nPortuguese Restaurant\n\n\n13\nAskar\n4.0\nCafeteria\nBurger Joint\nCafé\nPie Shop\nPastry Shop\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\n\n\n17\nBahrain Bay\n50.0\nCoffee Shop\nCafé\nIndian Restaurant\nPizza Place\nSteakhouse\nBurger Joint\nMiddle Eastern Restaurant\nRestaurant\nFried Chicken Joint\nAmerican Restaurant\n\n\n20\nBilad Al Qadeem\n46.0\nCafé\nMiddle Eastern Restaurant\nIce Cream Shop\nBreakfast Spot\nSandwich Place\nFast Food Restaurant\nPizza Place\nBurger Joint\nRestaurant\nBakery\n\n\n27\nDiplomatic Area\n50.0\nCoffee Shop\nCafé\nAmerican Restaurant\nFried Chicken Joint\nBurger Joint\nRestaurant\nFood Court\nMiddle Eastern Restaurant\nIndian Restaurant\nFrench Restaurant\n\n\n30\nDumistan\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n33\nEker\n6.0\nDiner\nSnack Place\nCreperie\nMiddle Eastern Restaurant\nCafé\nCafeteria\nPastry Shop\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n39\nHamala\n29.0\nCoffee Shop\nCafé\nBurger Joint\nPizza Place\nRestaurant\nSandwich Place\nItalian Restaurant\nMexican Restaurant\nHot Dog Joint\nMediterranean Restaurant\n\n\n42\nIsa Town\n43.0\nCafé\nIndian Restaurant\nRestaurant\nPizza Place\nBakery\nCafeteria\nCoffee Shop\nFast Food Restaurant\nTheme Restaurant\nItalian Restaurant\n\n\n43\nJanabiyah\n29.0\nCoffee Shop\nCafé\nBurger Joint\nPizza Place\nRestaurant\nSandwich Place\nItalian Restaurant\nMexican Restaurant\nHot Dog Joint\nMediterranean Restaurant\n\n\n49\nJurdab\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n50\nKarbabad\n12.0\nCafé\nSandwich Place\nCoffee Shop\nAsian Restaurant\nBakery\nMiddle Eastern Restaurant\nBurger Joint\nCafeteria\nAfghan Restaurant\nMediterranean Restaurant\n\n\n55\nMahazza\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n58\nMarquban\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n60\nMuqaba\n35.0\nCafé\nCoffee Shop\nRestaurant\nBakery\nBreakfast Spot\nCafeteria\nFried Chicken Joint\nFood Court\nMiddle Eastern Restaurant\nPersian Restaurant\n\n\n61\nMuqsha\n49.0\nCafé\nCoffee Shop\nBakery\nBurger Joint\nIndian Restaurant\nMiddle Eastern Restaurant\nFast Food Restaurant\nIce Cream Shop\nFalafel Restaurant\nFilipino Restaurant\n\n\n64\nNuwaidrat\n7.0\nCafé\nAsian Restaurant\nRestaurant\nBakery\nMiddle Eastern Restaurant\nFalafel Restaurant\nAfghan Restaurant\nMediterranean Restaurant\nNoodle House\nNew American Restaurant\n\n\n65\nRiffa\n49.0\nCafé\nCoffee Shop\nBurger Joint\nRestaurant\nBakery\nSandwich Place\nDessert Shop\nPizza Place\nItalian Restaurant\nLebanese Restaurant\n\n\n66\nReef Island\n13.0\nCafé\nAmerican Restaurant\nMiddle Eastern Restaurant\nRestaurant\nFrench Restaurant\nJapanese Restaurant\nBreakfast Spot\nDessert Shop\nLebanese Restaurant\nMediterranean Restaurant\n\n\n68\nSakhir\n8.0\nBurger Joint\nDiner\nFood Truck\nMiddle Eastern Restaurant\nCafé\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\n\n\n69\nSalmabad\n12.0\nCafé\nCupcake Shop\nBakery\nCoffee Shop\nMiddle Eastern Restaurant\nBreakfast Spot\nJuice Bar\nFood\nAsian Restaurant\nJapanese Restaurant\n\n\n73\nSehla\n6.0\nBurger Joint\nCafé\nAmerican Restaurant\nIce Cream Shop\nBakery\nFood Court\nPastry Shop\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n81\nZallaq\n29.0\nCafé\nCoffee Shop\nRestaurant\nJuice Bar\nMiddle Eastern Restaurant\nFast Food Restaurant\nCreperie\nCafeteria\nBurrito Place\nBurger Joint\n\n\n\n\n\n\n\n\n\nThis cluster has 29 areas\n\n\n\n\nCluster 3\n\ncluster3 = bh_merged.loc[bh_merged['Cluster Labels'] == 2, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster3\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n25\nAl Dair\n9.0\nBakery\nRestaurant\nBBQ Joint\nItalian Restaurant\nFast Food Restaurant\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\n\n\n44\nJannusan\n11.0\nBakery\nGastropub\nCafeteria\nIndian Restaurant\nSandwich Place\nFish & Chips Shop\nSeafood Restaurant\nSnack Place\nNew American Restaurant\nMovie Theater\n\n\n46\nJaww\n1.0\nBakery\nAfghan Restaurant\nKorean Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\nMediterranean Restaurant\n\n\n54\nMa'ameer\n3.0\nCreperie\nDiner\nBakery\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\n\n\n76\nSitra\n2.0\nTurkish Restaurant\nBakery\nAfghan Restaurant\nKorean Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n\n\n\n\n\n\n\nThis cluster has 5 areas\n\n\n\n\nCluster 4\n\ncluster4 = bh_merged.loc[bh_merged['Cluster Labels'] == 3, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster4\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n14\nAwali\n1.0\nCafé\nAfghan Restaurant\nPersian Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\nMediterranean Restaurant\n\n\n\n\n\n\n\n\n\nThis cluster has 1 area\n\n\n\n\nCluster 5\n\ncluster5 = bh_merged.loc[bh_merged['Cluster Labels'] == 4, bh_merged.columns[[0] + list(range(4, bh_merged.shape[1]))]]\ncluster5\n\n\n\n\n\n\n\n\nArea\nNumberOfFoodPlaces\n1st Most Common Food Place\n2nd Most Common Food Place\n3rd Most Common Food Place\n4th Most Common Food Place\n5th Most Common Food Place\n6th Most Common Food Place\n7th Most Common Food Place\n8th Most Common Food Place\n9th Most Common Food Place\n10th Most Common Food Place\n\n\n\n\n26\nDar Kulaib\n4.0\nRestaurant\nCoffee Shop\nSandwich Place\nBreakfast Spot\nAfghan Restaurant\nLebanese Restaurant\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\n\n\n56\nMalkiya\n2.0\nIce Cream Shop\nCoffee Shop\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n77\nSufala\n3.0\nCoffee Shop\nRestaurant\nAfghan Restaurant\nLebanese Restaurant\nNoodle House\nNew American Restaurant\nMovie Theater\nMoroccan Restaurant\nMiddle Eastern Restaurant\nMexican Restaurant\n\n\n\n\n\n\n\n\n\nThis cluster has 3 areas\n\n\n\n\nConclusion\nPhew! We’re done with finding our clusters, and finding out which areas fall into it. To understand the constraints and my discussion to conclude this solution, please refer to my report available on my github repo.\nI hope you’ve enjoyed reading & learning something new from this post. Doing this was part of my data-science course, and I hope you can do the same with your hobby projects.\nUntil next time, cheers!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "I’m a PhD candidate at SUNY-ESF, working on new tools to make complex, broad-scale systems easier for humans to understand. Right now, that means I split my time between predictive modeling (to monitor forest carbon sequestration across New York State, track the development of early-successional forests, and more) and visualization (including using game engines as a GIS and making it easier to make reproducible VR environments for research). By training I’m either an ecologist (specializing in landscape and translational ecology) or environmental scientist; professionally, I’ve worked as a data analyst, software engineer, and chicken farmer.\nOn this site I keep a list of my publications, presentations, and my CV, as well as a technical blog.\n\n\n\n\n\n\n\nFiltering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes. Mahoney, MJ, Johnson, LK, Bevilacqua, E, and Beier, CM. 2022. GIScience & Remote Sensing 59(1): 1266-1280. https://doi.org/10.1080/15481603.2022.2103069\nClassification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast. Mahoney, MJ, Johnson, LK, Guinan, AZ, and Beier, CM. 2022. International Journal of Remote Sensing 43(19-24): 7117-7138. https://doi.org/10.1080/01431161.2022.2155086\nunifir: A Unifying API for Interacting with Unity from R. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software 7(73): 4388. https://doi.org/10.21105/joss.04388\nterrainr: An R package for creating immersive virtual environments. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software, 7(69): 4060. https://doi.org/10.21105/joss.04060\n\n\n\nwaywiser | Ergonomic Methods for Assessing Spatial Models | 2023\nspatialsample | Spatial Resampling Infrastructure | 2022\nunifir | A Unifying API for Working with Unity in R | 2022\nterrainr | Retrieve Data from the USGS National Map and Transform it for 3D Landscape Visualizations | 2021"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "I’m a PhD candidate at SUNY-ESF, working on new tools to make complex, broad-scale systems easier for humans to understand. Right now, that means I split my time between predictive modeling (to monitor forest carbon sequestration across New York State, track the development of early-successional forests, and more) and visualization (including using game engines as a GIS and making it easier to make reproducible VR environments for research). By training I’m either an ecologist (specializing in landscape and translational ecology) or environmental scientist; professionally, I’ve worked as a data analyst, software engineer, and chicken farmer.\nOn this site I keep a list of my publications, presentations, and my CV, as well as a technical blog."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Isa AlDoseri",
    "section": "",
    "text": "Filtering ground noise from LiDAR returns produces inferior models of forest aboveground biomass in heterogenous landscapes. Mahoney, MJ, Johnson, LK, Bevilacqua, E, and Beier, CM. 2022. GIScience & Remote Sensing 59(1): 1266-1280. https://doi.org/10.1080/15481603.2022.2103069\nClassification and mapping of low-statured shrubland cover types in post-agricultural landscapes of the US Northeast. Mahoney, MJ, Johnson, LK, Guinan, AZ, and Beier, CM. 2022. International Journal of Remote Sensing 43(19-24): 7117-7138. https://doi.org/10.1080/01431161.2022.2155086\nunifir: A Unifying API for Interacting with Unity from R. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software 7(73): 4388. https://doi.org/10.21105/joss.04388\nterrainr: An R package for creating immersive virtual environments. Mahoney, MJ, Beier, CM, and Ackerman, AC. 2022. Journal of Open Source Software, 7(69): 4060. https://doi.org/10.21105/joss.04060\n\n\n\nwaywiser | Ergonomic Methods for Assessing Spatial Models | 2023\nspatialsample | Spatial Resampling Infrastructure | 2022\nunifir | A Unifying API for Working with Unity in R | 2022\nterrainr | Retrieve Data from the USGS National Map and Transform it for 3D Landscape Visualizations | 2021"
  }
]